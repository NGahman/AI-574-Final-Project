{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63dfdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d5b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify data path and change directory\n",
    "data_path = 'Data'\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3085a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Note: added an \"Introduction\" line to S1387700313001822.txt to \n",
    "# make reading in data easier. This file was the only one to not \n",
    "# include Introduction line ***\n",
    "# *** Note: switched \"Highlights\" and \"Abstract\" paragraph \n",
    "# locations to make reading in data easier for S016816561300552X.txt, \n",
    "# S1161030113001950.txt, and S1750583613004192.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77f0665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate titles, abstracts, and texts\n",
    "titles = []\n",
    "abstracts = []\n",
    "texts = []\n",
    "\n",
    "for filename in os.listdir():\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            title = lines[0].strip()\n",
    "            titles.append(title)\n",
    "            \n",
    "            abstract_text = ''\n",
    "            intro_text = ''\n",
    "            capturing_abstract = False\n",
    "            capturing_intro = False\n",
    "            \n",
    "            # Loop through lines to extract abstracts and texts\n",
    "            for line in lines:\n",
    "                if 'Abstract' in line:\n",
    "                    capturing_abstract = True\n",
    "                elif 'Introduction' in line:\n",
    "                    capturing_intro = True\n",
    "                \n",
    "                if capturing_abstract and not capturing_intro:\n",
    "                    abstract_text += line.strip() + ' '\n",
    "                elif capturing_intro:\n",
    "                    intro_text += line.strip() + ' '\n",
    "            \n",
    "            abstracts.append(abstract_text)\n",
    "            texts.append(intro_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa924900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles:\n",
      " ['Sylow p-groups of polynomial permutations on the integers mod pn', 'The transterminator ion flow at Venus at solar minimum', 'The modelling of the toughening of epoxy polymers via silica nanoparticles: The effects of volume fraction and particle size', 'Flow structure and near-field dispersion in arrays of building-like obstacles', 'A memory access model for highly-threaded many-core architectures', 'Investigating the feasibility of scale up and automation of human induced pluripotent stem cells cultured in aggregates in feeder free conditions', 'Phosphorus levels in croplands of the European Union with implications for P fertilizer use', 'Chirality delivery through multiple and helical H-bonding from chiral coordination complex to its supramolecular architecture', 'The Sleipner storage site: Capillary flow modeling of a layered CO2 plume requires fractured shale barriers within the Utsira Formation', 'Combined analysis of sMRI and fMRI imaging data provides accurate disease markers for hearing impairment']\n",
      "Abstracts:\n",
      " ['Abstract We enumerate and describe the Sylow p-groups of the groups of polynomial permutations of the integers mod pn for n⩾1 and of the pro-finite group which is the projective limit of these groups.  ', 'Abstract The transterminator ion flow in the Venusian ionosphere is observed at solar minimum for the first time. Such a flow, which transports ions from the day to the nightside, has been observed previously around solar maximum. At solar minimum this transport process is severely inhibited by the lower altitude of the ionopause. The observations presented were those made of the Venusian ionospheric plasma by the ASPERA-4 experiment onboard the Venus Express spacecraft, and which constitute the first extensive in-situ measurements of the plasma near solar minimum. Observations near the terminator of the energies of ions of ionospheric origin showed asymmetry between the noon and midnight sectors, which indicated an antisunward ion flow with a velocity of (2.5±1.5)kms-1. It is suggested that this ion flow contributes to maintaining the nightside ionosphere near the terminator region at solar minimum. The interpretation of the result was reinforced by observed asymmetries in the ion number counts. The observed dawn-dusk asymmetry was consistent with a nightward transport of ions while the noon-midnight observations indicated that the flow was highly variable but could contribute to the maintenance of the nightside ionosphere. Highlights ► The transterminator ion flow in the Venusian ionosphere is observed at solar minimum. ► This flow has a velocity of (2.5±1.5)kms-1. ► The occurrence of this flow is highly variable, but can be a significant source of the nightside ionosphere.  ', 'Abstract Silica nanoparticles possessing three different diameters (23, 74 and 170 nm) were used to modify a piperidine-cured epoxy polymer. Fracture tests were performed and values of the toughness increased steadily as the concentration of silica nanoparticles was increased. However, no significant effects of particle size were found on the measured value of toughness. The toughening mechanisms were identified as (i) the formation of localised shear-band yielding in the epoxy matrix polymer which is initiated by the silica nanoparticles, and (ii) debonding of the silica nanoparticles followed by plastic void growth of the epoxy matrix polymer. These mechanisms, and hence the toughness of the epoxy polymers containing the silica nanoparticles, were modelled using the Hsieh et al. approach (Polymer 51, 2010, 6284-6294). However, it is noteworthy that previous modelling work has required the volume fraction of debonded silica particles to be measured from the fracture surfaces but in the present paper a new and more fundamental approach has been proposed. Here finite-element modelling has demonstrated that once one silica nanoparticle debonds then its nearest neighbours are shielded from the applied stress field, and hence may not debond. Statistical analysis showed that, for a good, i.e. random, dispersion of nanoparticles, each nanoparticle has six nearest neighbours, so only one in seven particles would be predicted to debond. This approach therefore predicts that only 14.3% of the nanoparticles present will debond, and this value is in excellent agreement with the value of 10-15% of those nanoparticles present debonding which was recorded via direct observations of the fracture surfaces. Further, this value of about 15% of silica nanoparticles particles present debonding has also been noted in other published studies, but has never been previously explained. The predictions from the modelling studies of the toughness of the various epoxy polymers containing the silica nanoparticles were compared with the measured fracture energies and the agreement was found to be good. Graphical abstract  ', 'Abstract Dispersion in the near-field region of localised releases in urban areas is difficult to predict because of the strong influence of individual buildings. Effects include upstream dispersion, trapping of material into building wakes and enhanced concentration fluctuations. As a result, concentration patterns are highly variable in time and mean profiles in the near field are strongly non-Gaussian. These aspects of near-field dispersion are documented by analysing data from direct numerical simulations in arrays of building-like obstacles and are related to the underlying flow structure. The mean flow structure around the buildings is found to exert a strong influence over the dispersion of material in the near field. Diverging streamlines around buildings enhance lateral dispersion. Entrainment of material into building wakes in the very near field gives rise to secondary sources, which then affect the subsequent dispersion pattern. High levels of concentration fluctuations are also found in this very near field; the fluctuation intensity is of order 2 to 5. Highlights • We document key aspects of near-field dispersion in urban areas using DNS data. • We relate these dispersion features to the underlying flow structure. • The effects of wind direction, building layout and source location are examined. • Secondary sources in the near field have a strong effect on the plume spread. • Concentration means and fluctuations are mapped and their magnitudes quantified.  ', \"Abstract A number of highly-threaded, many-core Antarctica architectures hide memory-access latency by low-overhead context switching among a large number of threads. The speedup of a program on these machines depends on how well the latency is hidden. If the number of threads were infinite, theoretically, these machines could provide the performance predicted by the PRAM analysis of these programs. However, the number of threads per processor is not infinite, and is constrained by both hardware and algorithmic limits. In this paper, we introduce the Threaded Many-core Memory (TMM) model which is meant to capture the important characteristics of these highly-threaded, many-core machines. Since we model some important machine parameters of these machines, we expect analysis under this model to provide a more fine-grained and accurate performance prediction than the PRAM analysis. We analyze 4 algorithms for the classic all pairs shortest paths problem under this model. We find that even when two algorithms have the same PRAM performance, our model predicts different performance for some settings of machine parameters. For example, for dense graphs, the dynamic programming algorithm and Johnson's algorithm have the same performance in the PRAM model. However, our model predicts different performance for large enough memory-access latency and validates the intuition that the dynamic programming algorithm performs better on these machines. We validate several predictions made by our model using empirical measurements on an instantiation of a highly-threaded, many-core machine, namely the NVIDIA GTX 480. Highlights • We design a memory model to analyze algorithms for highly-threaded many-core systems. • The model captures significant factors of performance: work, span, and memory accesses. • We show the model is better than PRAM by applying both to 4 shortest paths algorithms. • Empirical performance is effectively predicted by our model in many circumstances. • It is the first formalized asymptotic model helpful for algorithm design on many-cores.  \", 'Abstract The transfer of a laboratory process into a manufacturing facility is one of the most critical steps required for the large scale production of cell-based therapy products. This study describes the first published protocol for scalable automated expansion of human induced pluripotent stem cell lines growing in aggregates in feeder-free and chemically defined medium. Cells were successfully transferred between different sites representative of research and manufacturing settings; and passaged manually and using the CompacT SelecT automation platform. Modified protocols were developed for the automated system and the management of cells aggregates (clumps) was identified as the critical step. Cellular morphology, pluripotency gene expression and differentiation into the three germ layers have been used compare the outcomes of manual and automated processes. Highlights • First published protocol for scalable automation of hiPSC in feeder-free conditions. • Successful transfer of hiPSC between sites representative of research and manufacture. • Comparability between manual and automated expansion protocols for hiPSC.  ', \"Abstract In the frame of the Land Use/Land Cover Area Frame Survey sampling of topsoil was carried out on around 22,000 points in 25 EU Member States in 2009 and in additional 2 Member States in 2012. Besides other basic soil properties soil phosphorus (P) content of the samples were also measured in a single laboratory in both years. Based on the results of the LUCAS topsoil survey we performed an assessment of plant available P status of European croplands. Higher P levels can be observed in regions where higher crop yields can be expected and where high fertilizer P inputs are reported. Plant available phosphorus levels were determined using two selected fertilizer recommendation systems: one from Hungary and one from the United Kingdom. The fertilizer recommendation system of the UK does not recommend additional fertilizer use on croplands with highest P supply, which covers regions mostly in Belgium and the Netherlands. According to a Hungarian advisory system there is a need for fertilizer P input in all regions of the EU. We established a P fertilizer need map based on integrating results from the two systems. Based on data from 2009 and 2012, P input demand of croplands in the European Union was estimated to 3,849,873 tons(P2O5)/year. Meanwhile we found disparities of calculated input need and reported fertilizer statistics both on local (country) scale and EU level. The first ever uniform topsoil P survey of the EU highlights the contradictions between soil P management of different countries of the Union and the inconsistencies between reported P fertilizer consumption and advised P doses. Our analysis shows a status of a baseline period of the years 2009 and 2012, while a repeated LUCAS topsoil survey can be a useful tool to monitor future changes of nutrient levels, including P in soils of the EU. Highlights • Soil P supply and fertilizer need maps of the EU were elaborated for the first time. • Estimated annual P input need of the EU's agriculture is 3.85 million tons. • Plant available P in cropland soils follows climatic patterns and yield levels in the EU. • Regional P means vary largely but cases with high P levels are present in all regions. • Inconsistencies between advised and reported P fertilizer consumptions pointed out.  \", 'Abstract The path of the chirality delivery in the crystalline and chiral nucleotide-Co(II) complex, [Co(GMP)(H2O)5]·3H2O (GMP=guanosine-5′-monophosphate), has been studied based on X-ray single crystal diffraction analysis, liquid- and solid-state circular dichroism (CD) spectroscopy. The multiple and helical H-bonding is a distinctive way of chirality delivery from the discrete molecules to three-dimensional supramolecular architecture. Graphical abstract The molecular chirality of crystallized nucleotide-transition metal coordination complex is delivered to three-dimensional supramolecular architecture through multiple and helical H-bonding in its crystal lattice. Liquid- and solid-state CD spectroscopy confirm the novel path of chirality delivery. Highlights • The chirality delivery in the crystallized coordination complex has been studied. • The path of chirality delivery in this complex is multiple and helical H-bonding. • Liquid- and solid-state CD spectroscopy confirm the mechanism of chirality delivery.  ', \"Abstract To prevent ocean acidification and mitigate greenhouse gas emissions, it is necessary to capture and store carbon dioxide. The Sleipner storage site, offshore Norway, is the world's first and largest engineered waste repository for a greenhouse gas. CO2 is separated from the Sleipner gas condensate field and stored in the pore space of the Utsira Formation, a saline aquifer approximately 1km below the surface and 200km from the coast. Statoil, the field operator, has injected almost 1Mt/yr of captured CO2 into the storage site since 1996. The buoyant CO2 plume ascended rapidly through eight thin shale barriers within the aquifer to reach the top seal in less than three years. The plume's progress has been monitored by eight seismic surveys, as well as gravimetric and electromagnetic monitoring, which record the spreading of nine thin CO2 layers. This paper presents a capillary flow model using invasion percolation physics that accurately matches the plume's geometry. The approach differs from standard Darcy flow simulations, which fail to match the plume geometry. The calibrated capillary flow simulation indicates that a mass balance for the plume is likely, but can only replicate the plume geometry if the thin intra-formational shale barriers are fractured. The model enables an estimate of the shale barrier behavior and caprock performance. The fractures are very unlikely to have been caused by CO2 injection given the confining stress of the rock and weak overpressure of the plume, and so fracturing must pre-date injection. A novel mechanism is suggested: the deglaciation of regional ice sheets that have rapidly and repeatedly unloaded approximately 1km of ice. The induced transient pore pressures are sufficient to hydro-fracture thin shales. The fractures enable fast CO2 ascent, resulting in a multi-layered plume. Shallow CO2 storage sites in the Northern North Sea and other regions that have been loaded by Quaternary ice sheets are likely to behave in a similar manner. Highlights • Flow model for Sleipner with a plausible layering of CO2 plume and mass balance. • CO2 mass balance analysis for poor temperature and layer thickness constraints. • An unusual capillary flow model, not Darcy flow, results in a successful 3D model. • Low shale barrier threshold pressures indicate pervasive micro-fracturing. • Pre-CO2 fracturing hypothesis based on rapid melting of Pleistocene ice sheets.  \", 'Abstract In this research, we developed a robust two-layer classifier that can accurately classify normal hearing (NH) from hearing impaired (HI) infants with congenital sensori-neural hearing loss (SNHL) based on their Magnetic Resonance (MR) images. Unlike traditional methods that examine the intensity of each single voxel, we extracted high-level features to characterize the structural MR images (sMRI) and functional MR images (fMRI). The Scale Invariant Feature Transform (SIFT) algorithm was employed to detect and describe the local features in sMRI. For fMRI, we constructed contrast maps and detected the most activated/de-activated regions in each individual. Based on those salient regions occurring across individuals, the bag-of-words strategy was introduced to vectorize the contrast maps. We then used a two-layer model to integrate these two types of features together. With the leave-one-out cross-validation approach, this integrated model achieved an AUC score of 0.90. Additionally, our algorithm highlighted several important brain regions that differentiated between NH and HI children. Some of these regions, e.g. planum temporale and angular gyrus, were well known auditory and visual language association regions. Others, e.g. the anterior cingulate cortex (ACC), were not necessarily expected to play a role in differentiating HI from NH children and provided a new understanding of brain function and of the disorder itself. These important brain regions provided clues about neuroimaging markers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant. This type of prognostic information could be extremely useful and is currently not available to clinicians by any other means. Highlights • We probe brain structural and functional changes in hearing impaired (HI) infants. • We build a robust two-layer classifier that integrates sMRI and fMRI data. • This integrated model accurately separates HI from normal infants (AUC 0.9). • Our method detects important brain regions different between HI and normal infants. • Our method can include diverse types of data and be applied to other diseases.  ']\n",
      "Texts:\n",
      " ['Introduction Fix a prime p and let n∈N. Every polynomial f∈Z[x] defines a function from Zpn= to itself. If this function happens to be bijective, it is called a polynomial permutation of Zpn. The polynomial permutations of Zpn form a group (Gn,∘) with respect to composition. The order of this group has been known since at least 1921 (Kempner [10]) to be |G2|=p!(p-1)ppp and |Gn|=p!(p-1)pppp∑k=3nβ(k) for n⩾3, where β(k) is the least n such that pk divides n!, but the structure of (Gn,∘) is elusive. (See, however, Nöbauer [15] for some partial results.) Since the order of Gn is divisible by a high power of (p-1) for large p, even the number of Sylow p-groups is not obvious. We will show that there are (p-1)!(p-1)p-2 Sylow p-groups of Gn and describe these Sylow p-groups, see Theorem 5.1 and Corollary 5.2. Some notation: p is a fixed prime throughout. A function g:Zpn→Zpn arising from a polynomial in Zpn[x] or, equivalently, from a polynomial in Z[x], is called a polynomial function on Zpn. We denote by (Fn,∘) the monoid with respect to composition of polynomial functions on Zpn. By monoid, we mean semigroup with an identity element. Let (Gn,∘) be the group of units of (Fn,∘), which is the group of polynomial permutations of Zpn. Since every function induced by a polynomial preserves congruences modulo ideals, there is a natural epimorphism mapping polynomial functions on Zpn+1 onto polynomial functions on Zpn, and we write it as πn:Fn+1→Fn. If f is a polynomial in Z[x] (or in Zpm[x] for m⩾n) we denote the polynomial function on Zpn[x] induced by f by [f]pn. The order of Fn and that of Gn have been determined by Kempner [10] in a rather complicated manner. His results were cast into a simpler form by Nöbauer [14] and Keller and Olson [9] among others. Since then there have been many generalizations of the order formulas to more general finite rings [16,13,2,6,1,8,7]. Also, polynomial permutations in several variables (permutations of (Zpn)k defined by k-tuples of polynomials in k variables) have been looked into [5,4,19,17,18,11]. Polynomial functions and permutations To put things in context, we recall some well-known facts, to be found, among other places, in [10,14,3,9]. The reader familiar with polynomial functions on finite rings is encouraged to skip to Section 3. Note that we do not claim anything in Section 2 as new. Definition For p prime and n∈N, let αp(n)=∑k=1∞[npk] and βp(n)=min{m|αp(m)⩾n}. If p is fixed, we just write α(n) and β(n). Notation For k∈N, let (x)k=x(x-1)…(x-k+1) and (x)0=1. We denote p-adic valuation by vp. Fact 2.1 (1) αp(n)=vp(n!). (2) For 1⩽n⩽p, βp(n)=np and for n>p, βp(n)<np. (3) For all n∈Z, vp((n)k)⩾αp(k); and vp((k)k)=vp(k!)=αp(k). Proof Easy. □ Remark The sequence (βp(n))n=1∞ is obtained by going through the natural numbers in increasing order and repeating each k∈Nvp(k) times. For instance, β2(n) for n⩾1 is: 2,4,4,6,8,8,8,10,12,12,14,16,16,16,16,18,20,20,…. The falling factorials (x)0=1, (x)k=x(x-1)…(x-k+1), k>0, form a basis of the free Z-module Z[x], and representation with respect to this basis gives a convenient canonical form for a polynomial representing a given polynomial function on Zpn. Fact 2.2 (Cf. Keller and Olson [9].) A polynomial f∈Z[x], f=∑kak(x)k, induces the zero-function mod pn if and only if ak≡0 mod pn-α(k) for all k (or, equivalently, for all k<β(n)). Proof Induction on k using the facts that (m)k=0 for m<k, that vp((n)k)⩾αp(k) for all n∈Z, and that vp((k)k)=vp(k!)=αp(k). □ Corollary 2.3 (Cf. Keller and Olson [9].) Every polynomial function on Zpn is represented by a unique f∈Z[x] of the form f=∑k=0β(n)-1ak(x)k, with 0⩽ak<pn-α(k) for all k. Comparing the canonical forms of polynomial functions mod pn with those mod pn-1 we see that every polynomial function mod pn-1 gives rise to pβ(n) different polynomial functions mod pn: Corollary 2.4 (See cf. Keller and Olson [9].) Let (Fn,∘) be the monoid of polynomial functions on Zpn with respect to composition and πn:Fn+1→Fn the canonical projection. (1) For all n⩾1 and for each f∈Fn we have |πn-1(f)|=pβ(n+1). (2) For all n⩾1, the number of polynomial functions on Zpn is|Fn|=p∑k=1nβ(k). Notation We write [f]pn for the function defined by f∈Z[x] on Zpn. Lemma 2.5 Every polynomial f∈Z[x] is uniquely representable asf(x)=f0(x)+f1(x)(xp-x)+f2(x)(xp-x)2+⋯+fm(x)(xp-x)m+⋯ with fm∈Z[x], degfm<p, for all m⩾0. Now let f,g∈Z[x]. (1) If n⩽p, then [f]pn=[g]pn is equivalent to: fk=gk mod pn-kZ[x] for 0⩽k<n. (2) [f]p2=[g]p2 is equivalent to: f0=g0 mod p2Z[x] and f1=g1 mod pZ[x]. (3) [f]p=[g]p and [f′]p=[g′]p is equivalent to: f0=g0 mod pZ[x] and f1=g1 mod pZ[x]. Proof The canonical representation is obtained by repeated division with remainder by (xp-x), and uniqueness follows from uniqueness of quotient and remainder of polynomial division. Note that [f]p=[f0]p and [f′]p=[f0′-f1]p. This gives (3). Denote by f∼g the equivalence relation fk=gk mod pn-kZ[x] for 0⩽k<n. Then f∼g implies [f]pn=[g]pn. There are pp+2p+3p+⋯+np equivalence classes of ∼ and pβ(1)+β(2)+β(3)+⋯+β(n) different [f]pn. For k⩽p, β(k)=kp. Therefore the equivalence relations f∼g and [f]pn=[g]pn coincide. This gives (1), and (2) is just the special case n=2. □ We can rephrase this in terms of ideals of Z[x]. Corollary 2.6 For every n∈N, consider the two ideals of Z[x]In={f∈Z[x]|f(Z)⊆pnZ} and Jn=({pn-k(xp-x)k|0⩽k⩽n}). Then [Z[x]:In]=pβ(1)+β(2)+β(3)+⋯+β(n) and [Z[x]:Jn]=pp+2p+3p+⋯+np. Therefore, Jn=In for n⩽p, whereas for n>p, Jn is properly contained in In. Proof Jn⊆In. The index of Jn in Z[x] is pp+2p+3p+⋯+np, because f∈Jn if and only if fk=0 mod pn-kZ[x] for 0⩽k<n in the canonical representation of Lemma 2.5. The index of In in Z[x] is pβ(1)+β(2)+β(3)+⋯+β(n) by Corollary 2.4(2) and [Z[x]:In]<[Z[x]:Jn] if and only if n>p by Fact 2.1(2). □ Fact 2.7 (Cf. McDonald [12].) Let n⩾2. The function on Zpn induced by a polynomial f∈Z[x] is a permutation if and only if (1) f induces a permutation of Zp, and (2) the derivative f′ has no zero mod p. Lemma 2.8 Let [f]pn and [f]p be the functions defined by f∈Z[x] on Zpn and Zp, respectively, and [f′]p the function defined by the formal derivative of f on Zp. Then (1) [f]p2 determines not just [f]p, but also [f′]p. (2) Let n⩾2. Then [f]pn is a permutation if and only if [f]p2 is a permutation. (3) For every pair of functions (α,β), α:Zp→Zp, β:Zp→Zp, there are exactly pp polynomial functions [f]p2 on Zp2 with [f]p=α and [f′]p=β. (4) For every pair of functions (α,β), α:Zp→Zp bijective, β:Zp→Zp∖{0}, there are exactly pp polynomial permutations [f]p2 on Zp2 with [f]p=α and [f′]p=β. Proof (1) and (3) follow immediately from Lemma 2.5 for n=2 and (2) and (4) then follow from Fact 2.7. □ Remark 2.9 Fact 2.7 and Lemma 2.8(2) imply that (1) for all n⩾1, the image of Gn+1 under πn:Fn+1→Fn is contained in Gn, and (2) for all n⩾2, the inverse image of Gn under πn:Fn+1→Fn is Gn+1. We denote by πn:Gn+1→Gn the restriction of πn to Gn. This is the canonical epimorphism from the group of polynomial permutations on Zpn+1 onto the group of polynomial permutations on Zpn. The above remark allows us to draw conclusions on the projective system of groups Gn from the information in Corollary 2.4 concerning the projective system of monoids Fn. Corollary 2.10 Let n⩾2, and πn:Gn+1→Gn the canonical epimorphism from the group of polynomial permutations on Zpn+1 onto the group of polynomial permutations on Zpn. Then |ker(πn)|=pβ(n+1). Corollary 2.11 (See cf. Kempner [10] and Keller and Olson [9].) The number of polynomial permutations on Zp2 is|G2|=p!(p-1)ppp, and for n⩾3 the number of polynomial permutations on Zp2 is |Gn|=p!(p-1)pppp∑k=3nβ(k). Proof In the canonical representation of f∈Z[x] in Lemma 2.5, there are p!(p-1)p choices of coefficients mod p for f0 and f1 such that the criteria of Fact 2.7 for a polynomial permutation on Zp2 are satisfied. And for each such choice there are pp possibilities for the coefficients of f0 mod p2. The coefficients of f0 mod p2 and those of f1 mod p then determine the polynomial function mod p2. So |G2|=p!(p-1)ppp. The formula for |Gn| then follows from Corollary 2.10. □ This concludes our review of polynomial functions and polynomial permutations on Zpn. We will now introduce a homomorphic image of G2 whose Sylow p-groups bijectively correspond to the Sylow p-groups of Gn for any n⩾2. A group between G1 and G2 Into the projective system of monoids (Fn,∘) we insert an extra monoid E between F1 and F2 by means of monoid-epimorphisms θ:F2→E and ψ:E→F1 with ψθ=π1,F1←ψE←θF2←π2F3←π3⋯. The restrictions of θ to G2 and of ψ to the group of units H of E will be group-epimorphisms, so that we also insert an extra group H between G1 and G2 into the projective system of the Gi,G1←ψH←θG2←π2G3←π3…. In the following definition of E and H, f and f′ are just two different names for functions. The connection with polynomials and their formal derivatives suggested by the notation will appear when we define θ and ψ. Definition We define the semigroup (E,∘) byE={(f,f′)|f:Zp→Zpf′:Zp→Zp} (where f and f′ are just symbols) with law of composition (f,f′)∘(g,g′)=(f∘g,(f′∘g)⋅g′). Here (f∘g)(x)=f(g(x)) and ((f′∘g)⋅g′)(x)=f′(g(x))⋅g′(x). We denote by (H,∘) the group of units of E. The following facts are easy to verify: Lemma 3.1 (1) The identity element of E is (ι,1), with ι denoting the identity function on Zp and 1 the constant function 1. (2) The group of units of E has the formH={(f,f′)|f:Zp→Zp bijective, f′:Zp→Zp∖{0}}. (3) The inverse of (g,g′)∈H is(g,g′)-1=(g-1,1g′∘g-1), where g-1 is the inverse permutation of the permutation g and 1/a stands for the multiplicative inverse of a non-zero element a∈Zp, such that (1g′∘g-1)(x)=1g′(g-1(x)) means the multiplicative inverse in Zp∖{0} of g′(g-1(x)). Note that H is a semidirect product of (as the normal subgroup) a direct sum of p copies of the cyclic group of order p-1 and (as the complement acting on it) the symmetric group on p letters, Sp, acting on the direct sum by permuting its components. In combinatorics, one would call this a wreath product (designed to act on the left) of the abstract group Cp-1 by the permutation group Sp with its standard action on p letters. (Group theorists, however, have a narrower definition of wreath product, which is not applicable here.) Now for the homomorphisms θ and ψ. Definition We define ψ:E→F1 by ψ(f,f′)=f. As for θ:F2→E, given an element [g]p2∈F2, set θ([g]p2)=([g]p,[g′]p). θ is well defined by Lemma 2.8(1). Lemma 3.2 (i) θ:F2→E is a monoid-epimorphism. (ii) The inverse image of H under θ:F2→E is G2. (iii) The restriction of θ to G2 is a group-epimorphism θ:G2→H with |ker(θ)|=pp. (iv) ψ:E→F1 is a monoid-epimorphism and ψ restricted to H is a group-epimorphism ψ:H→G1. Proof (i) follows from Lemma 2.8(3) and (ii) from Fact 2.7. (iii) follows from Lemma 2.8(4). Finally, (iv) holds because every function on Zp is a polynomial function and every permutation of Zp is a polynomial permutation. □ Sylow subgroups of H We will first determine the Sylow p-groups of H. The Sylow p-groups of Gn for n⩾2 are obtained in the next section as the inverse images of the Sylow p-groups of H under the epimorphism Gn→H. Lemma 4.1 Let C0 be the subgroup of Sp generated by the p-cycle (012…p-1). Then one Sylow p-subgroup of H is S={(f,f′)∈H|f∈C0,f′=1}, where f′=1 means the constant function 1. The normalizer of S in H is NH(S)={(g,g′)|g∈NSp(C0), g′ a non-zero constant}. Proof As |H|=p!(p-1)p, and S is a subgroup of H of order p, S is a Sylow p-group of H. Conjugation of (f,f′)∈S by (g,g′)∈H (using the fact that f′=1) gives (g,g′)-1(f,f′)(g,g′)=(g-1,1g′∘g-1)(f∘g,g′)=(g-1∘f∘g,g′g′∘g-1∘f∘g). The first coordinate of (g,g′)-1(f,f′)(g,g′) being in C0 for all (f,f′)∈S is equivalent to g∈NSp(C0). The second coordinate of (g,g′)-1(f,f′)(g,g′) being the constant function 1 for all (f,f′)∈S is equivalent to ∀x∈Zp,g′(x)=g′(g-1(f(g(x)))), which is equivalent to g′ being constant on every cycle of g-1fg, which is equivalent to g′ being constant on Zp, since f can be chosen to be a p-cycle. □ Lemma 4.2 Another way of describing the normalizer of S in H is NH(S)={(g,g′)∈H|∃k≠0∀a,b,g(a)-g(b)=k(a-b); g′ a non-zero constant}. Therefore, |NH(S)|=p(p-1)2 and [H:NH(S)]=(p-1)!(p-1)p-2. Proof Let σ=(012…p-1) and g∈Sp then gσg-1=(g(0)g(1)g(2)…g(p-1)). Now g∈NSp(C0) if and only if, for some 1⩽k<p, gσg-1=σk, i.e., (g(0)g(1)g(2)…g(p-1))=(0k2k…(p-1)k), all numbers taken mod p. This is equivalent to g(x+1)=g(x)+k org(x+1)-g(x)=k and further equivalent to g(a)-g(b)=k(a-b). Thus k and g(0) determine g∈NSp(C0), and there are (p-1) choices for k and p choices for g(0). Together with the (p-1) choices for the non-zero constant g′ this makes p(p-1)2 elements of NH(S). □ Corollary 4.3 There are (p-1)!(p-1)p-2 Sylow p-subgroups of H. Theorem 4.4 The Sylow p-subgroups of H are in bijective correspondence with pairs (C,φ¯), where C is a cyclic subgroup of order p of Sp, φ:Zp→Zp∖{0} is a function and φ¯ is the class of φ with respect to the equivalence relation of multiplication by a non-zero constant. The subgroup corresponding to (C,φ¯) is S(C,φ¯)={(f,f′)∈H|f∈C,f′(x)=φ(f(x))φ(x)}. Proof Observe that each S(C,φ¯) is a subgroup of order p of H. Different pairs (C,φ¯) give rise to different groups: Suppose S(C,φ¯)=S(D,ψ¯). Then C=D and for all x∈Zp and for all f∈C we get φ(f(x))φ(x)=ψ(f(x))ψ(x). As C is transitive on Zp the latter condition is equivalent to ∀x,y∈Zpψ(x)φ(x)=ψ(y)φ(y), which means that φ=kψ for a non-zero k∈Zp. There are (p-2)! cyclic subgroups of order p of Sp, and (p-1)p-1 equivalence classes φ¯ of functions φ:Zp→Zp∖{0}. So the number of pairs (C,φ¯) equals (p-1)!(p-1)p-2, which is the number of Sylow p-groups of H, by the preceding corollary. □ Proposition 4.5 If p is an odd prime then the intersection of all Sylow p-subgroups of H is trivial, i.e., ⋂(C,φ¯)S(C,φ¯)={(ι,1)}. If p=2 then |H|=2 and the intersection of all Sylow 2-subgroups of H is H itself. Proof Let p be an odd prime, and let (f,f′)∈⋂(C,φ¯)S(C,φ¯). Suppose f is not the identity function and let k∈Zp such that f(k)≠k. Note that φ in (C,φ¯) is arbitrary, apart from the fact that 0 is not in the image. Therefore, and because p⩾3, among the various φ there occur functions ϑ and η with ϑ(k)=η(k) and ϑ(f(k))≠η(f(k)). Now (f,f′)∈S(D,ϑ¯)∩S(E,η¯) for any cyclic subgroups D and E of Sp of order p. Therefore ϑ(f(k))ϑ(k)=f′(k)=η(f(k))η(k), and hence ϑ(f(k))=η(f(k)), a contradiction. Thus f is the identity and therefore f′=1. If p=2 then |H|=2 and therefore the one and only Sylow 2-subgroup of H is H. □ In the case p⩾5, the lemma above can be proved in a simpler way: There is more than one cyclic group of order p, so for (f,f′)∈⋂(C,φ¯)S(C,φ¯), there are distinct cyclic groups D and E of order p with f∈D∩E. Therefore f has to be the identity. Sylow subgroups of Gn and of the projective limit Again we consider the projective system of finite groups G1←ψH←θG2←π2⋯←πn-1Gn←πn where (Gn,∘) is the group of polynomial permutations on Zpn (with respect to composition of functions) and H is the group defined in section 3. Let G=limGn be the projective limit of this system. Recall that a Sylow p-group of a pro-finite group is defined as a maximal group consisting of elements whose order in each of the finite groups in the projective system is a power of p. Theorem 5.1 (i) Let (Gn,∘) be the group of polynomial permutations on Zpn with respect to composition. If n⩾2 there are (p-1)!(p-1)p-2 Sylow p-groups of Gn. They are the inverse images of the Sylow p-groups of H (described in Theorem 4.4) under the canonical projection π:Gn→H, with π=θπ2…πn-1. (ii) Let G=limGn. There are (p-1)!(p-1)p-2 Sylow p-groups of G, which are the inverse images of the Sylow p-groups of H (described in Theorem 4.4) under the canonical projection π:G→H. Proof In the projective system G1←ψH←θG2←π2⋯←πn-1Gn the kernel of the group-epimorphism Gn→H is a finite p-group for every n⩾2, because for n⩾2 the kernel of πn:Gn+1→Gn is of order pβ(n+1) by Corollary 2.10 θ:G2→H is of order pp by Lemma 3.2(iii). So the Sylow p-groups of Gn for n⩾2 are just the inverse images of the Sylow p-groups of H and, likewise, the Sylow p-groups of the projective limit G are just the inverse images of the Sylow p-groups of H, whose number was determined in Corollary 4.3. □ If we combine this information with the description of the Sylow p-groups of H in Theorem 4.4 we get the following explicit description of the Sylow p-groups of Gn. Recall that [f]pn denotes the function induced on Zpn by the polynomial f in Z[x] (or in Zpm[x] for some m⩾n). Corollary 5.2 Let n⩾2. Let Gn be the group (with respect to composition) of polynomial permutations on Zpn. The Sylow p-groups of Gn are in bijective correspondence with pairs (C,φ¯), where C is a cyclic subgroup of order p of Sp, φ:Zp→Zp∖{0} is a function and φ¯ its class with respect to the equivalence relation of multiplication by a non-zero constant. The subgroup corresponding to (C,φ¯) is S(C,φ¯)={[f]pn∈Gn|[f]p∈C,[f′]p(x)=φ([f]p(x))φ(x)}. Example A particularly easy to describe Sylow p-group of Gn is the one corresponding to (C,φ) where φ is a constant function and C the subgroup of Sp generated by (012…p-1). It is the inverse image of S defined in Lemma 4.1 and it consists of the functions on Zpn induced by polynomials f such that the formal derivative f′ induces the constant function 1 on Zp and the function induced by f itself on Zp is a power of (012…p-1). Combining Theorem 5.1 with Proposition 4.5 we obtain the following description of the intersection of all Sylow p-groups of Gn for odd p. Corollary 5.3 Let p be an odd prime. (i) For n⩾2 the intersection of all Sylow p-groups of Gn is the kernel of the projection π:G→H. (ii) Likewise, the intersection of all Sylow p-groups of G is the kernel of the canonical epimorphism of G onto H. (iii) The intersection of all Sylow p-groups of Gn (n⩾2) can also be described as the normal subgroup N={[f]pn∈Gn|[f]p=ι, [f′]p=1}, where ι denotes the identity function on Zp. Its order is ppp∑k=3nβ(k) and its index in Gn (for n⩾2) is [Gn:N]=p!(p-1)p. (iv) Likewise, the index of the intersection of all Sylow p-subgroups of G in G is p!(p-1)p. Proof (i) and (ii) follow immediately from Theorem 5.1 and Proposition 4.5. To see (iii), let π be the projection from Gn to H (that is π=θπ2…πn-1). Then N is the inverse image of {(ι,1)}, the identity element of H, under π, and is therefore the intersection of the Sylow p-groups of Gn by (i). As the kernel of a group homomorphism, N is a normal subgroup. The order of N is the order of the kernel of π, which is the product of pp (the order of the kernel of θ) and pβ(k) (the order of the kernel of πk-1) for 3⩽k⩽n. Finally, the index of the kernel of the homomorphism of Gn or G onto H is the order of H which is p!(p-1)p. □ Acknowledgments The authors wish to thank W. Herfort for stimulating discussions.  ', 'Introduction The nightside ionosphere of Venus has a dynamic and complex structure (Brace et al., 1979). To date the most extensive set of in situ observations of the ionospheric plasma were obtained by Pioneer Venus Orbiter (PVO). Although the PVO mission covered an entire solar cycle the ionospheric measurements were largely restricted to a limited period close to solar maximum between 1978 and 1980 when the PVO periapsis was at a sufficiently low altitude to allow sampling of the ionosphere. The solar flux during this period was about 200 solar flux units (sfu). These PVO observations covered all local time sectors. In the nightside ionosphere they showed that precipitating electrons could contribute only ∼25% of the plasma densities observed and that changes in ionospheric densities were much more variable than, and not correlated with, changes in the flux of precipitating electrons (Spenner et al., 1981). Observations of the flux of atomic oxygen ions across the terminator from the day to nightside showed that this ion flux was sufficient to explain the observed ion densities in the nightside ionosphere at solar maximum (Knudsen et al., 1980). The ions were assumed to follow ballistic trajectories and theoretical calculations predicted that 80% of the ions that crossed the terminator had recombined with electrons before they reached a solar zenith angle (SZA) of 110°. Only those ions that crossed the terminator at the highest altitudes reached the central region of the nightside ionosphere. A modelling study by Cravens et al. (1983) predicted that ions which crossed the terminator at altitudes below 500km recombined before reaching a SZA of 120°, whilst ions that crossed the terminator at 876km influenced the entire night sector. Taken collectively these results showed that the primary source of the nightside ionosphere was plasma transport from the dayside. The plasma flow from the subsolar region toward the nightside is primarily driven by the day-to-night pressure gradient (Knudsen et al., 1981). Knudsen et al. (1982) showed that the flow speed across the terminator was highly variable but was typically several kilometres per second. The average value of the antisunward component of the velocity in the terminator region at solar maximum increased with altitude from a few hundred metres per second at an altitude of 150km to ∼4kms-1 at 800km (Knudsen and Miller, 1992). The altitude of the ionopause in the terminator region played an important role in the total number of ions transported from the day to the nightside. Its altitude in this region was variable (Elphic et al., 1980) but was typically around 1000km (Brace et al., 1983). This variability was attributed to changes in the solar EUV flux and the solar wind dynamic pressure, the balance of which altered the ionopause altitude (Knudsen and Miller, 1992). As the ionopause moved to lower altitudes the total number of ions transported antisunward was reduced (Knudsen et al., 1981). Theoretical calculations by Brace et al. (1995) showed that the transterminator flow could transport more ions antisunward than were required to maintain the nightside ionosphere and it was suggested that some of these ions might be lost to the solar wind. Limited in situ ionospheric observations aboard PVO were made in the pre-dawn sector at low latitudes in 1992 in the declining phase of solar cycle 22 under conditions of moderate solar flux (∼120sfu). The observed ion densities in this sector were significantly larger than those that would be expected in the absence of an antisunward ion flow. This suggested that ion transport was significant in this sector (Brannon et al., 1993). The PVO observations showed that the total transterminator flux was 23% of that at solar maximum and that the largest reductions in the number of ions transported antisunward occurred at the highest altitudes (Spenner et al., 1995). The PVO mission did not include in situ observations of the Venusian ionosphere around solar minimum, however the behaviour of the ionopause was inferred from PVO radio occultation profiles, for which the temporal data coverage was less extensive than for the in situ measurements. The ionopause was at significantly lower altitudes at solar minimum than at solar maximum, typically between 200km and 300km for all SZA (Kliore and Luhmann, 1991). The radio occultation profiles from PVO also showed that the transport process was severely inhibited (Knudsen et al., 1987). Radio occultation profiles from Venera 9 and 10 observed the ionopause at higher altitudes in the terminator region at solar minimum with altitudes between 600km and 800km (Gavrik and Samoznaev, 1987). The Venusian ionosphere exhibited a number of asymmetries between the dawn and dusk sectors. Brace et al. (1982) observed that the ionopause was higher on the dawn side than at dusk due to interaction with the solar wind. Miller and Knudsen (1987) reported larger antisunward velocities within the ionosphere on the dawn side than on the dusk side above an altitude of 400km, with the pattern reversed below this altitude. The dawn-dusk asymmetry below 400km was largely attributed to photoionisation as plasma in the post-noon sector had been exposed to sunlight for longer than plasma in the pre-noon sector. The plasma flow from the dayside to the nightside was driven by the day-to-night pressure gradient, with the higher plasma densities in the post-noon sector enhancing the nightward transport of ions on the dusk side. The super-rotation of the neutral atmosphere also enhanced the ion flow on the dusk side and reduced the flow on the dawn side due to collisional interactions between the ions and the neutral species. A subsequent modelling study at the altitude of the peak density in the ionosphere (∼140km) showed that differences in the thermospheric composition between the dawn and dusk sides may also cause asymmetries in the ionosphere at these altitudes due to changes in the dominant chemical reactions (Fox and Kasprzak, 2007). Between August 2008 and October 2009 Venus Express (VEX) was in an orbit with periapsis near 86°N and an altitude between 185km and 215km with about 10min spent in the ionosphere during each orbit. Taken collectively over many orbits the in situ ionospheric measurements cover all local time sectors, with each orbit sampling the terminator region at polar latitudes. In the current study these observations are used to determine the plasma distribution near the terminator and to show that the transport process contributes to the maintenance of the nightside ionosphere close to solar minimum. Instrumentation Venus EXpress (VEX) is the first European mission to Venus (Titov et al., 2006). The VEX spacecraft was inserted into a near polar orbit in April 2006 and so every orbit sampled the terminator region at polar latitudes. The Analyser of Space Plasmas and Energetic Atoms (ASPERA-4) package on VEX contains an ELectron Spectrometer (ELS), an Ion Mass Analyzer (IMA), a Neutral Particle Detector (NPD) and a Neutral Particle Imager (NPI) (Barabash et al., 2007). In August 2008 periapsis was lowered from an altitude of around 300km to 185km, allowing the spacecraft to sample deeper into the ionosphere. Observations made using the IMA sensor once this manoeuvre had occurred are of particular interest to the present study. This instrument observes the ion energy per charge, E/q, the mass per charge, m/q, and the arrival direction of each ion as well as the number of ions observed. It has a 360° instantaneous field of view in azimuth and ±45° field of view in elevation in the spacecraft frame of reference and an energy range of 10eV-30keV. The standard observing mode used during the period considered in this study was a scan in decreasing energy through 96 equal logarithmic steps, observing for 250ms at each. These measurements were made at all azimuths simultaneously at a given elevation. The elevation angle was varied through eight positions, which gave a total cycle time of 192s. Observations Data subsequent to the lowering of the periapsis of VEX were considered for the study. One Venus year of data were selected between 4th August 2008 and 17th March 2009 allowing the spacecraft to sample all local time sectors twice as it transited these sectors at high latitudes in opposite directions half a Venusian year apart. Periapsis was at 86°N during this interval. The ion counts as a function of energy observed by the IMA during a spacecraft transit between 04:30 UT and 06:30 UT on 9th August 2008 are shown on a logarithmic scale in the upper panel of Fig. 1. The ion counts as a function of mass channel number are shown in the lower panel of Fig. 1 with lower channel numbers corresponding to higher mass ions (Barabash et al., 2007). These data from 9th August 2008 are considered as an example to show how data from the entire year were selected and processed. The data in the lower panel show two clear ion populations; one with a higher ion mass per unit charge (lower mass channel number) observed between 05:28 UT and 05:47 UT and one with a lower ion mass per unit charge (higher mass channel number) observed before and after this time interval. Prior to 04:46 UT and after 06:03 UT the IMA observed ions with energies of some 300-800eV (Fig. 1, upper panel) with a low mass to charge ratio (high channel number in Fig. 1, lower panel) indicating that the spacecraft was in the solar wind. In the intervals from 04:46 UT to 05:28 UT and 05:47 UT to 06:03 UT the IMA sensor observed ions over a larger range of energies than observed in the solar wind, from some 200eV to 1keV with mass to charge ratios similar to that observed in the solar wind. These data suggested that the spacecraft was in the shocked solar wind, downstream of the bow shock. The observations closest to periapsis, between 05:28 UT and 05:47 UT, showed ions at energies below some 50eV with higher masses than those observed in the solar wind. These low energy ions were interpreted as being of planetary origin. Inspection of the datasets from a large number of orbits showed that it was convenient to locate the Ion Composition Boundary (ICB), which marks the transition between the shocked solar wind and the planetary plasma (e.g. Martinecz et al., 2008), by considering the mass channel number at which the largest number of ions was observed in each 192s cycle. Data from times at which the mass channel number of the maximum ion count was 15 or less were taken to correspond to altitudes below the ICB. These data were then considered for further analysis. For example, in the data set for 9th August 2008 shown in Fig. 1, the data between 05:28 UT and 05:47 UT were interpreted as being from inside the ICB. These data are shown within the pink box in Fig. 1, and it was these data that were considered for further analysis in this particular example. The spacecraft velocity at periapsis (∼200km) was ∼10kms-1, which was larger than the ion velocities of ∼3kms-1 observed by PVO at these altitudes (Knudsen and Miller, 1992). To ensure that the ions were detected, observations were only considered if the spacecraft ram direction was within the field-of-view of the IMA. This selection criterion meant that observations were only considered when the spacecraft attitude was suitable for observing the ions. The IMA observed in the ram direction for all, or part, of the time when VEX was within the ionosphere on 136 orbits, and data from this sub-set of orbits (136 orbits out of 226 orbits) was considered for further analysis. In this subset of 136 orbits, ions were observed at eight elevation angles during each cycle of 192s duration. For each cycle of each orbit the ion count at the elevation angle with the maximum ion count was found and considered further. Using the counts from this elevation the next step was to obtain the \"summed ion count\" for the cycle, defined as the total ion count summed over all energy levels below 100eV. Thus a value of the summed ion count was determined for each cycle. The duration of each complete cycle was 192s, however, the summed ion count corresponded to observations from only one of the eight elevations and only a proportion of the 96 energy levels, with the actual observations at one elevation angle and at energies below 100eV being conducted in 6.5s. During this time interval the spacecraft moved some 65km (6.5s times the satellite velocity of ∼10kms-1). Thus the summed ion count was observed over a horizontal distance of some 65km which is approximately 0.01 Rv where Rv is the radius of Venus (6052km). The summed ion counts for all cycles are plotted in Venus Solar Orbital (VSO) coordinates in Fig. 2. The positive x-axis is directed towards the Sun. The positive y-axis is orthogonally directed and opposite to the planetary orbital velocity i.e. towards dawn, which is opposite to the Earth due to the retrograde rotation of Venus. The largest summed ion counts were in the polar region close to periapsis where the spacecraft sampled the lowest altitudes. In this region the spacecraft was in the topside ionosphere, where the ion density decreases with increasing altitude. Data in Fig. 2 exhibit asymmetries in both the dawn-dusk and noon-midnight directions. To investigate the dawn-dusk asymmetry data were selected from a narrow region aligned with the dawn-dusk axis. This region was centred on the terminator and had a width of 0.4 Rv (the x coordinate was restricted to |x|<0.2 Rv). The observations in this region were then binned into intervals of 0.1 Rv in the dawn-dusk direction (y-direction) near the y=0 axis. The small number of points further from this axis required larger bins and an interval of 0.2 Rv was considered between |y|=0.3 Rv and 0.5 Rv and an interval of 0.25 Rv between |y|=0.5 Rv and 0.75 Rv. The median and quartile values of the ion counts in each bin are plotted in the upper panel of Fig. 3. A strong dawn-dusk asymmetry was observed, with the median counts larger on the dusk side than on the dawn side by almost an order of magnitude with median values of ∼6×105 on the dusk side and ∼5×104 around dawn. A similar plot for a noon-midnight narrow region is shown in the lower panel of Fig. 3 with a restriction that |y|<0.2 Rv. The observations were binned into intervals of 0.1 Rv between |x|=0.0 Rv and 0.3 Rv, 0.2 Rv between |x|=0.3 Rv and 0.5 Rv, 0.5 Rv between |x|=0.5 Rv and 1.0 Rv, and 1.0 Rv for -0.2 Rv<x<-1.0 Rv. This ensured sufficient numbers of points in each bin. A noon-midnight asymmetry is apparent, with larger median summed ion counts ∼3×105 in the noon sector. Variability is observed on the dayside where the counts are expected to decrease away from the terminator as the spacecraft moves to higher altitudes and to increase because of a decreasing solar zenith angle. The ion counts decrease rapidly on the midnight side to values of ∼5×104. However, the upper quartile showed that significant numbers of ions could be present nightward of the terminator (located at x=0) and that these values could be comparable to those on the dayside ionosphere with values as large as ∼8×105 recorded in both the day and night sectors. The summed ion counts considered in the preceding paragraphs were for energies less than 100eV. The energy level within this range at which the largest number of ions occurred during each cycle of 192s was determined. For each cycle, the energy of this level was then corrected for the spacecraft potential using the method of Coates et al. (2008) based on the analysis of the ionospheric photoelectron peaks, and the corrected value considered as the energy representative of the ions at the location of the spacecraft. To investigate ion flow in the terminator region an additional constraint was imposed to restrict observations to within ∼30° latitude of the pole. Periapsis was close to 86°N throughout the study period of one Venus year, and the restriction was done by considering only observations at an altitude of 500km or lower. The resulting data were then divided into four bins depending upon the direction of travel of the spacecraft;• Spacecraft travelling essentially from noon-to-midnight (within 45° of this direction); • Spacecraft travelling essentially from midnight-to-noon (within 45° of this direction); • Spacecraft travelling essentially from dawn-to-dusk (within 45° of this direction); • Spacecraft travelling essentially from dusk-to-dawn (within 45° of this direction). The spacecraft velocity at periapsis was essentially constant for all observations, with a mean value of (9.78±0.01)kms-1. For each bin the median value of the observed energy was determined. This was (11±3)eV for the noon-to-midnight bin and (20±4)eV for the midnight-to-noon bin, with the uncertainties set by the upper and lower quartiles. The larger ion energies in the midnight-to-noon bin suggested that these ions had a velocity component that was antiparallel to the spacecraft direction of travel and the smaller values in the noon-to-midnight bin suggested that these ions had a velocity component that was parallel to the spacecraft direction of travel. Taken together both of these observations suggest that the ions travelled in the noon-to-midnight direction. For both the dawn-to-dusk and dusk-to-dawn bins the energies were (18±4)eV. The difference in the ion energies of these bins was zero within the error margin, which suggested that there was no net ion flow in this direction. Discussion Results have been presented of ion counts and energies measured by the ASPERA-4 experiment onboard the VEX spacecraft as it traversed the Venusian ionosphere at polar latitudes. Strict selection criteria were applied to the data to ensure that the measurements used in the study were of ionospheric ions. Median ion energy values near the midnight-noon meridian were larger when the spacecraft traversed from midnight-to-noon than from noon-to-midnight. The larger values of the former case suggested that the ions had a velocity component that was antiparallel to the spacecraft direction of travel, while the smaller values of the noon-to-midnight traversal suggested that the ions had a velocity component parallel to the spacecraft direction of travel. This suggested the nightward transport of the ions at polar latitudes. Median values near the dawn-dusk meridian were identical for traversal from dawn-to-dusk and from dusk-to-dawn within the error margins suggesting that there was no net ion flow in this direction. Taken collectively the observed ion energies therefore indicated an ion flow predominantly in the noon-to-midnight direction. The spacecraft velocity near periapsis was essentially the same for all orbits and all directions of travel and so the difference in the ion energy between the midnight-to-noon and noon-to-midnight traversals, (9±7)eV, may be attributed to the flow of ions. By using the same assumption as Knudsen and Miller (1992) that the ions were primarily singly ionised oxygen, and that the measured energy difference was representative kinetic energy of the ions a nightward ion velocity of (2.5±1.5)kms-1 is estimated. It is appreciated that there are substantial uncertainties in this velocity and that the IMA was operating close to the lowest energies it could observe, however it is encouraging that this velocity is in broad agreement with Knudsen and Miller (1992) who reported antisunward ion flows of some ∼3kms-1 at these altitudes. A dawn-dusk asymmetry in the plasma distribution of the Venusian ionosphere has been reported by Miller and Knudsen (1987) with larger plasma densities observed in the dusk sector. Their study was conducted at low- and mid-latitudes around solar maximum, and the observation associated with the asymmetry of plasma transport where higher density plasma was drawn antisunward (nightward) from the post-noon sector as a transterminator flow. The dawn-dusk ion asymmetry in the current study (Fig. 3, upper panel) was consistent with their interpretation. The observed ion counts in the noon-midnight plane (Fig. 3, lower panel) suggested that the transterminator flow was highly variable. The median values of the three points immediately sunward of the terminator showed the largest values. The median values fell rapidly nightward of the terminator, as expected in the absence of a plasma source. The lower median value of ∼8×104 on the dayside at 0.4 Rv was a likely consequence of the spacecraft sampling at higher altitudes where the ion densities were expected to be lower. Indeed, sunward of 0.5 Rv no data points were recorded. This may be explained by the altitude of the ionopause falling to 200km-300km on the dayside (Kliore and Luhmann, 1991) and the spacecraft sampling above these altitudes when it was located ∼0.3 Rv sunward of the terminator. The upper quartile values varied substantially between adjacent bins. Upper quartile values in the nightside at a distance of less than 0.5 Rv from the terminator were similar to, or greater than, the median values on the dayside. This suggested that in a substantial number of cases the ion counts nightward of the terminator were comparable to the values in the dayside ionosphere, although in general the ion counts nightward of the terminator were lower than those observed on the dayside as expected in the absence of a plasma source. This indicated that, at times, a process was operating to maintain the nightside ionosphere although the occurrence of this process was highly variable. In summary the observations of ion energies indicated that a nightward ion flow across the terminator at solar minimum can occur. The ion counts show that such a flow is highly variable but the results indicate that it can contribute to the maintenance of the nightside ionosphere. Conclusions In situ ion observations made by the ASPREA-4 experiment onboard the Venus Express spacecraft at solar minimum have shown dawn-dusk and noon-midnight asymmetries. Ion energies observed when the spacecraft trajectory was directed midnight-to-noon were significantly higher than those observed when the trajectory was directed noon-to-midnight. This difference in ion energies suggested an antisunward transterminator flow with a velocity of (2.5±1.5)kms-1. It is suggested that this flow contributes to maintaining the nightside ionosphere near the terminator region at solar minimum. The interpretation of the antisunward flow was reinforced by observed asymmetries in the ion number counts. The dawn-dusk ion asymmetry showed larger numbers of ions on the dusk side than on the dawn side consistent with the previously reported observations of antisunward transterminator flow at solar maximum from PVO. For the noon-midnight asymmetry larger numbers of ions occurred on the dayside and there was substantial variability in the observations of counts on the nightside. In a substantial number of cases the number of ions nightward of the terminator was comparable to the number observed on the dayside. In other cases the number of ions nightward of the terminator was much lower, as expected in the absence of a plasma source. These observations suggested that the transterminator flow was highly variable and, in some cases, did not operate at all. Acknowledgements The authors would like to thank the ASPERA-4 team for their extensive work planning, constructing and operating these instruments on the Venus Express spacecraft, and the subsequent dissemination of these data. The assistance of Neville Shane from Mullard Space Science Laboratory, University College London in implementing software tools at Aberystwyth University is gratefully acknowledged. Financial support for this paper was provided by the UK Science and Technology Facilities Council under grant PP/E001157/1.  ', \"Introduction Epoxy polymers are widely used in many different engineering applications, such as coatings, adhesives and matrices in composite materials. For example, as coatings, such polymers are employed widely for applications requiring good ultra-violet light protection or high-scratch resistance. Their insulating properties, good temperature resistance and ease of processing also allow epoxy polymers to be used extensively in the electronics industry for applications in printed circuit boards and encapsulated electrical components [1]. Furthermore, the use of adhesive and composite materials based on epoxy polymers is widespread in the aerospace, automobile and wind-energy industries due to their structural efficiency [2,3]. Indeed, their outstanding temperature resistance and durability to weathering, fuel, de-icing fluids, etc. leads to them invariably being the preferred materials, compared to acrylics and polyurethanes, for external aerospace applications [1-3]. Epoxies are amorphous, highly cross-linked, thermosetting polymers which exhibit good elevated temperature resistance and low creep. However, their high cross-link density causes them to be relatively brittle polymers, and this limits their application as structural materials, as they have a poor resistance to the initiation and growth of cracks. Thus, improvements in their fracture performance are highly sought after by industry [3]. The addition of silica nanoparticles has been shown to improve these properties without adversely affecting the thermo-mechanical properties of the epoxy polymer [4-8]. Another advantage [5,6,8-10] is that due to their very small size, and hence large number, then a relatively low volume fraction of such nanoparticles can induce relatively extensive toughening of the epoxy polymer. Furthermore, the particles are sufficiently small such that when resin transfer moulding manufacturing processes are employed they are not filtered-out of the matrix by the fibre preforms when added to the matrices for fibre-reinforced composite materials [7,9]; where they improve both the fracture and fatigue resistance of the composite material. Johnsen et al. [10] ascertained that a major toughening mechanism arose from plastic void growth of the epoxy matrix polymer around debonded silica nanoparticles. Liang and Pearson [11] extended these ideas to show that plastic shear-banding in the epoxy matrix polymer also contributed to the toughening of such modified epoxy polymers. These toughening mechanisms were then implemented into a mathematical model proposed by Hsieh et al. [12,13] and such a model was used to predict successfully the fracture energy, GC, of epoxy polymers toughened via the addition of silica nanoparticles. Further, Giannakopoulos et al. [14] and Chen et al. [15] have shown that this theoretical model also applies to the toughening of epoxy polymers via rubbery core-shell nano-sized particles. Interestingly, Giannakopoulos et al. [14] also reported that, within experimental error, there was little effect of particle diameter on the increase in toughness resulting from the addition of the core-shell particles, within the range of 100-300 nm. In the present study, the Hsieh et al. [12,13] model will be used to predict the fracture energy of nanoparticle-modified epoxy polymers, where rigid, amorphous silica nanoparticles of three distinct sizes have been used, at various concentrations, to modify the epoxy polymer. Further, the previous work discussed above required high-resolution scanning-electron microscopy of the fracture surfaces to be undertaken after the fracture test had been conducted in order to identify the quantitative details of the toughening mechanisms that were required in the predictive mathematical model. The present work develops a new approach which enables the modelling results to be deduced from the basic properties of the polymer, i.e. before any fracture tests are undertaken. Experimental Materials The epoxy resin consisted of a standard diglycidyl ether of bisphenol A (DGEBA) (DER331 resin, Dow Chemical Company, USA) with an equivalent molecular weight of 187 g/mol. The three different sizes of silica nanoparticles were employed which possessed average particle diameters of 23 nm, 74 nm, and 170 nm, respectively, and were surface modified by an organosilane via a sol-gel process. They were supplied pre-mixed in a silica-DGEBA master-batch for each particle size by the 3M Company, USA. Piperidine (Sigma-Aldrich, USA) was used as the curing agent. The required volume fraction of silica nanoparticles was achieved by blending the silica-DGEBA master-batch with the pure DGEBA, mixing at 85 °C using a mechanical stirrer, and then degassing for 4 h. The silica nanoparticle-epoxy blend was then mixed with five weight percent of piperidine, degassed for a second time and then poured in a release-coated steel mould and cured at 160 °C for 6 h. As expected, the viscosity of the epoxy resin/curing agent mixture increased at higher loadings of silica nanoparticles. However, the basic epoxy resin/curing agent mixture possessed a relatively low initial viscosity and the increase in viscosity upon addition of the higher concentrations of silica nanoparticles was not considered to be an important aspect of the production of the cast sheets via pouring into, and then curing in, the steel mould. The same batch of material was used as for previous studies [16]. In Ref. [16] transmission electron microscopy images were given which showed that a good dispersion of silica nanoparticles in the epoxy polymer was achieved, although at the very highest concentration of silica nanoparticles a small degree of agglomeration was observed. The density of the composites was measured using a pycnometer and the calculated densities of the epoxy polymer and silica nanoparticles were 1.16 g/cm3 and 1.92 g/cm3, respectively. Using the measured densities, the volume fraction of the silica nanoparticles was calculated from the known weight percentages. This confirmed that the volume fraction of silica nanoparticles was as stated. A glass transition temperature of 80 °C was measured for the epoxy polymer using differential scanning calorimetry, and this value was unaffected by the addition of the silica nanoparticles [16]. Material characterisation The Young's modulus, E, and yield stress, σy, of the unmodified and silica nanoparticle-modified epoxies were measured using uniaxial tensile tests. The bulk polymer samples were machined into a dog-bone shape with dimensions of 63.5 mm long by 3 mm thick, and 3 mm wide in the gauge section. They were tested at a constant displacement rate of 5 mm/min at room temperature, according to the ASTM-D638 (Type V) standard test method [17], with a minimum of five replicate samples per material type. It should be noted that since the present epoxy polymers are all relatively brittle materials it was not possible to obtain meaningful values of the strain to break from uniaxial tensile tests: any such data would be very dependent upon the sample preparation technique employed and will inevitably exhibit a relatively high degree of scatter. Indeed, for these reasons plane-strain compression tests have been undertaken of the unmodified epoxy polymer to ascertain the overall yield behaviour of the material, since as expected it failed around the yield point when uniaxial tensile tests were undertaken. The plane-strain compression tests were conducted as described previously [13]. The fracture toughness, KC, was measured using a single-edge notch bend (SENB) test, in accordance with the ASTM-D5045 standard [18]. Sample dimensions of 75.6 mm × 12.7 mm × 6.36 mm and a constant displacement rate of 1 mm/min were used. A pre-crack was made by lightly tapping a fresh razor blade into the machined notch, yielding a very sharp natural crack tip. The mean and standard deviation values of the fracture toughness were ascertained, using a minimum of five replicate samples for each material. The fracture energy, GC, was calculated from the values of the fracture toughness, Poisson's ratio and Young's modulus [19]. The fracture surfaces of the SENB samples were studied using scanning electron microscopy. High-resolution scanning-electron microscopy was performed using an electron microscope equipped with a field-emission gun (FEG-SEM); a Carl Zeiss Leo 1525 with a Gemini column was used, with a typical accelerating voltage of 5 kV. All samples were coated with an approximately 5 nm thick layer of chromium before imaging. The FEG-SEM images were used to study the debonding and any subsequent plastic void growth of the polymer. The fraction of silica nanoparticles that debonded during the fracture process was estimated from the images to compare with the results of the predictive model. The FEG-SEM images were overlaid with an evenly spaced grid. Next, each cell was analysed, and each particle identified. Via standard stereology, the area fraction of particles in the image and volume fraction of the silica nanoparticle modified epoxy were assumed to be equal, ensuring that almost all nanoparticles on the fracture surface were considered in the analysis. Thereafter, the fraction of particles that debonded with subsequent void growth were obtained by zooming into the image, and the diameters of the voids measured. To ensure that the appropriate number of silica nanoparticles were included in the analysis, the area fraction of such particles was measured and compared with the known volume fraction of the particles. The subsurface damage in the tested SENB samples was studied using transmitted-light optical microscopy (TOM). Cross-sections were cut from the fracture surfaces, then ground and polished using standard petrographic techniques to approximately 100 μm thick. These thin sections were examined under bright field and cross-polarised light using an Olympus model BH2 optical microscope. Results Mechanical properties The values of the tensile Young's modulus that were measured are shown in Table 1. A value of E = 3.50 GPa was measured for the unmodified epoxy polymer. The modulus was found to increase steadily with the silica nanoparticle content due to the much higher modulus of the silica particles (i.e. E = 70 GPa) compared with the polymer. There was no effect of the particle size, as expected [16]. The yield stress, σy, of the unmodified epoxy was measured to be 85 MPa. The addition of the nanoparticles was found to reduce the yield stress slightly and a minimum value of 78 MPa was recorded as shown in Table 1, and again there was no significant effect of particle size. (It should be noted that, as explained in detail below in Section 4.2, strong interfacial adhesion leads to matrix yielding whilst decreased particle-matrix interaction leads to debonding with a corresponding dependence of the yield stress on the particle volume fraction. Thus, the reason for the observed slight reduction of the tensile yield stress upon addition of the silica nanoparticles is discussed below.) Fracture energy The values of the measured fracture toughness, KC, and fracture energy, GC, for the epoxy polymers are listed in Table 1, and the values of the fracture energy are plotted as a function of the volume fraction of silica nanoparticles in Fig. 1. A value of GC = 303 J/m2 was measured for the unmodified epoxy, and this value is in good agreement with values previously reported in the literature [11]. The addition of the silica nanoparticles increased the values of the toughness, and the increase is approximately linear after an initial relatively steep increase at 2.5 vol%. The effect of the presence of the silica nanoparticles is clearly major, with the epoxy polymers containing 30 vol% of such particles having values of GC of about 1000 J/m2. As discussed in detail previously [16], there is no significant effect of the particle size, within experimental error, on the measured values of the fracture energy with the addition of silica nanoparticles, within the range studied of 23-170 nm in particle diameter (see Fig. 1). This observation also agrees with previous work, which showed no effect of particle diameter between 20 and 80 nm [11]. Toughening micromechanisms Fractography-transmission optical microscopy The toughening mechanism due to the formation of extensive localised shear-band yielding has been previously established for epoxy polymers containing silica particles, when using both nanoparticles and micrometre-sized particles [13]. The presence of plastic shear-band yielding in the present epoxy matrix polymers containing silica nanoparticles was confirmed using transmission optical microscopy. Examples of such micrographs are illustrated in Fig. 2, which shows both bright-field and crossed-polarised optical-light images. The horizontal line across the centre of the image is the fracture surface, and the subsurface damage is on the bottom half of each image. The dark lines on the micrographs on the left-hand side of Fig. 2, i.e. the bright-field images, are dilatational bands caused by the stress concentrations around the silica nanoparticles. Birefringence is observed in the micrographs on the left-hand side of Fig. 2, i.e. the dark-field images, as bright lines and a white-region. This indicates the presence of shear yielding (i.e. plastic shear banding) in the epoxy matrix polymer, as the plastic shear deformation causes orientation of the polymer molecules which rotates the plane of the polarised light and leads to a bright image. Fractography-FEG-SEM The process zone region of the fracture surfaces was examined using field-emission gun scanning-electron microscopy (FEG-SEM) to find evidence of any debonding and subsequent plastic void growth of the epoxy matrix polymer. Fig. 3 shows micrographs of the fracture surfaces for the epoxies modified with 10 vol% of silica nanoparticles for the three particle diameters that were studied. Some single nanoparticles are identified with arrows, and evidence to support debonding and subsequent void growth in the epoxy polymer are shown circled. As explained in detail above, to analyse such micrographs the FEG-SEM images were overlaid with an evenly spaced grid. Next, each cell was analysed, and each particle identified. Via standard stereology, the area fraction of particles in the image and volume fraction of the silica nanoparticle modified epoxy were assumed to be equal, ensuring that almost all nanoparticles on the fracture surface were considered in the analysis. The smallest particles employed, i.e. silica nanoparticles of 23 nm diameter, showed no evidence of debonding and subsequent void growth of the epoxy polymer. Now, Johnsen et al. [10] reported the diameter of the void growth surrounding the 20 nm silica nanoparticles in a somewhat different epoxy polymer that they studied as ∼30 nm, via using atomic force microscopy; and a diameter of void growth in the range of 30-35 nm diameter was measured from the FEG-SEM images. However, more recently, Hsieh et al. [13] have demonstrated that when the adhesion between the silica nanoparticle and the epoxy polymer is sufficiently high, then debonding of the silica particles does not occur. In the present work, it was relatively difficult to clearly observe the 23 nm silica nanoparticles, see Fig. 3(a), although some particles were observed, as indicated by the arrows. However, there was no evidence of particle debonding and subsequent plastic void growth in the epoxy polymer, and this of course explains why the particles are more difficult to identify. It can be argued that the coating used to make the fracture surfaces of the samples conductive has obscured the evidence of debonding from the matrix, and the subsequent void growth of the epoxy polymer. Therefore, several studies were undertaken to change the thickness of the chromium coating, but there was still no evidence of plastic void growth having occurred. However, the modelling work discussed below reveals that debonding and void growth would be expected. This proposal is supported, of course, by the relatively high values of the fracture energies that were measured, see Table 1 and Fig. 1, and which are not significantly different from the values for the two larger particle sizes. The 74 nm silica nanoparticles were readily identifiable on the fracture surfaces, see Fig. 3(b). The sizes of some of the particles were measured and a mean particle diameter of 76 nm was obtained, again agreeing well with the expected particle diameter. The fracture surfaces show evidence of debonding with subsequent plastic void growth of the epoxy polymer, as shown circled in Fig. 3(b). However, only some of the 74 nm diameter particles show evidence of such debonding and void growth. Dittanet and Pearson [16] reported from their studies that about 10% of the particles present resulted in the debonding and void growth process for this silica nanoparticle-modified epoxy polymer. In the present work, for this same epoxy polymer, debonding and subsequent void growth were found to be associated with about 10-15% of the particles present. The 170 nm diameter particles were relatively easy to identify on the fracture surfaces, see Fig. 3(c). A mean particle diameter of 160 nm was measured from the micrographs, which is in good agreement with the expected diameter when considering the experimental errors associated with such measurements. For the 170 nm diameter nanoparticle material, evidence of debonding and void growth could be identified from the micrographs. As for the 74 nm particles, debonding and subsequent void growth were found to be associated with about 10-15% of the 170 nm particles present. Again this is in good agreement with the earlier assessment [16] of about 10% for this modified epoxy polymer. The diameter of the voids was measured, and was found to correlate well with the calculated value that may be deduced from Ref. [12]: (1)rpv=(1+γf)rp where the radius of a particle void, rpv, may be deduced from the maximum hoop strain that a void could sustain before fracture in the polymer, and is a function of the plane-strain true fracture strain, γf, and particle radius, rp. For example, in the epoxy system in this study, γf = 0.71, see Table 2. Thus, the predicted diameter of the voids associated with the 170 nm particles is about 290 nm. From the examined images, the void size was measured to be 250 ± 80 nm, which agrees well with the prediction. It is noteworthy that the large variation in the mean value is attributed to the distribution in the sizes of the voids that were observed. Modelling studies Modelling the toughening mechanisms The mechanisms of shear band yielding and plastic void growth have been successfully modelled by Huang and Kinloch [20] for rubber-modified epoxy polymers, and more recently by Hsieh et al. [12,13] for silica nanoparticle-modified DGEBA epoxy polymers. Giannakopoulos et al. [14] and Chen et al. [15] have also applied this model to epoxy polymers toughened using core-shell rubbers. Huang and Kinloch [20] proposed a generalised solution to examine incremental increases in GC, where: (2)GC=GCU+ψ where GCU is the fracture energy of the unmodified epoxy polymer and Ψ represents the overall toughening contribution provided by the presence of the particulate phase, such that: (3)ψ=ΔGs+ΔGvHere the toughening increment due to the silica nanoparticles, termed Ψ, is a combination of the two mechanisms identified from the experimental work, and can be separated into their relative toughening contributions. These contributions are (i) the formation of localised plastic shear-band yielding, ΔGs, in the epoxy matrix polymer which is initiated by the silica nanoparticles, and (ii) debonding of the silica nanoparticles followed by plastic void growth, ΔGv, of the epoxy matrix polymer. It should be noted that the contribution to the increase in toughness due to particle debonding is widely considered to be negligible and thus the value of interfacial free energy between the silica nanoparticles and epoxy polymer matrix does not enter into the formulation of the model [21-23]. However, although the energy absorbed by debonding is small, the process of debonding is vital for plastic void growth of the epoxy polymer to occur. Ideally, whether or not the particles debond should be predicted prior to undertaking fracture tests, rather than relying upon analysis of the fracture surfaces after fracture testing. Therefore, it is necessary to consider firstly the role of the adhesion of the particles to the matrix. Adhesion of the particles to the matrix Pukánsky and Vörös [24-26] showed that very different levels of particle to matrix interfacial adhesion could be obtained with particle-filled polymers, including glass-particle modified epoxy polymers. Also, they showed that the degree of adhesion has a marked effect on the observed yield stress at different volume fractions, as was also shown by Dekkers and Heikens [27]. The work of Pukánsky and Vörös [24-26] focused on using stress-averaging principles to develop predictive models for the variation of the yield stress as a function of the volume fraction of glass particles. The general trend in their work was in good agreement to that of Vollenberg et al. [28-31] and Fu et al. [32]. These studies revealed that (i) smaller particles generated relatively higher values of the yield stress for a given level of particle-matrix adhesion and that (ii) strong interfacial adhesion leads to matrix yielding, whilst decreased particle-matrix interaction leads to debonding with a corresponding dependence of the yield stress on the particle volume fraction. Further, these authors reported that the interphase properties, the degree of interfacial adhesion and the particle size determined the stress necessary to separate the particle-matrix interface. Indeed, Pukánsky and Vörös extended their earlier work to include the interphase properties that surround the particles [33], recently reviewed in Ref. [34]. This is relevant with respect to the work of Zhang et al. [35] who have predicted the formation of an interphase in silica nanoparticle-modified epoxies, but were unable to support their hypothesis with experimental evidence. On the other hand, Sen et al. [36] reported the formation of an interphase around silica nanoparticles in their modified polystyrene using small-angle neutron-scattering experiments. Considering the level of interfacial adhesion, Kawaguchi and Pearson [37,38] varied the adhesion in glass-bead filled epoxies by using an adhesion promoter, i.e. aminopropyltrimethoxysilane, to coat the glass beads and found that better adhesion resulted in higher values of the yield stress in their modified epoxy polymers. Many researchers, for example Gent [39], Nicholson [40] and more recently, Chen et al. [41,42] and Williams [43] have reported a strong dependence of debonding stress on the particle size, with other notable studies reported in Refs. [24,25,32,44]. For example, the work of Chen et al. suggests that the debonding stress is relatively high for particles in the nanometre size range. As indicated above, the work of Pukánsky and Vörös [24-26] may be used to semi-quantitatively evaluate the interfacial adhesion between the silica nanoparticles and the epoxy matrix polymer. Now, since the reversible work of adhesion for the various particle-epoxy interfaces is unknown, they proposed a simple model to quantify the interfacial strength. Assuming that the particles carry a load proportional to their volume fraction, Pukánszky and Vörös [24,25] proposed that:(4)σe=vfkσe+(1-vf)σm where the applied stress acting on the modified polymer, σe is a function of the volume fraction, vf, of particles, the proportionality constant, k, for stress transfer between the particles and the matrix, and the average stress in the matrix, σm. The first term expresses the stress carried by the particles, with the second expressing the stress in the matrix; i.e. if there are no particles present then the applied stress is equal to the average matrix stress. This can be simplified further by taking σm to be the yield stress of the matrix, which is expressed for the unmodified epoxy in the equation below by σyu. Hence: (5)σe=σyu(1-vf)(1-kvf) where σe is now the applied stress needed to produce yielding in the modified polymer. The magnitude of k was reported to be greater than 0 for rigid particles and to increase with the level of particle-matrix adhesion. (No maximum can be given to the value of k because this is entirely dependent on the interphase region that forms between the particles and matrix. The value of k = 0 for the assumption of voids present in the epoxy would provide the lower limit to the model.) Plots of the normalised yield stress of the modified epoxy polymers (i.e. normalised relative to the unmodified epoxy) versus the volume fraction, vf, of the silica nanoparticles in the different epoxy polymers are shown in Fig. 4. The lines represent the predictions of the model of Vörös and Pukánszky [24,25] using the values of the interfacial parameter, k, as stated. Firstly, the values for the three piperidine-cured epoxy polymers, containing particle sizes of 23, 74 or 170 nm, clearly show that the plots for the three particle sizes lie close to one another, and thus the respective values of k are in good agreement. This reveals that, within the range of particle sizes that were studied, there is no effect of the particle size on the degree of adhesion between the particles to the matrix. Secondly, the data from the present work fits within the region of values of k, and hence the adhesion levels, where debonding and void growth were observed previously in other nanosilica-modified epoxy polymers by Hsieh et al. [13]. Hence, from both of these observations, it would be expected that all three piperidine-cured epoxy polymers, containing particle sizes of 23, 74 or 170 nm, examined in the present study will exhibit particle debonding, followed by plastic void growth of the epoxy matrix polymer, prior to fracture. Thus, it is predicted that the toughening mechanisms of plastic shear-band yielding and debonding followed by plastic void growth will both occur for all the materials studied in the present work. Modelling shear-band yielding The energy contribution from plastic shear-band yielding, ΔGs, initiated by the presence of the particles is related to the size of the plastic zone from Ref. [12] by: (6)ΔGs=0.5vfσycγfF′(ry) where vf is the volume fraction of the silica nanoparticles, σyc is the plane-strain compressive true yield stress, and γf is the true fracture strain for the unmodified epoxy, see Table 2. The F′(ry) term takes a modified form of the original formulation of the model, from Ref. [13], to be: (7)F′(ry)=ry[(4π3vf)1/3(1-rpry)3-85(1-rpry)(rpry)5/2-1635(rpry)7/2-2(1-rpry)2+1635] where the rp is the radius of the particle and ry is the radius of the plane-strain plastic zone at the crack tip at fracture in the nanoparticle-modified polymer. The value of ry is given by: (8)ry=Kp2(1+μm31/2)2rpz where Kp is the maximum stress concentration for the von Mises stresses around a rigid particle, and μm is a material constant which allows for the pressure-dependency of the yield stress. The value of μm was shown by Sultan and McGarry [45] to be in the range from 0.175 to 0.225 (taken as 0.2). The value of Kp is dependent on the volume fraction of particles, and was calculated from the data of Guild and Young [46]. The value of Kp varies from approximately 1.65-1.85 for the range of volume fractions used in the present work. The value of rpz, the Irwin prediction of the plane-strain plastic zone radius for the unmodified epoxy at fracture, was calculated from Ref. [47] as: (9)rpz=16πKCU2σyt2 where KCU is the fracture toughness and σyt is the tensile yield stress for the unmodified epoxy polymer. It should be noted that, via equation (7), the shear banding term is dependent on particle size, i.e. smaller particles provide a greater contribution to the value of ΔGs. Modelling plastic void growth Although the energy contribution from debonding is considered to be negligible, particle debonding is of great importance since this reduces the constraint at the crack-tip and allows the epoxy matrix polymer to deform plastically via void growth mechanisms. The contribution of ΔGv via the plastic void growth mechanism, assuming that 100% of the particles present debond, was taken from Ref. [20] as: (10)ΔGv=(1-μm23)(vfv-vf)σycrpzKv2 where μm is a material constant which allows for the pressure-dependency of the yield stress [45] and was taken to be 0.2, vfv and vf are the volume fraction of voids and the volume fraction of silica nanoparticles. The value of vfv was calculated from a void radius, rpv, of (1 + γf)rp [12], i.e. based upon the maximum hoop strain that a shell void could sustain, see Table 2. The value of Kv is taken as the von Mises stress concentration factor for voids from the work of Guild and Young [48]. The value of Kv was allowed to vary linearly between 2.11 and 2.14 for the volume fractions considered in the present study. Now, as written it should be noted that equation (8) assumes that 100% of the silica nanoparticles present will debond, and hence allow plastic void growth of the matrix to occur. However, it is very significant that Hsieh et al. [12,13] observed that only 15 ± 5% of the silica nanoparticles present actually debonded and so resulted in plastic void growth of the epoxy polymer, and this observation was independent of the epoxy matrix used. However, to obtain this value it was necessary to undertake fracture tests on all the different materials and then to examine in detail their fracture surfaces. However, for the model to be fully predictive it would be ideal to predict a priori the proportion of the nanoparticles present that will debond, without reference to the fracture surfaces obtained after the fracture tests have been completed. Predicting debonding of the silica nanoparticles The effect of debonding of a nanoparticle on the local stress-field was investigated using finite-element analysis, employing 'Abaqus version 6.12, Implicit'. The mesh was drawn in two-dimensions using a single layer of elements to allow the application of a pure hydrostatic stress. The mesh is shown in Fig. 5, with one particle debonded at A and the particles are arranged so that the nearest inter-particle distances are equal. This mesh represents 13.7 vol% of particles, which is equivalent to 20 wt% of particles. The relative size of the nanoparticles was calculated assuming that the two-dimensional area fraction is equal to the three-dimensional volume fraction, see above. Around the void, the particles are identified by numbers. Mirror boundary conditions were imposed on each edge of the mesh, with stress and constraining equal displacements imposed on edges BC and CD, and with edges AB and DA constrained in the orthogonal direction. The bottom plane of the mesh was constrained in the orthogonal direction, with stress and constraining equal displacements imposed on the top surface. Analyses were also undertaken with all the particles present. Elastic material properties were assumed for the silica nanoparticles and elastic-plastic properties, matched to the experimental stress-strain results, were used for the epoxy matrix polymer. The mesh shown in Fig. 5 is a deformed mesh, which is clear from the relative size of the void and the silica nanoparticles. Essentially, it was found that the growth of the void alters the stress-state around the surrounding nanoparticles. An energy-based criterion was used to predict debonding of the particles. The method used has been fully described elsewhere [49] and essentially it proposes that the criterion for debonding is based upon the energy released by the debonding process. To obtain the parameters needed for this energy-based criterion, a finite-element analysis modelling study has been used to derive the change in strain-energy arising from the cavitation process, with the addition of the strain-energy stored in the particle prior to debonding. The applied stress used for these simulations was derived from experimental observations. Namely, as implied above, the debonding of the silica nanoparticles from the epoxy matrix polymer appears to take place during the elastic deformation region and, as shown in Table 1, the yield stress for all modified epoxy polymers is approximately equal, irrespective of particle size. It has therefore been assumed that the debonding takes place at an applied uniaxial stress of about 70 MPa, which equates to a hydrostatic stress at the crack tip of about 210 MPa. Thus, the finite-element analysis simulations were analysed for an applied hydrostatic stress of 210 MPa. The results are shown in Fig. 6 where values of the energy required for debonding have been extracted for five of the numbered particles shown in Fig. 5, and they are also compared with the value for the isolated particle extracted from the analyses without the void. The results show that the values of energy required to debond the particles closest to the void (caused by the debonded particle at A), i.e. particles numbered 1 and 2, are significantly higher than that required to debond an isolated particle. Hence, these particles are shielded from debonding by the presence of the void. Results for the more distant particles (i.e. numbered 3, 4 and 6) show that the energies for debonding are all significantly closer in value to the energy required for the isolated particle. Indeed, the value of energy required to debond particle 6 is almost identical to the value required for an isolated particle. (The energy required to debond particle 5 will be very similar to that of particle 6.) The energy values shown in Fig. 6 are identical for all particle sizes, since they are normalised. However, since the value of energy is proportional to the volume of the particle, the actual energy values for the 174 nm diameter particle are more than 400 times the values for the 23 nm diameter particle. Further work regarding this debonding process will include 3-dimensional analysis and investigation of the criteria for nanoparticle debonding which is expected to be size dependent [49,50]. Now, the arrangement in Fig. 5 is idealised, but it shows that the nearest neighbours to the void are shielded from debonding. For a random distribution, the mean inter-particle distance would be a value between the distance from the void to particles 1 and 2, but Fig. 6 shows that the energy required to debond these particles is much higher than for particles further away from the void. Hence, it is now necessary to generalise these findings to a random distribution of particles, where each nanoparticle is surrounded by a set of other nanoparticles which can be described as its nearest neighbours but without prescribing the inter-particle distance or the arrangement of the particles. This will allow the number of nearest neighbours that a particle possesses to be determined, and hence the percentage of particles which are expected to debond to be calculated. This can be undertaken by considering previous work on the quantification of the dispersion of nanoparticles [51,52], as discussed below. Calculating the percentage of debonded silica nanoparticles Now, each nanoparticle is surrounded by a set of other nanoparticles which can be described as its nearest neighbours. A Voronoi tessellation [51,52] of the material, based around the positions of the nanoparticles, provides a method for deciding which particles are the nearest neighbours, as shown in Fig. 7. This tessellation breaks the material into a set of space-filling convex polygons around each particle, where any position within a polygon is closest to the engulfed nanoparticle. Hence, each Voronoi polygon, and its associated nanoparticle, is completely bordered by other polygons and the particles contained within the surrounding polygons are defined as the nearest neighbours. To calculate the total number of nearest neighbours around a nanoparticle, a dual representation to the Voronoi tessellation, termed a Delaunay network, is calculated. Here the vertices of each Delaunay triangle lie on nanoparticles, and each edge crosses over exactly one boundary between neighbouring Voronoi polygons. A nanoparticle is connected to one of its nearest neighbours through a triangle edge. Hence, the number of nearest neighbours of a nanoparticle is equivalent to the number of Delaunay triangles that contain that nanoparticle as a vertex, shown as the shaded triangles in Fig. 7. By this definition the mean number of neighbours is equivalent to the mean number of Delaunay triangles per unique vertex (particle). Thus, for a micrograph containing N dispersed particles, there will be N Voronoi polygons, i.e. one for each particle. When the Delaunay tessellation is generated there must be 2N Delaunay triangles in the micrograph. As there are three vertices per triangle then the total number of triangle vertices is 3 × 2N = 6N. Now, each particle is a potential position for a vertex, and multiple vertices can lie on a particle. Thus, the mean number of vertices on one particle must be equal to the total number of vertices divided by the number of particles = 6N/N = 6. As each vertex connects the particle with its nearest neighbours then the number of nearest neighbours is exactly six. This above methodology reveals that once one particle debonds and void growth occurs, then its six nearest neighbours will not debond. Hence only one in seven particles will exhibit debonding and void growth, which gives a value of the percentage of debonding particles as 14.3% of those present. This prediction agrees very well with the experimental observations from the fracture surfaces. Indeed, Hsieh et al. reported that in their studies the experimental observations indicated that 15 ± 5% of the nanoparticles present exhibited debonding and void growth [12,13] and, in the current work, 10-15% of the nanoparticles were observed to show debonding and void growth. The results from both of these studies clearly agree very well with the above prediction of 14.3%. Comparisons between the predicted and experimental toughness The modelling scheme outlined above was employed to calculate the values of ΔGs and ΔGv using the parameters given in Table 2. The results are compared with the measured fracture energies in Fig. 8. In this Figure the modelling results are shown as lines considering the contribution to the fracture energy, Gc, due to (i) plastic shear-band yielding in the epoxy matrix polymer only and (ii) plastic shear-band yielding and plastic void growth in the epoxy matrix polymer but arising from only 14.3% of those nanoparticles present being active in terms of initiating this latter debonding and the plastic void growth toughening mechanism. The individual contributions to the toughness from shear-band yielding and plastic void growth of the epoxy matrix polymer are shown in Table 3, where it again should be noted that the predicted values of GC assume that the toughening increment, ΔGv, from the plastic void growth mechanism arises from only 14.3% of the particles present. Fig. 8 shows that the model which considers both toughening mechanisms generally under-predicts the measured fracture energy at low particle contents, and may somewhat over-predict the toughness at high volume fractions. However, for all of the different particle diameters, a good agreement with the experimental data was obtained using the model that takes into account both the shear-banding yielding and the debonding and subsequent plastic void growth mechanisms in the epoxy matrix polymer, when the predicted value of about 14% of the nanoparticles present only are active in the latter toughening mechanism. Thus, the modelling, as well as the experimental studies, clearly confirm the important role of debonding of the nanoparticles which enables subsequent plastic void growth to occur in the epoxy matrix polymer; and the fact that it is only initiated and occurs via a relatively low percentage of the nanoparticles present due to stress-shielding of those nanoparticles immediately adjacent to a void. As noted above, the shear-banding term in the model is dependent on particle size, i.e. smaller particles provide a greater contribution to the value of ΔGs, whereas the plastic void growth term, ΔGv, is independent of particle size. However, Table 3 shows that the ΔGs contributions for the three particle diameters studied and there is no significant difference between the values for any given volume fraction of particles. For example, for 30 vol% of nanosilica particles, ΔGs = 300 J/m2 for the 23 nm diameter particles, 299 J/m2 for the 74 nm diameter particles, and 297 J/m2 for the 170 nm diameter particles. Hence, over the range of particle sizes used in the present work the model predicts that there is no significant effect of particle diameter on the predicted toughness. This agrees very well with the experimental data, where no size effect was seen. Finally, for the 23 nm diameter particles, it is unlikely that all the observed toughening effect could arise from shear yielding alone. Since there is no significant difference in the measured GC for any particle diameter for a given volume fraction of silica nanoparticles; and noting that the other two particle size particles experimentally, as well as theoretically, showed debonding and void growth. Thus, also observing that the contribution of the plastic void growth mechanism to the fracture energy, ΔGv, is predicted to be independent of particle size, it is suggested that particle debonding and subsequent plastic void growth in the epoxy polymer does indeed occur for the 23 nm diameter particles but that it is obscured by the coating process when the FEG-SEM observations are made. This confirms that it is very useful to be able to predict the toughening mechanisms associated with such small particles, rather than relying solely upon observations of the fracture surfaces. Conclusions Silica nanoparticles possessing three different diameters (23, 74 and 170 nm) were used to modify a piperidine-cured epoxy polymer. Fracture tests were performed and the values of the toughness increased steadily as the concentration of silica-nanoparticles was increased, but with no significant effects of particle size being observed. The toughening mechanisms were identified as (i) the formation of localised shear-band yielding in the epoxy matrix polymer which is initiated by the silica nanoparticles, and (ii) debonding of the silica nanoparticles followed by plastic void growth of the epoxy matrix polymer. These toughening mechanisms, and hence the toughness of the epoxy polymers containing the silica nanoparticles, were modelled. However, previously, the percentage of nanoparticles that actually initiate this latter mechanism of debonding and subsequent plastic void growth of the epoxy matrix polymer, was calculated by observations from the fracture surfaces after the fracture test had been performed. The present paper has obtained the value of this important parameter via firstly developing a finite-element model of a number of particles in the epoxy polymer. This model showed that once one silica nanoparticle debonds and forms a void, then its nearest neighbours are shielded from the applied stress-field and hence will not debond. A statistical analysis, using Delaunay triangles, was then employed which revealed that, for a random dispersion of nanoparticles, each nanoparticle has exactly six nearest neighbours, so only one in seven particles will debond. This predicted value of 14.3% of the particles present that will actually debond, and hence lead to subsequent plastic void growth in the epoxy matrix polymer, was in excellent agreement with the observation from the fracture surfaces that about 10-15% of the nanoparticles present debonded. Indeed, this value of about 15% only of the silica nanoparticles particles present debonding has also been noted in other published studies, but has never been previously explained. Thus, the predictions from the modelling studies of the toughness may now be undertaken without the need for any fracture tests to be first conducted, i.e. a priori from the basic material properties of the modified epoxy polymers. The predicted fracture energies of the various epoxy polymers containing the silica nanoparticles were so deduced and compared with the measured fracture energies. The agreement was found to be good. Further, for example, over the range of particle diameters (i.e. from 23 nm to 170 nm) used in the present work the model predicted that there is no effect of particle diameter on the toughness, as was indeed observed from the experimental data. Acknowledgements The authors dedicate the present paper to Professor Alan Gent (1927-2012). The work at Lehigh University was partially funded through the Semiconductor Research Corporation (SRC Contract 1292.027). The silica nanoparticles used in this study were kindly supplied by Dr. Bill Schultz and Wendy Thompson of the 3M Company. Dr Peerapan Dittanet is grateful for financial support through the Royal Thai Fellowship. The authors would like to thank the EPSRC for a doctoral training award for Dr Kunal Masania, and for providing research funding under the grant EP/H00582X/1.  \", \"Introduction Understanding dispersion processes in urban areas is important for modelling air quality as well as pollution from accidental or terrorist releases. The chaotic nature of turbulent flow and the complexity of the building geometry both contribute to making such modelling non-trivial. Urban geometry affects the mean flow and turbulence significantly (Barlow and Coceal, 2009) and thereby exerts a strong control on dispersion processes (Belcher, 2005). For localised releases, such effects are particularly important in the near-field region (here defined as within a few building blocks of the release), where conventional models such as Gaussian plume models fail (Belcher et al., 2013). In this region, certain characteristics of the dispersion become significant, although they are usually justifiably neglected further downstream. Recently, Wood et al. (2009) reported above-background dosages within 6-8 times the building height in all directions around the source in field experiments in central London. But comprehensive data and understanding on these near-field dispersion aspects are currently lacking. The present paper is an attempt to document such near-field aspects for simple urban-like geometries, where their effects can be more easily isolated and therefore better understood. Specifically, the main questions investigated here are the following: • What are the main characteristics of near-field dispersion from localised sources in urban areas? • How are they related to the underlying flow structure around the buildings? • How do the mean concentration and concentration fluctuations vary in the near field? These questions are explored by analysing data from the recent direct numerical simulations (DNS) reported in Branford et al. (2011) and new simulations presented here. These simulations involve the continuous release of a passive scalar from localised ground-level sources within a regular array of cubes. The effect of different flow directions, obstacle layout and source location is investigated. The paper is structured as follows. In Section 2, we elaborate on the methodology and the cases studied. Results are presented in Section 3, which first looks at the instantaneous and mean flow field; the scalar field is then explored by looking at the instantaneous and mean concentration patterns, concentration profiles and time series. Conclusions are presented in Section 4. Method The numerical technique for solving the Navier-Stokes equations in the DNS is described in Yao et al. (2001) and Coceal et al. (2006). In brief, the equations are discretized using second-order central finite differences in space and a second-order Adams-Bashforth scheme in time, based on the pressure correction method. The Poisson equation for pressure is solved by a multigrid method. The code is parallelized using Message Passing Interface (MPI). Branford et al. (2011) performed a series of DNS for different wind directions (0°, 30° and 45°) over a regular array of cubical obstacles of height H. The domain size was 16H x 16H×16H in the horizontal and 8H in the vertical - see Fig. 1a and b. In the present paper we analyse data from the 0° and 45° runs. Additionally, we present results for new simulations done for flow at an angle of 45° over a smaller staggered array of domain size 8H x 8H×8H x 8H×8H - see Fig. 1c. Comparison with simulations over a larger staggered array of domain size 16H x 12H×12H x 8H×8H (Coceal et al., 2007) revealed similar turbulence statistics; hence, since the emphasis here is on near-source dispersion, the size of the present domain is adequate. All the simulations were conducted under conditions of neutral stability and fully rough turbulent flow. Periodic boundary conditions were imposed in the horizontal directions, effectively simulating flow over an infinite array of cubes. The Reynolds number based on the velocity at the top of the domain and the cube height was typically between 4750 and 7000. While this is much less than Reynolds numbers at full scale, it is comparable to typical Reynolds numbers achieved in many wind tunnel experiments. Numerical tests showed that a grid resolution of H/32 was sufficient, producing flow and concentration statistics that agreed with test runs at double the resolution (H/64) to within a few percent (Coceal et al., 2006, Branford et al., 2011). Based on the wall friction velocity uτ and the cube dimension H, a non-dimensional time scale characterizing the turnover time of eddies shed from the cubes can be defined as T=H/uτ. The simulations were run with a time step of 0.00025T, spun up for a duration of approximately 200T to allow fully developed turbulence conditions and statistics were collected over an interval of approximately 100T. The simulations for the larger arrays typically took a few weeks in total to run on 124 dual-core 2.5GHz PowerPC 970MP processors. In this study we investigate the dispersion of a passive scalar, which is modelled by numerically solving the scalar transport equation using an Eulerian approach. Point sources are discretized as Gaussian balls over a few grid points. The Schmidt number is fixed at one in all the simulations. A sponge layer is applied to the scalar field around the domain to prevent the scalar from re-entering the domain due to the periodic boundary conditions. The scalar is allowed to freely escape at the top of the domain. Further details are given in Branford et al. (2011). The scalar is released continuously and at a steady rate from an ensemble of point sources close to the ground (at z=0.0625H) within the simulated urban area; the source locations are indicated in Fig. 1. The sources in both cases (a) and (b) are placed in equivalent locations, therefore they form an ensemble of equivalent releases and ensemble averaging can be performed to extend the time series of each individual release. In case (c) the flow is not symmetrical because of the staggered arrangement of the cubes. Here the sources are in three different locations: behind a cube (3 equivalent locations), in front of a cube (3 equivalent locations) and in the space between two cubes (1 location). Near-source dispersion is sensitive to the building geometry and it is therefore impossible to perform a comprehensive study of the influence that specific urban geometrical features have on the dispersion. Our motivation, therefore, is to gain insight into generic features of the dispersion patterns and how they are influenced by the flow structure in simple commonly-occurring urban setups. The choice of these different configurations allows us to look at the effect of wind direction, building layout and source location on dispersion in a simple but plausible setup. Detailed comparisons of mean concentration profiles at different locations within and above the array showed good agreement with data from a water-channel experiment (Hilderman and Chong, 2007) on the same configuration of cubical buildings, as reported in Branford et al. (2011). Mean flow and turbulence statistics were compared extensively with wind tunnel data in Coceal et al. (2006, 2007). The reader is referred to those papers for details of the validation of the DNS. Results and discussion The flow field Dispersion within a building array is heavily influenced by the flow structure and how it varies in space and time. We therefore begin by surveying the spatial and temporal variation of the flow field. Instantaneous flow patterns To form an idea of the instantaneous flow field, Figs. 2 and 3 show wind vector snapshots for the aligned array for the two wind directions of 0° and 45° at four different heights, corresponding to values of z/H of 0.23, 0.52, 0.80 and 1.08. For clarity, in these and other vector plots, vectors are only shown at every fourth grid point. We adopt a Cartesian coordinate system in which the x coordinate is directed from left to right (along the mean forcing direction for the 0° case), the y coordinate points upwards and along the plane of the paper and the z coordinate points vertically out of the paper. The corresponding instantaneous velocity components are u, v and w, with mean components denoted by U, V and W and fluctuating components denoted by u′, v′ and w′ respectively. The instantaneous, mean and fluctuating components of the scalar concentration field are denoted by c, C and c′ respectively. Fig. 2 shows that the instantaneous pattern is quite complex, even for this simple configuration. Broadly speaking, two main flow patterns can be discerned in these plots: a recirculation in the wake region behind the cubes with generally low velocity and a faster channelling flow along the unobstructed channels between alternate rows of cubes. However, on an instantaneous basis, the flow in these channels is far from being linear along the direction of the forcing. Rather, the wind vectors change direction continuously and unpredictably, especially around the intersections. A consequence of the highly unsteady character of the turbulent flow in this complex geometry is that individual flow realisations may behave in somewhat unintuitive ways. For example, at z=0.23H there is a swirling motion in the intersection region which drives faster fluid at an angle into the downstream section of the upper channel (Fig. 2a). Elsewhere, for example in the upstream section of the lower channel at z=0.52H (Fig. 2b), much of the flow diverts into the side 'street' behind the building wake instead of proceeding downstream along the channel as one might expect. Such intermittent flow features drive turbulent exchanges between the channels along the forcing direction and the 'streets' perpendicular to it. Further up at z=0.80H (Fig. 2c) the flow pattern along the lower channel meanders between the alternate rows of buildings. Clearly then, there is significant vertical variation in the structure of the flow too. Even above the array at z=1.08H (Fig. 2d) the wind vector pattern is still horizontally inhomogeneous. Fig. 3 shows that the patterns of wind vector snapshots for the 45° flow are again quite complex. The array is laterally symmetric with respect to the flow direction of 45°, but at any particular instant, the actual flow pattern is non-symmetrical. Again, the flow structure is different at different heights. Hence, the flow is highly inhomogeneous and three-dimensional spatially and unsteady in time. The unsteadiness is accentuated by the intermittent character of the recirculations behind the leeward faces of the cubes. For example, at z=0.23H there is a recirculation behind the eastward face of the cube on the lower left but none on the northward face. Above the array at 1.08H, the flow is broadly in the direction of the forcing on average, but there are strong deviations associated with flow around the cube tops. Similar observations on the asymmetry and unsteadiness of the instantaneous flow patterns can be made for the staggered array (not shown). Mean flow structure: horizontal The time-averaged flow structure in the aligned array for the two forcing directions of 0° and 45° and in the staggered array for 45° are shown in Fig. 4. Streamlines are plotted in horizontal planes at two different heights of z=0.12H and 0.73H for each configuration. Fig. 4a and b shows the streamlines for a 0° flow. The implicit averaging over many instantaneous realisations has removed much of the small scale complexity in the flow pattern and emphasized the two main features identified earlier, namely large recirculations within the 'canyon' region behind the buildings and a channelling flow in the open 'street' region between the rows of cubes. The streamlines within this channelling region appear much more linear now and look similar at the two heights within the array. In contrast, the structure of the recirculations changes with height, indicating that the building wake has a three-dimensional structure (Goulart, 2012). Fig. 4c and d shows the flow structure for the flow at 45° over the aligned array. There is a recirculation behind each face of the cubes, symmetrically disposed with respect to the 45° diagonal line through the cubes. The recirculations reduce in size with height. Streamlines from the two perpendicular streets upstream of the intersection come together and then diverge around the corner of the next building at the intersection, part of them feeding into a recirculation on the leeward face of the upstream buildings in the street perpendicular to the original flow direction and another part channelling into the far side of that same street. As we shall see, both of these flow features play an important role in near-source dispersion. Firstly, the diverging streamlines around the building corner lead to considerable mean transport into lateral streets and thereby enhance horizontal (lateral) dispersion, a process known as 'topological dispersion' (Davidson et al., 1995, Belcher, 2005). Secondly, the recirculations trap material and release them on a slower timescale (Vincent, 1978) higher up and into the flow above the array. The relative roles of the recirculations and the channelling flow change with increasing height, as the recirculations reduce in size, coupled with stronger mean flow into the side streets. Fig. 4e and f shows the corresponding pattern of streamlines for flow at 45° over the staggered array. Staggering the cube rows in the streamwise direction means that the flow is no longer symmetrical with respect to the 45° diagonal line through the array. This asymmetry modifies the details of the flow topology drastically, but broadly similar structural elements can be identified as in the aligned array. There are now two recirculations behind one leeward face of the cube and a smaller one on the other leeward face. The streamlines still diverge around the cubes, but now do so around a cube face rather than around a corner. The exact location at which this occurs depends on height, being different at z=0.12H and z=0.73H. As for the aligned array, the recirculations are weaker and the channelling flow is stronger in the gaps further up. Mean flow structure: vertical Given that the flow field is three-dimensional, particularly when the flow direction is oblique, it is instructive to look at its structure in a vertical plane. Fig. 5 shows the locations of eight vertical y-z planes through the aligned and staggered arrays at values of x/H of 0.02, 0.14, 0.39, 0.64, 1.02, 1.14, 1.39 and 1.64. Fig. 6 shows a sequence of mean wind vector plots in these vertical planes for the aligned array for 45° flow. Since the array is regular, the same patterns repeat in other units. Also, due to symmetry about the 45° axis, the same pattern applies to the (u, w) wind vectors in corresponding x-z planes. The salient features of the flow at these different locations are summarised below. • At x=0.02H (close to the entry into the 'street', Fig. 6a), there is a strong flow component towards the downstream side of the street canyon. • At x=0.14H (a little further into the canyon, Fig. 6b), we start seeing a weak updraft up the leeward face of the buildings. • At x=0.39H (almost halfway into the canyon, Fig. 6c), there is a recirculation with an associated strong updraft along the leeward face. • At x=0.64 H (about two-third of the way into the street, Fig. 6d), a similar picture applies, but the core of the vortex is further up and there is now a strong flow in the lower part of the canyon towards the lee of the building. This would cause any material in the street canyon (which, for example, might have come from advection along the street from upstream) to be driven into the recirculation region. • At x=1.02H (into the long street region and close to the face of a cube, Fig. 6e), an updraft inclined at some angle can be seen over most of the building's face. At the canyon exit, the vortex is still present, but has moved further along. • At x=1.14H (a little further into the long street, Fig. 6f), there is still a strong updraft over most of the leeward face of the cube, now almost straight upward, whereas the vortex is still present but with its core a little lower down. • At x=1.39H (almost halfway into the street, Fig. 6g), there is now a strong flow along the street, but an upward displacement of the flow in the intersection region. Hence, the flow structure is still three-dimensional in the intersection. • At x=1.64H (closer to the face of the downstream buildings, Fig. 6h), advection along the street predominates, with a weaker upward flow close to the downstream cube. Qualitatively similar flow features are observed for 45° flow in the staggered array (Fig. 7), except in the last two vertical planes at x=1.39H and x=1.64H, where the presence of a building at the end of the street at x=2H (see Fig. 5b) gives rise to a reverse flow region in the middle part of the plane shown. These observations indicate that the mean flow structure is quite complex, three-dimensional and highly heterogeneous. A key three-dimensional characteristic of the flow is the strong updraft up the leeward face of the buildings due to the recirculating vortex in the building wake. This will tend to move material caught in the wake upwards and out of the array. This is a manifestation of the 'chimney effect' observed especially in the vicinity of tall buildings (see e.g. Heist et al., 2009). The concentration field Having surveyed the structure of the flow field, we now move on to consider its effect on the resulting concentration patterns and dispersion characteristics. Instantaneous concentration patterns Figs. 8 and 9 show snapshots of the concentration field for the aligned array for the two wind directions of 0° and 45° at four different heights, corresponding to values of z/H of 0.23, 0.52, 0.80 and 1.08. In these plots the plume is represented using 'smoke' visualisation and with the regions of highest concentration indicated by contour lines. Focusing first on the 0° case (Fig. 8), in which the release is close to the ground (at z=0.0625 H) in the middle of the gap between the two middle cubes, it is clear that much of the material is captured in the wake region behind the upstream cube. This results in a build-up of material in the building wake, where the concentration is highest (Fig. 8a-c). Material is then released gradually from the wake at higher levels. Due to this trapping and the fact that the mean flow in the adjacent channels is at 0° (see Fig. 4c and d), the lateral spread of the plume is rather limited. The situation for the 45° case is more interesting (Fig. 9). Here the release is in an intersection and the flow is oblique. In this snapshot, at z=0.23H the whole plume is blown into the canyon region between the cubes in the y-direction and none in the x-direction. Note that the flow is constantly changing and that this is literally a snapshot. In a different snapshot this might of course be different and on average the plume would be equally likely to be blown into the gap in the x-direction. A little further up, at z=0.52H, we see that much of the material is now caught in the wake region. A little still comes from the horizontal location of the original source. Higher still, at z=0.80H, most of the material is concentrated in the region between the cubes. Nothing comes from the horizontal location of the original source. The highest concentration is around the centre of the gap, but somewhat away from the mean location of the core of the recirculation behind the upstream cube. This indicates the possible release of material trapped further down in the recirculation. These visualisations suggest the idea that transfer from the original source location to the wake occurs predominantly at low levels, whereas the re-release of material occurs mainly at higher levels. It would be of interest to test this hypothesis quantitatively in future studies. Above the array, material appears to be coming from the wake region, which therefore acts as a 'secondary source'. For the staggered array (not shown), the setup is somewhat different as there are no clearly defined 'intersections'. Moreover, the flow structure is asymmetrical with respect to the 45° direction. This spatial asymmetry results in a plume that tends to fluctuate in a preferential direction and causes more material to accumulate in the wake of particular buildings in the immediate vicinity of the source; this is examined in the next section. Mean concentration patterns Fig. 10 shows contour plots of the mean concentration at two different heights z=0.23H and z=0.98H for 0° and 45° flow over the aligned array. These result from the integration of the instantaneous concentration distribution over a total duration of 100T, where T is an eddy turnover time. In Fig. 10a and b (which show results for the 0° case) the flow is perpendicular to the faces of the cubes. As discussed earlier, the concentration pattern results from the trapping of material into the wake of the building immediately upstream of the source, and the escape of material into the channels on either side, which is then swept downstream in the fast flow in the channel. Some of that material enters the wake regions of the cubes downstream, but this represents a much smaller fraction compared to that entrained into the wake of the first cube. This is not surprising; as the plume becomes more distributed, there is less material left to be trapped in subsequent wakes. It is most significant in the immediate vicinity of the release, where the plume is concentrated in a small region of space. In the lower part of the array (z=0.23 H), a lateral profile of concentration at most locations downstream would show a bimodal distribution (not shown), due to the bifurcation of the plume around the obstacles. Near the array top (z=0.98 H), this is less so, presumably because material can flow over the cube tops and then be re-entrained into the region below the cube tops. There is a similar bifurcation effect for the 45° case (Figs. 10c and d). But this division of the plume is accentuated by the effect of the two secondary sources in the wakes of the two cubes immediately downstream of the source. Indeed, as Fig. 10c shows, material from the source in the intersection rapidly spreads into these wakes in the lower part of the array (here shown for z=0.23H) and this trapped material is released further up (here shown at z=0.98H, Fig. 10d), effectively giving two overlapping plumes originating from the two secondary sources. The combined effect of these secondary sources and the topological dispersion around the cubes contribute to making the plume wider than that in the 0° case. This difference is particularly pronounced at higher locations. A similar picture emerges regarding the formation and effect of secondary sources for the staggered array (Fig. 11). The mean concentration pattern is plotted at two different heights (z=0.23H and z=0.98H) when the source is directly downstream of a cube in the x direction (Fig. 11a and b), directly upstream of a cube (Fig. 11c and d) and in the channel between two cubes (Fig. 11e and f). In the first and second cases material is drawn back into the recirculation behind the upstream cube(s) (Fig. 11a and c); in the third case, it is first swept forward along the channel and thereafter entrained into the building wakes upstream (Fig. 11e). Further up, the material is released and appears to originate from secondary sources in the respective building wakes (Fig. 11b,d and f). In this case, the secondary sources are not symmetrical like the aligned array due to the asymmetry of the flow pattern, so that one of the secondary sources is predominant (Goulart, 2012). Concentration profiles The secondary sources in the near field have a strong effect on the subsequent dispersion pattern above the array. This is illustrated here for the 45° aligned array case by looking at lateral profiles of concentration just above the array (at z=1.02H) at different distances from the original source (Fig. 12). In this plot and in the text below horizontal distances are given in a coordinate system along the mean plume direction (x') and perpendicular to it (y'). In the very near field (x'<~H, Fig. 12a), before the next obstacle, the pattern is multimodal. There is a central peak associated with material coming directly from the original source, but there are also two stronger peaks on either side associated with the secondary sources. The concentration at the higher peaks due to the secondary sources is about twice that of the central peak. There are two other, smaller, peaks on either side of the centre with a mean concentration about a third of that of the central peak. Fitting a triple Gaussian to the concentration profile captures the central part of the plume very well, including the three central peaks. Further on (x'~2-3H, Fig. 12b), the profile only has a double peak, with the maximum concentration at the peaks about twice that of the central minimum. This double-peaked profile arises as the original source is obscured and material at that point originates mostly from the two secondary sources. Macdonald et al. (1998) reported similar double-peaked lateral profiles in their wind-tunnel experiments involving a release upstream of an aligned array of cubical obstacles. Their measurements were within the array, and they attributed the double peaks to the bifurcation of the plume around an obstacle immediately downstream of the release, an interpretation supported by visualisation of the plume in a similarly-configured field experiment. In the present case, the profile is above the array and the double peaks cannot be explained simply by bifurcation around the cube; rather they are a consequence of the two strong secondary sources. Still further from the source (x'~4H, Fig. 12c), as the two plumes widen they begin to merge and their superposition results in an approximately single-peaked profile. A Gaussian fit to the profile works fairly well except at the plume edges. The shape of this lateral mean concentration profile therefore marks a transition region where the influence of the near-field distribution of sources begins to be smoothed out, but where the profile is still not quite Gaussian yet. In the terminology of Theurer et al. (1996), the 'radius of homogenization' has not been reached yet. At x'~9H (Fig. 12d), a Gaussian fit to the lateral profile works much better. This makes sense physically in terms of the superposition of plumes coming from the two secondary sources - far enough they appear so close together that they are approximately equivalent to a single effective source. Owing to the generality of this argument, one would expect a similar behaviour in the far field for more complex building morphologies, for example with buildings of different heights - although the precise distance at which this occurs would likely depend on the details of the building arrangement, which determines the physical locations of the resulting secondary sources. So in the far field, which may be thus defined, the initial near-field source distribution does not matter; and a practical diagnostic of the far field might be construed in terms of the goodness of fit to a Gaussian (Goulart, 2012). Concentration time series As seen in Section 3.1, the flow field in the arrays is highly unsteady in time as well as spatially inhomogeneous. An important question for dispersion modelling is to understand the temporal as well as spatial variation of concentration. How do concentration fluctuations vary around the area of a release? What is the order of magnitude of the fluctuating concentration compared to the mean concentration? In this section we analyse the DNS results to shed some light on these questions. Figs. 13-16 show concentration time series and computed statistics at different locations within and just above the aligned array for 0° and 45° flow. In these figures the location of each subplot corresponds to the actual location of the block that it pertains to, i.e. the distribution of subplots shows the actual layout of the array. The black squares represent the cube locations. Indices i and j in the plot titles refer to the location of each block in the x and y directions respectively, starting from the lower left hand corner (i=1, j=1). Each time series is sampled at the centre of the relevant block. So the sampling locations within the array (Figs. 13 and 15) are situated at z=0.5 H and those above the array (Figs. 14 and 16) are situated at z=1.5H. The time series span a duration of 100T, over which are computed the mean concentration C (solid line), the root mean square fluctuating concentration crms (dash-dot line) and the relative fluctuation intensity I, defined as I=crms/C, in each of the blocks. In Fig. 13, the location of the source is in the block located at (i=4, j=7) and the mean wind is from left to right (0°). Not surprisingly, the mean and r.m.s. concentrations are both very large in the block where the source is located. The blocks immediately adjacent in the channels on either side also have high mean and r.m.s. concentrations, although not as high as in the source block. The concentration fluctuation intensity in those blocks is among the highest, namely between 3 and 4. Concentration fluctuations are therefore very large in the near field; thereafter C, crms and I all decrease monotonically downstream. Within the canyons directly downstream of the source (along the middle of the array in Fig. 13), C, crms and I are all smaller compared to their values in corresponding blocks in the adjacent channel. Moreover, the intermittency at those locations is considerably reduced in comparison. This is probably due to the combined effect of more efficient mixing in those canyons due to the canyon vortex and less exposure to larger scale eddies, which are more likely to be present in the channels where they might cause meandering of the plume and hence greater intermittency. In the top and bottom rows, the fluctuation intensity is quite high, however; this is because it is at the plume edge where the mean concentration is small in comparison to the fluctuations. Finally, we note the intermittent bursts in the concentration in the two blocks upstream of the release with correspondingly high fluctuation intensity of around 5 or greater. Corresponding time series and concentration statistics are shown for the blocks just above the array in Fig. 14. The most striking difference is that the fluctuation intensity is larger than within the array. Closer inspection of the time series reveals that the intermittency is also larger. This is not unexpected: the flow above the array is exposed to larger scale fluctuations compared to the size of the plume, so that the plume above the array is more prone to meander. Davidson et al. (1995) found similar results in their field experiments. Along the plume centreline the mean concentration decreases very slowly whereas the r.m.s. concentration decreases more rapidly, hence the rapid decrease in the fluctuation intensity. The whole plume is displaced downstream relative to that within the array due to the fast flow above. Hence, the maximum concentration at this height is not directly above the source block but one or two blocks downstream. Inspection of Fig. 15 reveals a broadly similar picture for the 45° flow as in the 0° case, but with some notable differences. Here the release is in the (i=4, j=4) block. Interestingly, we find that the largest value of mean concentration C is found not in the source cell (at least at that height) but in the two blocks immediately downstream in the x and y directions. These blocks encompass the region where most of the wake trapping occurs. The r.m.s. concentration crms is also high at those locations. However, crms is highest in the source block and so is the fluctuation intensity I, which has a value of around four there. As within the array, there is a general decrease in the values of C, crms and I with distance from the source. Fig. 16 shows corresponding plots for blocks immediately above the array for the 45° flow. It is noteworthy that there is not much material above the source cell at all. Hence, little material is detrained into the air directly above the source. There is more material above the location of the secondary sources. This is consistent with the picture that material released at the initial source location moves horizontally into the wake region of adjacent buildings where it is then pushed up by the updraft flow visualised in Section 3.1.3 and then spread by the faster flow above the array. Due to this faster flow, the whole pattern is displaced downstream relative to the pattern in the array below. As in the 0° flow, the fluctuation intensity I is larger than within the array; it generally decreases with distance from the source, except when the mean concentration C is very small. Conclusions In this study we have analysed data from direct numerical simulations to document the effect of buildings on near-source dispersion for three case studies with different wind directions, building layout and source location. The analysis has highlighted the following aspects of near-field dispersion: • Dispersion is strongly affected by the presence of buildings in the near field. First, diverging streamlines around buildings lead to considerable mean transport and enhance lateral dispersion. Secondly, trapping and re-release of material in the wakes of buildings in the immediate vicinity of the release gives rise to secondary sources. This appears to be a ubiquitous phenomenon, at least in the relatively high packing density regime investigated here, which is characteristic of city centres. Without knowledge of these secondary sources, erroneous modelling assumptions might be made based purely on the initial location of the release. In an emergency response scenario involving accidental or terrorist releases this could be critical. • These near-field dispersion characteristics derive from the underlying flow structure around the buildings, which exerts a strong influence in the near field where there is a higher concentration of material. The detailed flow topology depends on the wind direction and building layout, but salient features such as diverging streamlines and wake recirculations are robust. The three-dimensional structure of these flow features is especially pertinent and needs to be taken into account in models that aim to predict near-field concentrations. • The levels of concentration fluctuations within and directly above the building arrays depend significantly on location around the source. High levels of concentration fluctuations are found in the very near field, in open channels and immediately above the buildings. An order of magnitude estimate for the fluctuation intensity is between 2 and 5 in the near field. These results could be incorporated for example in semi-empirical models of concentration fluctuations (e.g. Cierco et al., 2010). These results are valuable for informing the development and validation of simple models of dispersion that could perform better than current approaches in the near-field region. In this context, incorporating some of the current findings into street network models (Soulhac, 2000; Belcher, 2005; Hamlyn et al., 2007) offers an especially promising possibility. Preliminary work in this respect has been performed by the authors and is reported in Goulart (2012) and Belcher et al. (2013). Ongoing work is focusing on extending the capabilities of such models to predicting near-field dispersion by accounting for wake sources and three-dimensional effects. Statistical analysis of concentration fluctuations will also help to develop and validate a stochastic version of the network model. Finally, inverse modelling using empirical data such as the present datasets can be used to infer optimal parameters for the street network model. Acknowledgements Omduth Coceal gratefully acknowledges funding from the Natural Environment Research Council (NERC) through their National Centre for Atmospheric Science (NCAS) under grant no. R8/H12/83/002. Simon Branford was supported by the University of Reading Research Endowment Trust Fund. Elisa V. Goulart's Ph.D. was funded by the National Council for Scientific and Technological Development (CNPq), Brazil. Underlying research materials (e.g. data) used in this work can be accessed by contacting the corresponding author.  \", 'Introduction Highly-threaded, many-core devices such as GPUs have gained popularity in the last decade; both NVIDIA and AMD manufacture general purpose GPUs that fall in this category. The important distinction between these machines and traditional multi-core machines is that these devices provide a large number of low-overhead hardware threads with low-overhead context switching between them; this fast context-switch mechanism is used to hide the memory access latency of transferring data from slow large (and often global) memory to fast, small (and typically local) memory. Researchers have designed algorithms to solve many interesting problems for these devices, such as GPU sorting or hashing [1-4], linear algebra [5-7], dynamic programming [8,9], graph algorithms [10-13], and many other classic algorithms [14,15]. These projects generally report impressive gains in performance. These devices appear to be here to stay. While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis [16-20], there are no systematic theoretical models to analyze the performance of programs on these machines. We are interested in analyzing and characterizing performance of algorithms on these highly-threaded, many-core machines in a more abstract, algorithmic, and systematic manner. Theoretical analysis relies upon models that represent underlying assumptions; if a model does not capture the important aspects of target machines and programs, then the analysis is not predictive of real performance. Over the years, computer scientists have designed various models to capture important aspects of the machines that we use. The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) model [21], which we teach undergraduates in their first algorithms class. This model assumes that all operations, including memory accesses, take unit time. While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines. Aggarwal and Vitter proposed the Disk Access Machine (DAM) model [22] which counts the number of memory transfers from slow to fast memory instead of simply counting the number of memory accesses by the program. Therefore, it better captures the fact that modern machines have memory hierarchies and exploiting spatial and temporal locality on these machines can lead to better performance. There are also a number of other models that consider the memory access costs of sequential algorithms in different ways [23-29]. For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) model [30], and there is a large body of work describing and analyzing algorithms in the PRAM model [31,32]. In the PRAM model, the algorithm\\'s complexity is analyzed in terms of its work-the time taken by the algorithm on 1 processor, and span (also called depth and critical-path length)-the time taken by the algorithm on an infinite number of processors. Given a machine with P processors, a PRAM algorithm with work W and span S completes in max(W/P,S) time. The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time. For modern machines, however, this assumption seldom holds. Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines [33-35], shared memory machines and multi-cores [36-40], or the combination of the two [41,42]. All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines. While superficially highly-threaded, many-core machines such as GPUs are shared memory machines, their characteristics are very different from traditional multi-core or multiprocessor shared memory machines. The most important distinction between the multi-cores and highly-threaded, many-core machines is the number of threads per core. On multi-core machines, context switch cost is high, and most models nominally assume that only one (or a small constant number of) thread(s) are running on each machine and this thread blocks when there is a memory access. Therefore, many models consider the number of memory transfers from slow memory to fast memory as a performance measure, and algorithms are designed to minimize these, since memory transfers take a significant amount of time. In contrast, highly-threaded, many-core machines are explicitly designed to have a large number of threads per core and a fast context switching mechanism. Highly-threaded many-cores are explicitly designed to hide memory latency; if a thread stalls on a memory operation, some other thread can be scheduled in its place. In principle, the number of memory transfers does not matter as long as there are enough threads to hide their latency. Therefore, if there are enough threads, we should, in principle, be able to use PRAM algorithms on such machines, since we can ignore the effect of memory transfers which is exactly what PRAM model does. However, the number of threads required to reach the point where one gets PRAM performance depends on both the algorithm and the hardware. Since no highly-threaded, many-core machine allows an infinite number of threads, it is important to understand both (1) how many threads does a particular algorithm need to achieve PRAM performance, and (2) how does an algorithm perform when it has fewer threads than required to get PRAM performance? In this paper, we attempt to characterize these properties of algorithms. To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs [3]. The problem is embarrassingly parallel, each set membership test is independent of every other membership test. Fig. 1 shows the performance of this application, varying the number of threads per processor core, for two distinct GPUs. For both machines, the pattern is quite similar, at low thread counts, the performance increases (roughly linearly) with the number of threads, up until a transition region, after which the performance no longer increases with increasing thread count. While the location of the transition region is different for distinct GPU models, this general pattern is found in many applications. Once sufficient threads are present, the PRAM model adequately describes the performance of the application and increasing the number of threads no longer helps. In this work, we propose the Threaded Many-core Memory (TMM) model that captures the performance characteristics of these highly-threaded, many-core machines. This model explicitly models the large number of threads per processor and the memory latency to slow memory. Note that while we motivate this model for highly-threaded many-core machines with synchronous computations, in principle, it can be used in any system which has fast context switching and enough threads to hide memory latency. Typical examples of such machines include both NVIDIA and AMD/ATI GPUs and the YarcData uRiKA system. We do not try to model the Intel Xeon Phi, due to its limited use of threading for latency hiding. In contrast, its approach to hide memory latency is primarily based on strided memory access patterns associated with vector computation. If the latency of transfers from slow memory to fast memory is small, or if the number of threads per processor is infinite, then this model generally provides the same analysis results as the PRAM analysis. It, however, provides more intuition. (1) Ideally, we want to get the PRAM performance for algorithms using the fewest number of threads possible, since threads do have overhead. This model can help us pick such algorithms. (2) It also captures the reality of when memory latency is large and the number of threads is large but finite. In particular, it can distinguish between algorithms that have the same PRAM analysis, but one may be better at hiding latency than another with a bounded number of threads. This model is a high-level model meant to be generally applicable to a number of machines which allow a large number of threads with fast context switching. Therefore, it abstracts away many implementation details of either the machine or the algorithm. We also assume that the hardware provides 0-cost and perfect scheduling between threads. In addition, it also models the machine as having only 2 levels of memory. In particular, we model a slow global memory and fast local memory shared by one core group. In practice, these machines may have many levels of memory. However, we are interested in the interplay between the farthest level, since the latencies are the largest at that level, and therefore have the biggest impact on the performance. We expect that the model can be extended to also model other levels of the memory hierarchy. We analyze 4 classic algorithms for the problem of computing All Pairs Shortest Paths (APSP) on a weighted graph in the TMM model [43]. We compare the analysis from this model with the PRAM analysis of these 4 algorithms to gain intuition about the usefulness of both our model and the PRAM model for analyzing performance of algorithms on highly-threaded, many-core machines. Our results validate the intuition that this model can provide more information than the PRAM model for the large latency, finite thread case. In particular, we compare these algorithms and find specific relationships between hardware parameters (latency, fast memory size, limits on number of threads) under which some algorithms are better than others even if they have the same PRAM cost. Following the formal analysis, we assess the utility of the model by comparing empirically measured performance on an individual machine to that predicted by the model. For two of the APSP algorithms, we illustrate the impact of various individual parameters on performance, showing the effectiveness of the model at predicting measured performance. This paper is organized as follows. Section 2 presents related work. Section 3 describes the TMM model. Section 4 provides the 4 shortest paths algorithms and their analysis in both the PRAM and TMM models. Section 5 provides the lessons learned from this model; in particular, we see that algorithms that have the same PRAM performance have different performance in the TMM model since they are better at hiding memory latency with fewer threads. Section 6 continues the discussion of lessons learned, concentrating on the effects of problem size. Section 7 shows performance measurements for a pair of the APSP algorithms executing on a commercial GPU, illustrating correspondence between model predictions and empirical measurements. Finally, Section 8 concludes. Related work In this section, we briefly review the related work. We first review the work on abstract models of computations for both sequential and parallel machines. We then review recent work on algorithms and performance analysis of GPUs which are the most common current instantiations of highly-threaded, many-core machines. Many machine and memory models have been designed for various types of parallel and sequential machines. In an early work, Aggarwal et al. [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels. It differs from the RAM model by defining that access to location x takes logx time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms. The Block Transfer model (BT) [27] addresses this deficiency by defining that a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time. Alpern et al. propose the Memory Hierarchy (MH) Framework [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level. But there are too many parameters in this model that can obscure algorithm analysis. Thus, they simplified and reduced the MH parameters by putting forward a new Uniform Memory Hierarchy (UMH) model [28,29]. Later, an \\'ideal-cache\\' model was introduced in [23,24] allowing analysis of cache-oblivious algorithms that use asymptotically optimal amounts of work and move data asymptotically optimally among multiple levels of cache without the necessity of tuning program variables according to hardware configuration parameters. In the parallel case, although widely used, the PRAM [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free. Quite different to PRAM, the Bulk-Synchronous Parallel (BSP) model [34] attempts to bridge theory and practice by allowing processors to work asynchronously, and it models latency and limited bandwidth for distributed memory machines without shared memory. Culler et al. [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead. It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing. Vitter et al. [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines. But this model assumes that processors are interconnected via sharing of internal memory. More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs). Arge et al. [36] present the Parallel External Memory (PEM) model with P processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the P processors. Blelloch et al. [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip. Bender et al. [44] present a concurrent cache-oblivious model. Blelloch et al. [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies. Chowdhury et al. [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores. They in [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private L1 caches and a shared L2 cache. All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e. the explicit use of many threads for the purpose of hiding memory latency. While there has not been much work on abstract machine models for highly-threaded, many-core machines, there has been a lot of recent work on designing calibrated performance models for particular instantiations of these machines such as NVIDIA GPUs. We review some of that work here. Liu et al. [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely. Their model incorporates the relationship between problem size and performance, but only targets their biosequence application. Govindaraju et al. [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops. It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general. Ryoo et al. [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications. They do not, however, consider memory latency and multiple conflicting performance indicators. Kothapalli et al. are the first to define a general GPU analytical performance model in [47]. They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing. Using a different approach, Hong et al. [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP). Meantime, Baghsorkhi et al. [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly. The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures. Zhang and Owens [15] present a quantitative performance model that characterizes an application\\'s performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses. More recently, Sim et al. [48] develop a performance analysis framework that consists of an analytical model and profiling tools. The framework does a good job in performance diagnostics on case studies of real codes. Kim et al. [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters. Parakh et al. [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces. All of these efforts are mainly focused on the practical calibrated performance models. No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines. TMM model The TMM model is meant to model the asymptotic performance of algorithms on highly-threaded, many-core machines. The model should abstract away the details of particular implementations so as to be applicable to many instantiations of these machines, while being particular enough to model the performance of algorithms on these machines with reasonable accuracy. In this section, we will describe the important characteristics of these highly-threaded, many-core architectures and our model for analyzing algorithms for these architectures. Highly-threaded, many-core architectures The most important high-level characteristic of highly-threaded, many-core architectures is that they provide a large number of hardware threads and use fast and low-overhead context-switching in order to hide the memory access latency from slow global memory. Highly-threaded, many-core architectures typically consist of a number of core groups, each containing a number of processors (or cores), a fixed number of registers, and a fixed quantity of fast local on-chip memory shared within a core group. A large slow global memory is shared by all the core groups. Registers and local on-chip memory are the fastest to access, while accessing the global memory may potentially take 100s of cycles. The TMM model models these machines as having a memory hierarchy with two levels of memory: slow global memory and fast local memory. In addition, on most highly-threaded, many-core machines, data is transferred from slow to fast memory in chunks; instead of just transferring one word at a time, the hardware tries to transfer a large number of words during a memory transfer. The chunk can either be a cache line from hardware managed caches, or an explicitly-managed combined read from multiple threads. Since this characteristic of using high-bandwidth transfers in order to counter high latencies is common to most many-core machines (and even most multi-core machines), the TMM model captures the chunk size as one of its parameters. These architectures support a large number of hardware threads, much larger than the number of cores. Cores on a single core group execute in synchronous style where groups of threads execute in lock-step. When a thread group executing on a core group stalls on a slow memory access, in theory, a context switch occurs and another thread group is scheduled on that core group. The abstract architecture is shown in Fig. 2. Note that this architecture abstraction ignores a number of details about the physical machine, including thread grouping, scheduling, etc. TMM model parameters The TMM model captures the important characteristics of a highly-threaded, many-core architecture by using six parameters shown in Table 1. L is the latency for accessing the slow memory (in our case, the global memory which is shared by all the core groups). P is the total number of cores (or processors) in the machine. C is the maximum chunk size; the number of words that can be read from slow memory to fast memory in one memory transfer. The parameter Z represents the size of fast local memory per core group and Q represents the number of cores per core group. As mentioned earlier, in some instantiations, a core group can have a single core. In this case, a many-core machine looks very much like a multi-core machine with a large number of low-overhead hardware threads. Note that we do not have a parameter for the number of core groups, that quantity is simply P/Q. Finally X is the hardware limit on the number of threads an algorithm is allowed to generate per core. This limit is enforced due to many different constraints, such as constraints on the number of registers each thread uses and an explicit constraint on the number of threads. We unify these constraints into one parameter. In addition to the architecture parameters, we must also consider the parameters which are determined by the algorithm. We assume that the programmer has written a correct synchronous program and taken care to balance the workload across the core groups. These program parameters are shown in Table 2. T1 represents the work of the algorithm, that is, the total number of operations that the program must perform (including fast memory accesses). T∞ represents the span of the algorithm, that is, the total number of operations on the critical path. These are similar to the analogous PRAM parameters of work and time (or depth or critical-path length). Next, we come to program parameters that are specific to many-core programs. M represents the total number of global memory operations performed by the algorithm. Note that this is the total number of operations, not total number of accesses. Since many-core machines often transfer data in large chunks, multiple memory accesses can combine into one memory transfer. For instance, if the many-core machine has a hardware managed cache, and the program accesses data sequentially, then there is only one memory operation for C memory accesses; these will count as one when accounting for M. T is the number of threads created by the program per core. We assume that the work is perfectly distributed among cores. Therefore, the total number of threads in the system is TP. On highly-threaded, many-core architectures, thread switching is used to hide memory latency. Therefore, it is beneficial to create as many threads as possible. However, the maximum number of threads is limited by both the hardware and the program. The software limitation has to do with parallelism, the number of threads per core is limited by T≤T1/(T∞⋅P). The hardware limits T≤X. Finally, we have a parameter S, which is the local memory used per thread. S and T are related parameters, since there is a limited amount of local memory in the system. The number of threads per core is at most T≤Z/(QS). TMM model applicability The TMM model is a high-level abstract model, meant to be applicable to many instantiations of hardware platforms that feature a large number of threads with fast context switching and a hierarchical memory subsystem of at least two levels with a large memory latency gap in between. Typical examples of this set include NVIDIA GPUs, AMD/ATI GPUs, and the uRiKA machine from YarcData. For NVIDIA GPUs, a number of streaming multiprocessors (core groups in our terminology) share the same global memory. On each of these core groups, there are a number of CUDA cores that share a fixed number of registers and on-chip (fast) memory shared among the cores of the core group. A fast hardware-supported context-switching mechanism enables a large number of threads to execute concurrently. Transfers between slow global memory and fast local memory can occur in chunks of at most 32 words; these chunks can only be created if the memory accesses are within a specified range. Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cache [51]. All these features are well captured in the TMM model. Streaming multiprocessors serve the same role as a core group, while CUDA cores are equivalent to the cores defined in TMM. The width of memory access C is 32 due to the coalescing of the threads in a warp. Global memory latency and size of on-chip shared memory/L1 cache are also depicted by L and Z respectively. Considering AMD/ATI GPUs and taking Cypress, the codename for Radeon HD5800 series GPUs, as an example, the architecture is composed of 20 Single-Instruction-Multiple-Data (SIMD) computation engines. In each SIMD engine, there are 16 Thread Processors (TP) and a 32 kB Local Data Store (LDS). Every TP is arranged as a five-way or four-way Very Long Instruction Word (VLIW) processor, and consists of 5 Stream Cores (SC). Low context-switch threading is well supported, and every 64 threads are grouped into a wavefront executing the same instruction. Basically, the SIMD engine can naturally be modeled by core groups. Each SC is modeled as a core in TMM, summing up to 1600 cores totally. LDS is straightforwardly described by the fast local memory of TMM. The width of memory access C in TMM equals to the wavefront width of 64 for AMD/ATI GPUs. The uRiKA system from YarcData is also a potential target for the TMM model. Based on the description from Alverson et al. [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O. Therefore, 128 defines parameter X, the hard limit of number of threads per processor. There can be up to 65,000 threads in a 512 processor system and over 1 million threads at the maximum system size of 8192 processors, so that the latencies are hidden by accommodating many remote memory references in flight. The processor\\'s instruction execution hardware essentially does a context switch every instruction cycle, finding the next thread that is ready to issue an instruction into the execution pipeline. This suggests that the memory access width or chunk size C is 1 on these machines. Threads do not share anything, as the Threadstorm processor has 128 hardware copies of the register set, program counter, stack pointer, etc., necessary to hold the current state of one software thread that is executing on the processor. Conceptually, each of the Threadstorm processors is mapped to a core group in the TMM model but, different than the two GPU architectures, it has only one core on-chip, thus Q equals 1. TMM analysis structure In order to analyze program performance in the TMM model, we must first calculate the program parameters for the particular program. Once we have calculated these values, we can then try to understand the performance of the algorithm. We first calculate the effective work of the algorithm TE. The effective work should consider both work due to computation and work due to memory accesses. Total work due to memory accesses is M⋅L, but since this work is hidden by using threads, the real effective work due to memory accesses is (M⋅L)/T Therefore, we have (1)TE=O(max(T1,M⋅LT)). Note that this expression assumes perfect scheduling (the threads are context swapped with no overhead, as soon as they are stalled) and perfect load balance between threads. The time to execute on P cores is represented by TP and is defined as: (2)TP=O(max(TEP,T∞))=O(max(T1P,T∞,M⋅LT⋅P)). Therefore, speedup on P cores, SP, is (3)SP=T1TP=Ω(min(P,T1T∞,P⋅T1⋅TM⋅L)). For linear speedup, SP should be P. More precisely, for PRAM algorithms, SP=min(P,T1/T∞). Therefore, if the first two terms in the min of Eq. (3) dominate, then a highly-threaded, many-core algorithm\\'s performance is the same as the corresponding PRAM algorithm. On the other hand, if the last term dominates, then the algorithm\\'s performance depends on other factors. If T could be unbounded, then the last term will never dominate. However, as we explained earlier, T is not an unlimited resource and has both hardware and algorithmic upper bounds. Therefore, based on the machine parameters, algorithms that have the same PRAM performance can have different real performance on highly-threaded, many-core machines. Therefore, this model can help us pick algorithms that provide performance as close as possible to PRAM algorithms. Analysis of all pairs shortest paths algorithms using the TMM model In this section, we demonstrate the usefulness of our model by using it to analyze 4 different algorithms for calculating all pairs shortest paths in graphs. All pairs shortest paths is a classic problem for which there are many algorithms. We are given a graph G=(V,E) with n vertices and m edges. Each edge e has a weight w(e). We must calculate the shortest weighted path from every vertex to every other vertex. In this section, we are interested in asymptotic insights, therefore, we assume that the graphs are large graphs. In particular n>Z. Dynamic programming via matrix multiplication Our first algorithm is a dynamic programming algorithm [53] that uses repeated matrix multiplication to calculate all pairs shortest paths. The graph is represented as an adjacency matrix A where Aij represents the weight of edge (i,j). Al is a transitive matrix where Aijl represents the shortest path from vertex i to vertex j using at most l intermediate edges. A1 is the same as the adjacency matrix A and we want to calculate An-1 to calculate all pairs shortest paths. A2 can be calculated from A1 as follows: (4)Aij2=min0≤k<n(Aij1,Aik1+Akj1). Note that, the structure of this equation is the same as the structure of a matrix multiplication operation where the sum is replaced by a min operation and the multiplication is replaced by an addition operation. Therefore, we can use repeated matrix multiplication which calculates An using O(lgn) matrix multiplications. PRAM algorithm and analysis Parallelizing this algorithm for the PRAM model simply involves parallelizing the matrix multiplication algorithm such that each element in the matrix is calculated in parallel. The total work of lgn matrix multiplications using a PRAM algorithm is T1=O(n3lgn). The span of a single matrix multiplication algorithm is O(n). Therefore, the total span of the algorithm is T∞=O(nlgn). The time and speedup using P processors are (5)TP=O(max(n3lgnP,nlgn))(6)SP=Ω(min(P,n2)). Therefore, the PRAM algorithm gets linear speedup as long asP≤n2. TMM algorithm and analysis TMM algorithms are tailored to highly-threaded, many-core architectures generally by using fast on-chip memory to avoid accesses to slow off-chip global memory, coalescing to diminish the time required to access slow memory, and threading to hide the latency of accesses to slow memory. Due to its large size, the adjacency matrix is stored in off-chip global memory. Following traditional block-decomposition techniques, sub-blocks of the result matrix (whose size is denoted by B) are assigned to core groups for computation. The threads in a core group read in the required input sub-blocks, perform the computation of Eq. (4) for their assigned sub-block, and write the sub-block out to global memory. This happens lgn times by repeated squaring. The work and the span of this algorithm remain unchanged from the PRAM algorithm. However, we must also calculate M, the number of memory operations. Let us first consider a single matrix multiplication operation. There are a total of n2 elements and each element is read for the calculation of O(n/B) other blocks. However, due to the regularity in memory accesses, each block can be read fully coalesced. Therefore, the number of memory operations for one matrix multiply is O((n2/C)⋅(n/B))=O(n3/(BC)). Also note that since we must fit a B×B block in a local memory of size Z on one core group, we get B=Θ(Z). Therefore, for lgn matrix multiplication operations, M=O(n3lgn/(Z⋅C)). Now we are ready to calculate the time on P processors. (7)TP=O(max(T1P,T∞,M⋅LT⋅P))(8)=O(max(n3lgnP,nlgn,n3lgn⋅LZ⋅C⋅T⋅P)). Therefore, the speedup on P processors is (9)SP=T1/TP(10)=Ω(min(P,n2,Z⋅C⋅TL⋅P)). We can now compare the PRAM and TMM analysis and note that the speedup is P as long as ZCT/L≥1. We also know that T≤min(X,Z/(QS)), and S=O(1), since each thread only needs constant memory. Therefore, we can conclude that the algorithm achieves linear speedup as long as L≤min(ZCX,Z3/2C/Q). Johnson\\'s algorithm: Dijkstra\\'s algorithm using binary heaps Johnson\\'s algorithm [54] is an all pairs shortest paths algorithm that uses Dijkstra\\'s single source algorithm as the subroutine and calls it n times, once from each source vertex. Dijkstra\\'s algorithm is a greedy algorithm for calculating single source shortest paths. The pseudo-code for Dijkstra\\'s algorithm is given in Algorithm 1 [55]. The single source algorithm consists of n insert operations, m decrease-key operations and n delete-min operations from a min-priority queue. The standard way of implementing Dijkstra\\'s algorithm is to use a binary or a Fibonacci heap to store the array elements. We now consider a binary heap implementation so that each operation (insert, decrease-key, and delete-min) takes O(lgn) time. Note that Dijkstra\\'s algorithm does not work when there are negative weight edges in the graph. PRAM algorithm and analysis A simple parallel implementation of Johnson\\'s algorithm using Dijkstra\\'s algorithm consists of doing each single-source shortest path calculation in parallel. The total work of a single-source computation is O(mlgn+nlgn). For simplicity, we assume that the graph is connected, giving us O(mlgn). Therefore, the total work for all pairs shortest paths is T1=O(mnlgn). The span is T∞=O(mlgn) since each single source computation executes sequentially. The time and speedup using P processors are (11)TP=O(max(mnlgnP,mlgn))(12)SP=Ω(min(P,n)). Therefore, the PRAM algorithm gets linear speedup as long as P≤n. TMM algorithm and analysis The TMM algorithm is very similar to the PRAM algorithm where each thread computes a single source shortest path. Therefore, each thread requires a min-heap of size n. Since n may be arbitrarily large compared to Z/QT (the share of local memory for each thread), these heaps cannot fit in local memory and must be allocated on the slow global memory. The work and span are the same as the PRAM algorithm. We must now compute M. Note that each time the thread does a heap operation, it must access global memory, since the heaps are stored in global memory. In addition, binary heap accesses are not predictable and regular, so the heap accesses from different threads cannot be coalesced. Therefore, the total number of memory operations is M=O(mnlgn). Now we are ready to calculate the time on P processors. (13)TP=O(max(T1P,T∞,M⋅LT⋅P))(14)=O(max(mnlgnP,mlgn,mnlgn⋅LT⋅P)). Therefore, the speedup on P processors is (15)SP=Ω(min(P,n,TL⋅P)). Note that this algorithm gets linear speedup only if T/L≥1. Therefore, the number of threads this algorithm needs to get linear speedup is very large. We know that T≤min(X,Z/(QS)), and S=O(1) for this algorithm. This allows us to conclude that this algorithm achieves linear speedup only if L≤min(X,Z/Q), since each thread needs only constant memory. These conditions are much stricter than those imposed by the dynamic programming algorithm. Johnson\\'s algorithm: Dijkstra\\'s algorithm using arrays This algorithm is similar to the previous algorithm in that it still uses n single-source Dijkstra\\'s algorithm calculations. However, instead of binary heaps, we use arrays to do delete-min and decrease-key operations. PRAM algorithm and analysis The PRAM algorithm is very similar to the algorithm that uses binary heaps. Each single source shortest path is computed in parallel. However, in this algorithm, we simply store the current estimates of the shortest path of vertices in an array instead of a binary heap. Therefore, there are n arrays of size n, one for each single source shortest path calculation. Each decrease-key now takes O(1) time, since one can simply decrease the key using random access. Each delete-min, however, takes O(n) work, since one must look at the entire array to find the minimum element. Therefore, the work of the algorithm is T1=O(n3+mn) and the span is O(n2+m). We can improve the span by doing delete-min in parallel, since one can find the smallest element in an array in parallel using O(n) work and O(lgn) time using a parallel prefix computation. This brings the total span to T∞=O(nlgn+m) while the work remains the same. The time and speedup using P processors is (16)TP=O(max(n3P,nlgn+m))(17)=O(max(n3P,nlgn,m))(18)SP=Ω(min(P,n2lgn,n3m)). TMM algorithm and analysis The TMM algorithm is similar to the PRAM algorithm, except that each core group is responsible for a single-source shortest path calculation. Therefore, all the threads on a single core group (QT in number) cooperate to calculate a single shortest path computation. Since we assume that n>Z, the entire array does not fit in local memory and must be read with each delete-min operation. Therefore, the span of the delete-min operation changes. For each delete-min operation, elements are read into local memory in chunks of size Z. For each chunk, the minimum is computed in parallel in O(lgZ) time. Therefore, the span of each delete-min operation is O((n/Z)lgZ). Therefore, the total span is T∞=O(n2lgZ/Z). The work is the same as the PRAM work. We must now compute the number of memory operations, M. There are n2 delete-min operations in total, and each reads the array of size n coalesced. In addition, there are a total of mn decrease key operations, but these reads cannot be coalesced. Therefore, M=O(n3/C+mn). (19)TP=O(max(T1P,T∞,M⋅LT⋅P))(20)=O(max(n3P,n2lgZZ,(n3C+mn)⋅LT⋅P))(21)=O(max(n3P,n2lgZZ,n3⋅LC⋅T⋅P,mn⋅LT⋅P)). Speedup is (22)SP=Ω(min(P,nZlgZ,C⋅TL⋅P,n2⋅Tm⋅L⋅P)). Again, in this algorithm, T≤min(X,Z/(QS)), and S=O(1) since each thread needs only constant memory. Therefore, the PRAM performance dominates if L≤min(CX,CZ/Q,n2X/m,n2Z/(mQ)). n iterations of Bellman-Ford algorithm This is another all pairs shortest paths algorithm that uses a single-source Bellman-Ford algorithm as a subroutine. The algorithm is given in Algorithm 2 [56,57]. PRAM algorithm and analysis Again, one can do each single source computation in parallel. Each single source computation takes O(mn) work, making the total work of all pairs shortest paths O(mn2) and the total span O(mn). One can improve the span by relaxing all edges in one iteration in parallel making the span O(n). (23)TP=O(max(mn2P,n)).(24)SP=Ω(min(P,mn)). TMM algorithm and analysis The TMM algorithm for this problem is more complicated and requires more data structure support. Each core group is responsible for one single-source shortest path calculation. For each single source calculation, we maintain three arrays, A,B and W, of size m, and one array D of size n. D contains the current guess of the shortest path to vertex i. B contains ending vertices of edges, sorted by vertex ID. Therefore B may contain multiple instances of the same vertex if that vertex has multiple incident edges. A[i] contains the starting vertex of the edge that ends at B[i] and W[i] contains the weight of that edge. Therefore, both D and B are sorted. Each thread deals with one index in the array and relaxes that edge in each iteration. All threads relax edges in parallel in order of B. The total work and span are the same as the PRAM algorithm. We can now calculate the time and speedup assuming threads can read all the arrays coalesced, M=O(mn2/C+n3/C)=O(mn2/C) for connected graphs. (25)TP=O(max(T1P,T∞,M⋅LT⋅P))(26)=O(max(mn2P,n,mn2⋅LC⋅T⋅P)). Therefore, the speedup on P processors is (27)SP=Ω(min(P,mn,C⋅TL⋅P)). In this case, we get linear speedup if CT/L≥1. Subject to the limits on threads of T≤min(X,Z/(QS)) and S=O(1) for constant local memory usage per thread, this requires L≤min(CX,CZ/Q). Comparison of the various algorithms As our analysis of shortest paths algorithms indicates, the TMM model allows us to take the unique properties of highly-threaded, many-core architectures into consideration while analyzing the algorithms. Therefore, the model provides more nuance in the analysis of these algorithms for the highly-threaded, many-core machines than the PRAM model. In this section, we will compare the running times of the various algorithms and see what interesting things this analysis tells us. Table 3 indicates the running times of the various algorithms in both the PRAM model and the TMM model, as well as the conditions under which TMM results are the same as the PRAM results. We have ignored the span term, since the span is small relative to work in all of these algorithms. As we can see, if L is small, then highly-threaded, many-core machines provide PRAM performance. However, the cut-off value for L is different for different algorithms where the performance in the TMM model differs from the PRAM model is different for different algorithms. Therefore, the TMM model can be informative when comparing between algorithms. We will perform two types of comparison between these algorithms in this section. The first one considers the direct influence of machine parameters on asymptotic performance. Since machine parameters do not scale with problem size, in principle, machine parameters cannot change the asymptotic performance of algorithms in terms of problem size. That is, if the PRAM analysis indicates that some algorithm has a running time of O(n) and another one has the running time of O(nlgn), for large enough n, the first algorithm is always asymptotically better since eventually lgn will dominate whatever machine parameter advantage the second algorithm may have. Therefore, for this first comparison, we only compare algorithms which have the same asymptotic performance under the PRAM model. Second, we will also do a non-asymptotic comparison where we compare algorithms when the problem size is relatively small, but not very small. In particular, we look at the case when lgn<Z. In this case, even algorithms that are asymptotically worse in the PRAM model can be better in the TMM model, for large latency L. In the next section, we will look at even smaller problem sizes where the effects are even more dramatic. Influence of machine parameters As the table shows, the limits on machine parameters to get linear speedup are different for different algorithms. Therefore, even when two algorithms have the same PRAM performance, their performance on highly-threaded, many-core machines may vary significantly. Let us consider a few examples: Dynamic programming vs. Johnson\\'s algorithm using binary heaps when m=O(n2) If m=O(n2) (i.e., the graph is dense), the PRAM performance for both algorithms is the same. However whenZ/Q<L<Z3/2C/Q, Johnson\\'s algorithm has a significantly worse running time. Take the example of L=O(Z3/2C/Q). The Johnson running time is O(n3lgnZC/P) while the running time of the dynamic programming algorithm is simply O(n3lgn/P). Johnson\\'s algorithm using binary heaps vs. Johnson\\'s algorithm using arrays when m=O(n2/lgn) If m=O(n2/lgn) (i.e., a somewhat sparse graph), these two algorithms have the same PRAM performance, but if Z/Q<L≤ZC/Q, then the array implementation is better. For L=ZC/Q, the binary heap implementation has a running time of O(n3C/P), while the array implementation has a running time of simply O(n3/P). Influence of graph size The previous section shows the asymptotic power of the model; the results there hold for large sizes of graphs asymptotically. However, the TMM model can also help decide on what algorithm to use based on the size of the graph. In particular for certain sizes of graphs, algorithm A can be better than algorithm B even if it is asymptotically worse in the PRAM model. Therefore, the TMM model can give us information that the PRAM model cannot. Consider the example of dynamic programming vs. Johnson\\'s algorithm using arrays. In the PRAM model, the dynamic programming algorithm is unquestionably worse than Johnson\\'s. However, if lgn<Z, we may have a different conclusion. In this case, dynamic programming has runtime: (28)n3lgn⋅LZCTP=n2LTP⋅nlgnZC<n2LTP⋅nC. While Johnson\\'s algorithm has runtime: (29)min(n3LCTP,mnLTP)=n2LTP⋅min(nC,mn). If n2/m<C, i.e. dense graphs, n/C<m/n. Combine (28) and (29), we have (30)n3lgn⋅LZCTP<n3LCTP,if n2m<C. This indicates that when for small enough graphs where lgn<Z, there is a dichotomy. For dense graphs n2/m<C, the dynamic programming algorithm should be preferred, while for sparse graphs, Johnson\\'s algorithm with arrays is better. We illustrate this performance dependence on sparsity with experiments in Section 7. We get a similar result when comparing the dynamic programming algorithm with Bellman-Ford when m=O(n). In spite of being worse in the PRAM world, the dynamic programming algorithm is better when lgn<Z. Our model therefore allows us to do two things. First, for a particular machine, given two algorithms which are asymptotically similar, we can pick the more appropriate algorithm for that particular machine given its machine parameters. Second, if we also consider the problem size,then we can do more. For small problem sizes, the asymptotically worse algorithm may in fact be better because it interacts better with the machine. We will draw more insights of this type in the next section. Effect of problem size In Section 5, we explored the asymptotic insights that can be drawn from the TMM model. However, the TMM model can also inform insights based on problem size. In particular, some algorithms can take advantage of smaller problems better than others, since they can use fast local memory more effectively. In this section, we explore the insights that the TMM model provides in these cases. Vertices fit in local memory When n<Z, all the vertices fit in local memory. Note that this does not mean that the entire problem fits in local memory, since the number of edges can still be much larger than the number of vertices. In this scenario, the number of memory accesses by the first, second, and fourth algorithms is not affected at all. In the dynamic programming algorithm, we consider the array of size n2 and being able to fit a row into local memory does not reduce the number of memory transfers. In Johnson\\'s algorithm using binary heaps, each thread does its own single source shortest path. Since the local memory Z is shared among QT threads, each thread cannot hold its entire vertex array in local memory. In the Bellman-Ford algorithm, the cost is dominated by the cost of reading the edges. Therefore, the bounds do not change. For Johnson\\'s algorithm using arrays, the cost is lower. Now each core group can store the vertex array and does not need to access it from slow memory. Therefore the bound on the number of memory operations changes to M=O(n2/C+mn)=O(mn) for connected graphs. For these small problem sizes, the TMM model can provide even more insight. As an example, compare the two versions of Johnson\\'s algorithm, the one that uses arrays and the one that uses heaps. When m=O(n2/lg2n), the algorithm that uses heaps is better than the algorithm that uses arrays in the PRAM model. But in the TMM model, for large L, the algorithm that uses heaps has the running time of O(Lmnlgn/(TP))=O(Ln3/(TPlgn)), while the algorithm that uses arrays has the running time of O(Ln3/(TPlg2n)). Therefore, the algorithm that uses arrays is better. Note that asymptotic analysis is a little dubious when we are talking about small problem sizes; therefore, this analysis should be considered skeptically. However, the analysis is rigorous when we consider the circumstance that local memory size grows with problem size (i.e., Z is asymptotic). Moreover, this type of analysis can still provide enough insight that it might guide implementation decisions under the more realistic circumstance of bounded (but potentially large) Z. Edges fit in the combined local memories When m=O(PZ/Q), the edges fit in all the memories of the core groups combined. Again, the running time of the first,second, and third algorithms do not change, since they cannot take advantage of this property. However, the Bellman-Ford algorithm can take advantage of this property and each thread across all core groups is responsible for relaxing a single edge. Now a portion of the arrays A,B and W fit in each core group\\'s local memory and they never have to be read again. Therefore, the number of memory operations reduces to M=O(n3/C). And the run time under the TMM model reduces to O(n3L/(CTP)). Again, compare Bellman-Ford algorithm with Johnson\\'s algorithm using binary heaps. When m=O(n2/lgn), Johnson\\'s algorithm is better than the Bellman-Ford algorithm in the PRAM model. However, in the TMM model, Johnson\\'s has run time of O(Lmnlgn/(TP))=O(Ln3/(TP)), while Bellman-Ford with a run time of O(Ln3/(CTP)) flips to be the better one. Empirical investigation In this section, we conduct experiments to understand the extent of the applicability of our model in explaining the performance of algorithms on a real machine. This evaluation is a proof-of-concept that the model successfully predicts performance on one example of a highly-threaded, many-core machine. It is not meant to be an exhaustive empirical study of the model\\'s applicability for all instances of highly-threaded, many-core machines. We implemented two all-pairs shortest paths algorithms: the dynamic programming using matrix multiplication and Johnson\\'s algorithm using arrays, on an NVIDIA GPU. In these experiments, we investigate the following aspects of the TMM model: • Effect of the number of threads: the fact that the TMM model incorporates the number of threads per processor in the model is the primary differentiator between the PRAM and TMM models. The TMM model predicts that as the number of threads increases the performance increases, up to a certain point. After this point, the number of threads does not matter, and the TMM model behaves the same as the PRAM model. In this set of experiments, we will use both the dynamic programming and Johnson\\'s algorithms to demonstrate this dependence on the number of threads. • Effect of fast local memory size: in some algorithms, including the dynamic programming via matrix multiplication, the size of the fast memory affects the performance of the algorithm in the TMM model. We investigate this dependence. • Comparison of the dynamic programming algorithm and Johnson\\'s algorithm with arrays: for Johnson\\'s algorithm using arrays, the PRAM performance does not depend on the graph\\'s density. However, the TMM model predicts that performance can depend on the graph\\'s density, when the number of threads is insufficient for the performance to be equivalent to the PRAM model. Therefore, even though Johnson\\'s algorithm is always faster than the dynamic programming algorithm according to the PRAM model (since its work is n3 while the dynamic programming algorithm has work n3lgn), the TMM model predicts that when the number of threads is small, the dynamic programming algorithm may do better, especially for dense graphs. We demonstrate through experiments that, this is a true indicator of performance. Experimental Setup The experiments are carried out on an NVIDIA GTX 480, which has 15 multiprocessors, each with 32 cores. As a typical highly-threaded, many-core machine, it also features a 1.5 GB global memory and 16 kB/48 kB of configurable on-chip shared memory per multiprocessor, which can be accessed with latency significantly lower than the global memory. Runtimes are measured across various configurations of each problem, including graph size, thread count, shared memory size, and graph density. When plotted as execution time, the performance units are in seconds. In many cases, however, the trends we wish to see are more readily apparent when performance is shown in terms of speedup rather than execution time. This poses a problem, however, as it is arguably meaningless to attempt to realistically measure the single-core execution time of an application deployed on a modern GPU. We address this issue using the following technique: all speedup plots compare the measured, empirical execution time on P cores to the theoretical, asymptotic execution time on 1 core using the PRAM model. As a result, the speedup axis does not represent a quantitatively meaningful scale, and the scale is labeled \"arbitrary\" on the graphs to reflect this fact; however, the shape of the curves are representative of the speedup achievable relative to a fixed serial execution time. Effect of the number of threads The TMM model indicates that when the number of threads is small, the performance of algorithms depends on the number of threads. With sufficient number of threads, the performance converges to the PRAM performance and only depends on the problem size and the number of processors. We verify this result using both the dynamic programming and Johnson\\'s algorithms. For the dynamic programming algorithm, we generate random graphs with {1k,2k,4k,8k,16k} vertices. To better utilize fast local memory, the problem is decomposed into sub-blocks, and we must also pick a block size. Since we only care about the effect of threads and not the effect of shared memory (to be considered in the next subsection), here we show the results with a block size of 64, as it allows us to generate the maximum number of threads. We increase the number of threads until we reach either the hardware limit or the limit imposed by the algorithm. Fig. 3 shows the speedup while varying the number of threads per core. We see that the speedup increases approximately linearly with the number of threads per core (as predicted by Eq. (10)) and then flattens out. This indicates that for this experiment, 16 is an estimated threshold of threads/core where the TMM model switches to the \"PRAM range\" and the number of threads no longer matters. Note that the expression for this threshold does not depend on the graph size, as it is equal to L/ZC. Also note that the speedup (both in and out of the PRAM range) is not impacted by the size of the graph (again as predicted by Eq. (10)). We see a similar performance dependence on the number of threads in Johnson\\'s algorithm. Here we ran experiments with 8k vertices and varied the number of edges (ranging between 32k and 32 M). The speedup graph is shown in Fig. 4. As we increase the number of threads, the speedup increases. We see two other interesting things, however. First, we never see the flattening of performance with increasing thread counts that is seen with the dynamic programming algorithm. Therefore, it appears that Johnson\\'s algorithm requires more threads to reach the PRAM range where the performance no longer depends on the number of threads. This is also predicted by our model as the number of threads/core required by the dynamic programming algorithm to reach PRAM range is T≥L/ZC while the corresponding number of threads required by Johnson\\'s is T≥L/C, clearly a larger threshold. Johnson\\'s algorithm is not taking advantage of the fast local memory, and this factor influences the number of threads required to hide the latency to global memory. Second, we see that the performance depends on the number of edges. This is consistent with the fact that we are in the TMM range where the runtime is (mnL/TP) and not in the PRAM range where the runtime only depends on the number of vertices. The dependence on graph density is explored further in Fig. 5. Here, the runtime is plotted vs. number of graph edges for varying threads/core. The linear relationship predicted by the last term of Eq. (21) (for dense graphs) is illustrated clearly in the figure. Effect of fast local memory size In highly-threaded, many-core machines, access to local memory is faster than access to slow global memory. Among our shortest paths algorithms, only the dynamic programming algorithm makes use of the local memory and the running time depends on this fast memory size. In this experiment we verify the effect of this fast memory size on algorithm performance. We set the fast memory size on our machine and measure its effect. Fig. 6 illustrates how this change has an impact on speedup across a range of threads/core. For a fixed Z (fast memory size), the maximum sub-block size B can be determined. Then, varying thread counts has the same effect as previously illustrated in Fig. 3, increasing threads/core increases performance until the PRAM range is reached. But as we can see from the figure, different block sizes have different performance for the same number of threads/core. This effect is predicted by Eq. (10). As we increase the size of local memory, the performance improves, since we can use bigger blocks. In order to isolate the effect of block size from the effects of other parameters, we also plot this data in a pair of different formats in Figs. 7 and 8, in both cases limiting the number of threads/core to below the PRAM range (i.e., the range where speedup is linear in threads/core). The first curve shows the difference between the speedups for different block sizes. As the curve indicates, the delta speedup increases linearly with the number of threads/core, consistent with the model prediction of (B1-B2)T. The second curve shows the ratio of the performance of block size 64 to block size 32, indicating a flat line, since the thread term cancels out. Comparison between the dynamic programming and Johnson\\'s algorithms It is interesting to compare the dynamic programming algorithm and Johnson\\'s algorithm with arrays, since the PRAM and the TMM model differ in predicting the relative performance of these algorithms. The PRAM model predicts that Johnson\\'s algorithm should always be better. However, from Section 5.2, for a small number of threads/core working on a dense graph, the TMM model predicts that dynamic programming may be better. For the graphs with 8k vertices that we explored earlier, lgn<Z. Consequently, TMM predicts Johnson\\'s algorithm is generally faster than dynamic programming for sparse graphs, but slower for relatively dense ones. Fig. 9 demonstrates this effect concretely. In addition, for the dense graph, the figure also shows the intersection between the runtime curves of the two algorithms. At that point (32 threads/core), dynamic programming has already been in the PRAM range with stable performance since 16 threads/core, while Johnson\\'s has not. Its runtime is still benefiting by increasing the threads/core. As a result, we predict that Johnson\\'s runtime will flip to be the better one if given sufficient threads. The peak performance of Johnson\\'s being better than that of dynamic programming is consistent with what the PRAM model predicts. Conclusions In this paper, we present a memory access model, called the Threaded Many-core Memory (TMM) model, that is well suited for modern highly-threaded, many-core systems that employ many threads and fast context switching to hide memory latency. The model analyzes the significant factors that affect performance on many-core machines. In particular, it requires the work and depth (like PRAM algorithms), but also requires the analysis of the number of memory accesses. Using these three values, we can properly order algorithms from slow to fast for many different settings of machine parameters on highly-threaded, many-core machines. We analyzed 4 shortest paths algorithms in the TMM model and compared the analysis with the PRAM analysis. We find that algorithms with the same PRAM performance can have different TMM performance under certain machine parameter settings. In addition, for certain problem sizes which fit in local memory, algorithms which are faster on PRAM may be slower under the TMM model. Further, we implemented a pair of the algorithms and showed empirical performance is effectively predicted by the TMM model under a variety of circumstances. Therefore, TMM is a model well-suited to compare algorithms and decide which one to implement under particular environments. To our knowledge, this is the first attempt to formalize the analysis of algorithms for highly-threaded, many-core computers using a formal model and asymptotic analysis. There are many directions of future work. One obvious direction is to design more algorithms under the TMM model. Ideally, this model can help us come up with new algorithms for highly-threaded, many-core machines. Empirical validation of the TMM model across a wider number of physical machines and manufacturers is also worth doing. In addition, our current model only incorporates 2 levels of memory hierarchy. While in this paper we assume that it is global memory vs. memory local to core groups, in principle, it can be any two levels of fast and slow memory. We would like to extend it to multi-level hierarchies which are becoming increasingly common. One way to do this is to design a \"parameter-oblivious\" model where algorithms do not know the machine parameters. Other than the dynamic programming algorithm, all of the algorithms presented in this paper are, in fact, parameter-oblivious. And matrix multiplication in the dynamic programming can easily be made parameter-oblivious. In this case, the algorithms should perform well under all settings of parameters, allowing us to apply the model at any two levels and get the same results. Acknowledgments This work was supported by NSF grants CNS-0905368 and CNS-0931693 and Exegy, Inc.  ', 'Introduction Human induced pluripotent stem cells (hiPSC) are generated from reprogrammed fibroblasts by overexpression of pluripotency factors (Takahashi et al., 2007; Yu et al., 2007). These pluripotent cells have the unique characteristic to self renew in vitro while maintaining the capacity to differentiate into a broad number of cell types. By combining these unique properties, hiPSC could enable the generation of large quantity of cells for clinical applications. Furthermore, the possibility of generating hiPSC from somatic cells using epigenetic reprogramming represents a unique opportunity for personalized regenerative medicine. Indeed, these pluripotent stem cells could enable the production of patient specific cell types that are fully immuno-compatible with the original donor thereby avoiding the need for immune suppressive treatment after cell transplantation. Nevertheless, the practical, financial and temporal obstacles in producing and validating personalized clinical-grade hiPSC and their differentiated progeny will almost certainly limit the feasibility of this approach. These limitations could selectively restrict patient access to autologous cell-based therapies (Faden et al., 2003). The creation of clinical banks of hiPSC from donors that can provide HLA matching to recipients is proposed as a strategy to attenuate the host immune response to transplanted tissue (Lin et al., 2009; Taylor et al., 2005, 2011). Similarly, hiPSC can be used to develop in vitro disease models, allowing large scale studies otherwise restricted due to the limited availability of primary cells and biopsy material. This application has been proven useful to model neurodegenerative diseases, cardiac syndromes and metabolic disorders in vitro for basic studies and drug screening (Ebert et al., 2009; Moretti et al., 2010; Rashid et al., 2010). However, each of these applications requires large quantity of hiPSC produced in reproducible and standardized ways. Indeed, expansion of hiPSC remains time and resources consuming while experimental variability due to human intervention is almost systematic. Process automation has been a key mechanism to achieve controlled and standardized cell production. Successful automated protocols have been developed for the expansion of human mesenchymal stem cells (Thomas et al., 2007) and human embryonic stem cells (hESC) (Thomas et al., 2009b). Scale up automation enables scale out for conventional formats with predictable process variation and quality outcome by removing manual interventions. However, little work has been done in developing technologies for automation and scale up of hiPSC for healthcare applications. Several solutions and technologies have been developed for live cell production in suspension platforms (Ratcliffe et al., 2012). However these are not readily adapted for cells growing in adherent conditions. Furthermore, large scale production of hiPSC for clinical applications would require expansion in culture using clinically compatible conditions in a reproducible way without loss of function and in sufficient numbers to create reproducible and cost effective therapeutic products. Finally, passaging represent the main difficulty to develop an automation platform to expand hiPSC since these cells must be propagated as aggregates/clumps to maintain their integrity and quality (Beers et al., 2012). Indeed, evidence indicates that hiPSC grown and harvested as single cells are more likely to acquire genetic anomalies (Amps et al., 2011). Consequently, standardization of cell counting and cell clump size measurement has proven to be impractical. Here, we have addressed all these issues by transferring an established manual method to grow hiPSC in feeder free and chemically defined medium onto an automated platform compatible with large scale production. This study shows for the first time that large scale automated production of hiPSC is possible without the need of single cell dissociation thereby respecting their natural properties. Materials and methods Manual maintenance and passage of hiPSC in feeder free and chemically defined medium hiPSC were cultured in feeder-free conditions using chemically defined medium (CDM-PVA) with Activin A (10ngml-1, R&D System) and FGF2 (12ngml-1, R&D Systems) - iPSC medium, as previously described (Brons et al., 2007). The composition of CDM-PVA was 50% IMDM (Gibco) added to 50% F12+GlutaMax-1 (Gibco), supplemented with 1% lipid concentrate (Gibco), 7μgml-1 of insulin (Roche), 15μgml-1 of transferrin (Roche), 450μM of monothioglycerol (Sigma) and 1mgml-1 of Polyvinyl Alcohol (Sigma). Cells were harvested after 6 or 7 days of culture (dependent on visually confirmed confluence) using 1mgml-1 collagenase IV (Gibco) and 1mgml-1 dispase (Gibco). Detached colonies were aspirated and pooled into a conical tube and washed with CDM-PVA medium. A second wash step was performed before colonies were gently broken down into smaller cell aggregates (clumps) by pipetting and allowed to settle under gravity. It must be noted here that the aim was only to reduce the size of the aggregates and not to reduce them to single cells. Cell clumps were plated at 1:10 split ratio to 0.1% porcine gelatin plates (Sigma), pre-coated with mouse embryonic fibroblast medium containing 10% FBS (Biosera) for 24h at 37°C and 5% (v/v) CO2. iPSC medium and 10μM of Y27632 (Sigma) was added for the first 48h. Following this, maintenance medium was replaced daily until readiness for the next passage. This protocol is routinely used for 6 well plates and T-25 flasks (Fig. 1). Manufacturing platform and instrumentation The CompacT SelecT (The Automation Partnership, UK) is a fully automated cell culture platform which incorporates a small six-axis anthropomorphic robotic arm (Cell Therapy Manufacturing Facility, 2011) that can access 90 T175 flask and plate incubators, controlled at 37°C under an atmosphere of 5% (v/v) CO2 and humidity. The system allows the automation of seeding, feeding and other cell culture processes in order to maintain cell lines in standard T175 cell culture flasks. Flasks are bar-coded for identification and cell process tracking. Two flask decappers and flask holders, automated medium pumping and an automatic cell counter (Cedex®, Roche Innovatis AG, Germany) are integrated within a high-efficiency particulate air (HEPA) filtered cabinet to ensure sterility. At Loughborough University, the CompacT SelecT has been successfully used to culture many different cell types including human mesenchymal cells and human embryonic stem cells (hESC) (Thomas et al., 2007, 2009a,b). The CompacT SelecT has also been shown to be successful at preventing contamination when the GMP version of the CompacT SelecT passed the \"sterile fill\" runs (Chandra et al., 2012). The CompacT SelecT allows activities during cell culture such as seeding, media changes and measurement cells in a controlled environment (Thomas, 2012). Thus this platform can be used to expand and differentiate batches of cells to a tighter specification than manual cell culture (Liu et al., 2010). Automated passage of hiPSC in feeder free and chemically defined medium using CompacT SelecT The automation enables scale out for conventional formats with predictable process variation and quality outcome by removing manual interventions. The CompacT SelecT is a preferred platform for development process friendly method of automating the culture of cells that grow in adherent conditions. The automation step mimics the manual process and is therefore demonstrably similar to the manual cell culture steps. For many manual cell culture protocols, there is a centrifugation step to concentrate the cell suspension and allow for cells to be washed. However, in this instance, cells grow in aggregates and do not require centrifugation as they settle under gravity. In order to transfer the culture protocol to the CompacT SelecT it was necessary to scale up from a T25 to a T175 flask, media volumes were scaled proportionally to flask surface area, and work within the restricted set of plasticware and the allowable positioning of the plasticware within the automated system (Fig. 1). Operating conditions of the manual culture process were followed as closely as possible (temperature, timing, splitting ratio, mixing, volumes), however a number of detailed changes had to be made to the manual protocol as are discussed later in the manuscript. Differentiation of hiPSC into endoderm, mesoderm and neuroectoderm hiPSC were plated into gelatin plates pre coated with 10% FBS and maintained for 24h in iPSC medium before inducing differentiation into the three germ layers: endoderm, mesoderm and neuroectoderm as described previously (Vallier et al., 2009). Differentiation was induced by supplementing CDM-PVA with Activin (R&D System), bFGF (R&D System), BMP4 (R&D System), LY294002 (Promega), SB431542 (Tocris) and CHIR99021 (Stemgent) at different times and concentrations (Supplementary Online Material). Supplementary material related to this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbiotec.2013.12.009. Table S1 Detailed protocol for inducing differentiation of hiPSC in the three germ layers: endoderm, mesoderm and neuroectoderm. RPMI-B27 composition was 98% RPMI 1640+GlutaMax (Gibco), 2% of B-27 supplement (Gibco) and 1% MEM non-essential amino acids (Gibco). Immunochemistry hiPSC were fixed for 20min at 4°C in 4% paraformaldehyde (PFA) and washed three times in PBS. Cells were incubated 1h at room temperature in PBS containing 10% donkey or goat serum (depending on antibody, Serotec), for intra-cellular epitopes 0.1% Triton X-100 (Sigma) was added to the blocking solution. Cells were then subsequently incubated with primary antibodies diluted in 1% serum in PBS overnight at 4°C. Dilutions were as follows: TRA-1-60 (Santa Cruz, 1:100), OCT4 (Santa Cruz, 1:100), NANOG (R&D Systems, 1:100), and anti-SOX2 (R&D Systems, 1:100), SOX17 (R&D Systems, 1:200), EOMES (Abcam, 1:100), BRAC (R&D Systems, 1:100), MIXL1 (Abcam, 1:100), Nestin (Abcam, 1:100). Cells were washed 3 times in PBS and incubated with secondary antibodies, Alexa Fluor 568 (Invitrogen) and Alexa Fluor 488 (Invitrogen), for 2h at room temperature. Cells were washed three times with PBS and stained with Hoechst 33258 (Sigma, 1:10,000 diluted in PBS) for 5min. The cells were viewed in PBS using a Zeiss Axiovert 200M microscope. RNA extraction, reverse-transcriptase PCR and quantitative PCR Total RNAs were extracted from hiPSC using the GenElute Mammalian Total RNA Miniprep Kit (Sigma). For each sample 0.5μg of total RNA was reverse transcribed using Superscript II reverse transcriptase (Invitrogen). RNA and primer were denatured at 65°C for 5min and RT-PCR reaction mixtures incubated at 25°C for 10min, 42°C for 50min and 70°C for 10min. qPCR was performed using Sensi Mix Sybr Low Rox Kit (Bioline). A negative control that contained only water and a positive control that contained RNA extracted from human embryonic stem cells (H9-WiCell) were also ran. The expression of the PBGD housekeeping gene was used to normalize qPCR reactions. Results and discussion An human induced pluripotent stem cell line - BBHX8 (Vallier et al., 2009), derived from adult human fibroblasts using retroviral reprogramming with OCT4, SOX2, KLF4 and c-MYC was derived and manually maintained at the Anne McLaren Laboratory for Regenerative Medicine (LRM), University of Cambridge. In order to develop an automation process to expand hiPSC using CompacT SelecT, the hiPSC line was transferred to the Centre for Biological Engineering (CBE), Loughborough University where it was further cultured and passaged in T25 flasks using the manual protocol established at LRM (Brons et al., 2007). This line was subsequent scaled up to T175 flasks (Fig. 1). When transferring to T175 flasks, two major limitations of the CompacT SelecT were taken into consideration: (i) as an alternative to the use of conical tubes the T175 flask was used to wash and pellet the cell clumps by gravity. This removed a process step but required a protocol modification that increased the time required for settling of aggregates. (ii) 10ml pipettes were used instead of 1000μl tips when breaking the cell clumps to a desirable size (Fig. 2). Cells were maintained using the manual protocol in both T25 and T175 flasks to show process transfer and scale up within both facilities. The manual process of expanding T175 flasks was executed for three weeks to gain confidence in the protocol before cells were transferred to the CompacT SelecT. A split ratio of 1:10 was maintained and the cell clumps broken to the appropriate size by the operator using a 10ml pipette (Table 1). However it become evident that mimicking the breaking of cell clumps to the desirable size by the operator would be the most difficult step to automate. To adapt the T175 manual protocol to the CompacT SelecT, four iterations of the automation protocol were written and tested in the CompacT SelecT. The changes made in the four versions of the automated protocol were based on: (a) the time for the colonies to settle down by gravity after washing, (b) the distance of the pipette from the bottom of the flask when removing diluted enzyme solution without aspirating the cells clumps, (c) the pipetting speed and number of mixing steps required to achieve an homogeneous cell suspension of the desirable cell clump size without the presence of an undue number of single cells, and (d) the split ratio (Table 2). To take into account the small volumes of some reagents and to prevent inaccuracies of the dispensed volumes by a 10ml pipette or 1.6mm bore tubing, the media formulation was mixed manually. Therefore, the automation platform was used to dispense the complete media. In the first version (A) of the automated protocol, several cell clumps were lost during the washing steps as the time for cell clumps to settle was insufficient, additionally several clumps were aspirated due to the pipette reaching close to the bottom of the flask when aspirating the enzymatic solution. Care is essential during the two washes in order to dilute the enzyme concentration without aspirating cell clumps. Moreover the low speed of mixing was insufficient to break the clumps to the desired size. A higher split ratio was used when comparing to manual protocol. This was chosen to account for any potential cell loss during the automation steps described in the above paragraph (a)-(c). The following versions (B-D) looked at improving Version A (Table 2) and subsequently the outcome of the passage. It proved difficult to use the CEDEX cell counter to count clumps of cells in order to passage cells based on cell count as the CEDEX could not count the clumps accurately (data not shown). This would have improved process reproducibility, however the methods available for counting cell aggregates/clump are not yet suitable to be integrated within an automation platform. A feature of the CompacT SelecT is its 90 T175 flask carousel incubators. While the robot placed the flasks gently in the incubator carousel without disturbing the cells, as soon as the carousel was rotated, cell clumps moved under centrifugal force to concentrate in parts of the flask resulting in a heterogeneous distribution of the cell clumps/colonies in the flask; this in particular can result in the growth of differentiated colonies. This observation showed the importance of leaving the unattached cells in an undisturbed condition and that even the slightest movement can result in a heterogeneous distribution of the colonies. Of the four flasks/protocol versions (A-D) tested in the CompacT SelecT, C was the best protocol based on colony morphology and absence of differentiation 7 days after passage (Fig. 2). To further assess the quality of these cells when compared to manual protocols (T25Manual and T175Manual), expression of pluripotency markers was analyzed using qPCR and immunochemistry. These analyses showed high levels and homogenous expression of pluripotency markers and morphology characteristic of hiPSC for both manual and automated protocols (Fig. 3). In addition, capacity of differentiation toward the three germ layers was confirmed by growing the hiPSC into culture conditions inductive for endoderm, mesoderm and neuroectoderm differentiation. Both manual protocols in T25 flasks and the automated protocol in T175 flasks showed homogeneous differentiation of hiPSC into the three germ layers as observed by the change of cell morphology and by the expression of endoderm markers: SOX17 and EOMES, mesoderm markers: BRACHURY and MIXL1 and neuroectoderm markers: NESTIN and SOX2 (Fig. 4). These results suggest that cells expanded with our automation protocol retain the capacity to produce cell types with a clinical interest. Considered together, these data suggest that the transfer of protocols between the two facilities was successful and that both scale-up and automation protocols were shown to maintain cell function comparable to manually passaged hiPSC. Conclusion In conclusion, we have successfully demonstrated a protocol passaging hiPSC in an automated system, the CompacT SelecT, with cells maintained as aggregates. This work has shown that hiPSC can be passaged in an automated system without losing their pluripotent capabilities. Cells maintained their characteristic hiPSC morphology and expressed pluripotency markers both by immunochemistry and qPCR. Additionally, these cells maintained the capacity to differentiate into the three germ layers (endoderm, mesoderm and neuroectoderm). The comparison between manual and automated conditions showed the maintenance and passaging of hiPSC is feasible using the CompacT SelecT system, however to improve reproducibility some detailed changes will have to be implemented. Further work is necessary to define more accurately the critical protocol parameters and take account of the constraints required when generating fully functional hiPSC lines for clinical applications. With respect to future automated solutions incubator design must take into account the current necessity of not disturbing cells that require to be grown as aggregates, such as hiPSC and that take time (potentially up to 24h) to attach. Automation would most certainly benefit from development of (conical) plastic vessels of the volume necessary to accommodate cell suspensions from T175 flasks as this could reduce the time for cell clumps to settle and thus speeding the passaging protocol. The option to use a pipette with smaller bore than a 10ml pipette would assist the optimization of breaking cell clumps to the desirable size and reduce the single cell debris caused by pipetting. While these modifications would be required to optimize the present protocol to ensure maximum yields of cells, this study represents a proof of principle that hiPSC can be expanded in clumps using automation without loss of quality. Acknowledgements The authors acknowledge the contributions of the following: \\'Fundação para a Ciência e a Tecnologia\\' (SFRH/BD/69033/2010) and the Cambridge Hospitals National Institute for Health Research Biomedical Research Center, EPSRC Centre for Innovative Manufacturing in Regenerative Medicine and TAP Biosystems.  ', 'Introduction Soil represents a temporary reservoir for phosphorus (P) in which its availability affects plant growth and biological processes (Lair et al., 2009). Soil phosphorus (P) is an essential element for plant growth but is often slowly available to plants within the soil environment. This is mainly due to soil P being sorbed to the soil reactive clay surfaces, Al and Fe oxides, carbonates, organic matter. The soil pH then determines the chemical complexion of P (Torrent, 1997; Borggaard et al., 2004). At a soil pH above 5.5 most soil phosphate reacts with calcium and at a pH below 5.5 it will react with Al and Fe oxides leaving P only slowly available to plants. Historically crop production did rely on natural availabilities of soil phosphorus (P) and input from organic manure. However with the increased food demand, improved agrotechnology and availability of mineral P forms in the 20th and 21st centuries, fertilizer P application became the substantial source of soil P (Cordell et al., 2009). In developed countries P accumulation took place in the past decades, due to high doses of P fertilization (Lemercier et al., 2008). Although the impact of P input to soils had a positive impact on crop production the impact on the environment such as eutrophication has become a problem within Europe (Csathó et al., 2011). Additionally the world\\'s P supply is both finite and non-renewable (Jordan-Meille et al., 2012) which has caused tension within global P markets (IFA, 2012). Hence, P fertilizer usage must be carried out to secure a sustainable environment and best possible utilization by crops. To meet these challenges fertilizers recommendations to farmers become a common practice worldwide generally optimizing fertilizer doses to sustain a desired yield without a load to the environment. Consequently, soil P recommendation systems are widely used around the world to ensure good soil management and nutrient efficiency promoting agricultural sustainability. However recommendation systems differ considerably among countries. Not many systems can be found as peer reviewed literature; however a brief overview on those available is hereby given. Phosphorus recommendation systems are commonly used in Brazil, in a country where soils are generally nutrient poor. The Brazilian recommendation systems are based on quantitative analyses of soil input variables. The input variable consists of the following factors; cation exchange capacity (CEC), base saturation (BS), base sum, exchangeable aluminium (Al), calcium/magnesium (Ca/Mg), potassium (K) and P levels, sodium (Na) saturation and electrical conductivity. The output variable of the system is the amount of fertilizer to be applied. This is mainly based on 4 classes, low, medium, high to very high (Palhares et al., 2001). While Brazil follows a detailed set of variables when recommending P fertilizer levels, the agronomists at Kansas University - who, among other land grant Universities in the United States, provide single rate recommendation for nutrients such as P - are developing a fertilizer recommendation system that gives growers the flexibility to choose a soil management practice suitable for their needs. This flexibility included choosing from 2 systems, the \"nutrient sufficiency recommendation system\" which is developed to provide a 90-95% maximum yield for the year, and the \"build maintenance fertility program\" based soil test values over a planned period of time, usually 4-8 years, for both immediate crop needs and build up levels to a non-limiting value (Leikam et al., 2003). In West Africa, a framework to optimize soil fertility management in rice production is in use were the yield potential is estimated by an ecophysiological model based on weather conditions, cultivar species and sowing date. This yield potential is used as an input into a static model together with field specific data such as recovery efficiency of applied N, P and K, indigenous NPK supply and maximum NPK accumulation. Outputs of the framework include, required fertilizer doses to obtain different yield targets depending on yield potential and the soil nutrient supply (Haefele et al., 2003). Sims (1992) conducted a study assessing different P tests for fertilizer recommendations used in Europe and confirmed their effectiveness. The amount of P extracted did however differ, with different extraction methods. Jordan-Meille et al. (2012) published an overview of fertilizer P recommendation systems in Europe where fertilizer recommendation systems from 18 countries were compared were data on different fertilization systems was obtained from the peer reviewed literature, personal contact and the \"grey literature\". In Europe P recommendation systems are mainly based on 3 steps. The first step includes soil testing to approximate the crop available P pool in soil. The second test involves relating results from the before mentioned soil tests to yield response (correlations between soil P tests and field trials) to account, similarly to already mentioned Brazilian system, for a 90% maximum yield per year. Based on these results, threshold values are often developed to divide soils into 3 different categories, \"low\", \"medium\", \"high\" and sometimes \"excessive\". From these categories the third step takes place, that is, the actual P recommendation is calculated. According to the review conducted by Jordan-Meille et al. (2012), the main difference between P recommendation systems in European countries was the chemical method used to extract P during the soil P test. Some use strong extractants which dissolved strongly bound P and hence does not necessarily represent the actual labile pool of P in soils and others use week extractants like water or week acids which might underestimate available soil P (Neyroud and Lischer, 2003). Moreover about half of the recommendation systems used in Europe take into account other factors such as crop characteristic (Belgium, Hungary, Sweden, Denmark, England, France, Germany and Switzerland) and soil characteristics such as soil texture, clay and organic matter content, soil pH, carbonate content and soil type (France, Italy, Switzerland and the Netherlands) (Jordan-Meille et al., 2012). In the frame of the Land Use/Land Cover Area Frame Survey (LUCAS, Eurostat, 2013a) sampling of topsoil (upper 20cm) was carried out on around 22,000 points in 25 EU Member States in 2009 (Tóth et al., 2013a) and in other 2 Member States - Bulgaria and Romania - in 2012 (Tóth et al., 2013c). Beside other basic soil properties soil nutrient (N, P, K) content of these samples were measured in a single laboratory using standard determination method (ISO, 1994) which is based on the method of Olsen et al. (1954). Results of the LUCAS topsoil survey and laboratory analysis allows an assessment of nutrient status of croplands at a European scale. As no coherent figures from EU Member States were available to date - mainly due to data accessibility problems or lack of data - the LUCAS topsoil survey provides a unique opportunity for a European overview of this issue. The LUCAS topsoil P data can help to refine and update incomplete or outdated national spatial phosphorus datasets or just provide an independent set of data for cross-comparison for countries where soil P data is available, such as the UK (Emmett et al., 2010) or France (Huyghe, 2013). The aim of our current study was to make a comparative assessment of plant-available phosphorus levels of croplands in regions of the European Union using the data from the LUCAS topsoil survey. Plant available phosphorus levels were determined using two selected fertilizer recommendation systems: one from Hungary (Antal et al., 1979) one from the United Kingdom (DEFRA, 2010). These two systems were chosen as they are developed for two contrasting agro-ecologic regions of Europe, did not include site specific criteria which were not adaptable in other parts of the EU and hence were easily applicable to a large Pan European dataset such as the hereby presented LUCAS soil dataset. Further to the determination and comparison of plant available phosphorus levels we made an attempt for a general estimation of P demand of croplands in the European Union, based on yield statistics and the data from the LUCAS topsoil survey. Materials and methods Databases used The LUCAS topsoil database Approximately 22,000 topsoil (upper 20cm) samples with unique georeferenced location were collected in 2009 from 25 European Union (EU) Member States (EU-27 except Bulgaria and Romania) and in 2012 in Bulgaria and Romania with the aim to produce the first coherent baseline topsoil database for continental scale monitoring (Tóth et al., 2013a,b,c). The soil sampling was undertaken within the frame of the Land Use/Land Cover Area Frame Survey (LUCAS), a EU wide project to monitor changes in the management and character of the land surface (Eurostat, 2013a). Based on a stratified sampling scheme samples were taken from all land cover classes, with systematically higher proportions from arable and grasslands (Tóth et al., 2013a). Soil samples have been analysed for basic soil properties such as particle size distribution, pH, organic carbon, carbonates, NPK, cation exchange capacity (CEC) and multispectral signatures. Analysis of soil parameters followed standard procedures. Tóth et al. (2013a) provided detailed description on the methodology and data of the LUCAS topsoil survey. Analysis of the P amount was carried out with spectrometric determination of phosphorus soluble in sodium hydrogen carbonate solution (ISO, 1994). Results of P measurement of samples from the LUCAS topsoil survey were used in our assessment. Fig. 1a shows the spatial representation of measured phosphorus content at the LUCAS sampling sites (Hermann, 2013). Region (NUTS) maps of the European Union For the regional analysis of P levels in the EU the maps of basic regions for the application of regional policies (NUTS2; Eurostat, 2013b) were used. The spatial dataset of the NUTS2 units was accessed from the Eurostat website. CORINE land cover data The CORINE land cover (CO-oRdination of INformation on the Environment; CLC) database (EEA, 2011) was used to delineate agricultural areas for the assessment. The CLC data of 2000 includes information on land cover in European countries, including member states of the European Union (JRC-EEA, 2005), therefore this dataset was used in the analysis. The dataset uses a classification scheme, including 44 land cover classes organized into three hierarchical levels (CEC-EEA, 1993). We focused our assessment on arable land (Corine categories 211, 212 and 213) for two reasons. First, because arable areas are the main targets of fertilizer use and we were interested in analysing P levels from the viewpoint of actual and recommended P inputs. Second, because P levels of arable lands are crucial both for food security and environmental reasons. Statistical data on crop yields Official statistics of the European Union on common wheat yields by regional (NUTS2) levels, and national crop statistics from the UK (DEFRA, 2012a) and Hungary (HCSO, 2013) were used in the analyses. Data - which is presented in Fig. 2 - were accessed through the Eurostat website (Eurostat, 2013c) and from the cereal production survey of UK (DEFRA, 2012a,b). Statistical data on fertilizer use Official fertilizer statistics from the UK (DEFRA, 2012b), Hungary (HCSO, 2013) and the FAO (FAO, 2013) were used for our comparative assessment. Methods Categorization of measured P concentrations We used the measured P concentrations to establish nutrient level categories for each LUCAS topsoil sample and to perform comparative analysis of P levels in cropland of the EU. In this study we classified the P concentration of soil samples from the LUCAS topsoil survey based on measured Olsen-P levels using equal-sized data subsets by each 20 percentiles (i) and threshold values of two different fertilizer recommendation systems (ii and iii). The most widely applied P fertilizer recommendation systems of Hungary and the UK were selected for this study to come to comparative figures on P supplies based on systems which were designed to support agricultural practices under distinct climatic conditions. The two systems were selected based on their applicability - they did not include site specific criteria which were not adaptable in other parts of the EU - and because they represent systems from different biophysical zones of Europe. P concentration threshold values related to P requirement of wheat were adapted following methodologies described by Antal et al. (1979) for Hungary and by DEFRA (2010) for the UK. Wheat was used as an indicator crop for three reasons. On the one hand wheat has wide climate tolerance and cultivated in nearly all regions of the European Union. It is also a plant with one of the largest areal share in the croplands of the EU. Furthermore wheat has medium phosphorus requirement (appr. 11kg P/ton grain yield) compared to other crops; thus can be indicative for a wide ranges of crop rotations as far as general P requirements of cropping systems are concerned. The UK system is based on Olsen-P and the Hungarian system uses AL-P. Correction function (Eq. (1)) of Sárdi et al. (2009) was applied to convert the AL-P based thresholds of the Hungarian system to Olsen-P levels: (1)y=0.5722x-1.0939 (r2=0.9672) where y is the Olsen-P level in mg/kg and x is the AL-P mg/kg. Both the UK and the Hungarian systems define five categories with regards to available P levels. The UK system numbers the categories as \\'P index\\' from 0 to 4, the Hungarian system use qualifiers - very low, low, medium, high and very high - to describe the classes. In our study we used the class qualifier names of the Hungarian system for the P index categories of the UK system as well. In addition to P measurements, the Hungarian system, considering the high pedodiversity in the country uses also soil criteria - such as soil texture, and CaCO3 - to classify soil samples into P level categories. Therefore these soil properties were considered as well from the LUCAS topsoil database to assist the categorization. The system from the UK uses only the measured Olsen-P levels in its categorization. Table 1 summarizes the main characteristics of the two systems from the P categorization point of view. Threshold values of the two fertilizer recommendation systems (Hungary, UK) were used separately to establish plant available P categories for each soil samples from the LUCAS Topsoil Survey. Each soil sample was categorized into one of the five classes according to the two methods. It is worth underlying that category thresholds of the different systems are calibrated by their authors according to the corresponding regional climatic-, soil- and management conditions, as well as related to attainable yields under these conditions. Consequently, P category thresholds differ. Spatial delineations and areal P level calculations In order to assess the distribution of phosphorus in the soils of the EU and enable estimations for P fertilizer need, two approaches were used. First, we categorized LUCAS topsoil samples from agricultural land into five equal-sized data subsets based on measured lowest and highest P concentrations. The first quintile of the LUCAS P concentration data were classified as having very low concentration, the second 20% having low, the third 20% with medium, the fourth 20% having high and the top 20% having very high P concentration. Derived categories were ordered on a nominal scale from 1 (very low) to 5 (very high) and mean P categories and standard deviation figures were calculated by NUTS2 regions of the EU. Results are presented for the point observations of LUCAS topsoil survey and also as generalized for the NUTS2 regions of the EU (Fig. 1a and b). Second, plant available P levels were calculated for all LUCAS topsoil samples taken from agricultural land using the two different fertilizer recommendation methods: one from Hungary and one from the UK. P level categories derived using the two methods were ordered on a nominal scale, than mean P categories and standard deviation figures were calculated by NUTS regions of the EU, using the two methods in parallel. Two maps displaying available P level in 10 categories - with subdivision of the five classes for better visual presentation - and variability within NUTS region were drawn for 27 EU Member States. Estimation of fertilizer P requirement of cropland of the EU P fertilizer requirement of croplands in the EU were calculated by the fertilizer recommendation systems of Hungary (Antal et al., 1979) and UK (DEFRA, 2012b). Wheat was considered as an indicator crop and fertilizer doses for wheat cultivation were calculated. Recommended P fertilizer doses were computed for each LUCAS samples and for each NUTS region taking into account the mean P supply category, the average wheat yield and the spatial extent of cropland of the regions. Mean wheat yields were obtained from time series statistics (Eurostat, 2013c; DEFRA, 2012a, HCSO, 2013). Mathematical functions provided by Antal et al. (1979) to calculate fertilizer doses were used. The UK system sets fertilizer dose targets according to three yield levels, therefore an interpolation using the forecast function based on regional yield statistics and fertilizer need was applied. Spatial extent of cropland in each NUTS region was determined using the CLC database. Following assessments using the two different systems and after analysing the spatial validity of each system, results were integrated to produce a single map of P input need for regions of the EU and an estimation of the overall P demand of arable land of the EU. In this process the UK method was applied for regions under oceanic and sub-oceanic influence and for temperate mountainous areas and the Hungarian system was applied in climatic zones under continental and Mediterranean influence. The climate zonation of Hartwich et al. (2005) was used for the delineations. Spatial analyses were performed using ArcGIS 10.0. For statistical computations the SPSS 16.0 software package was used. Results Plant available P levels in agricultural soils of the EU Based on the assessment of the LUCAS topsoil samples originating from agricultural land, considerably large differences can be observed, both among and within regions of the EU. While most European regions have soil samples which fall to the top 20% with regards to measured soil P contents (Fig. 1a), differences between distinct zones can be observed when looking at means of quintile categories of P concentrations by NUTS regions (Fig. 1b). Measured P levels displayed by means of quintile categories of individual cropland topsoil samples in each NUTS region as well as P supply levels established on the basis of the systems of the UK (DEFRA, 2010) and Hungary (Antal et al., 1979) show similar pattern throughout Europe (Figs. 1 and 3), for the latter comparison shown by a Pearson correlation of 0.965 between them (the correlation is significant at the <0.01% level). Only a slight difference can be observed by comparing the results of the two expert-based categorizations, as the UK system (Fig. 3a) defines somewhat higher P categories in Spain, Ireland and a few other regions in North-Western Europe, while the Hungarian system (Fig. 3b) grades some central European regions in higher categories than the UK system. Results based on each approach of categorization suggest that plant available P levels follow main climatic patterns in Europe. Areas of the Atlantic North Western Europe have the highest levels and the Mediterranean the lowest of phosphorus in cropland soils. According to the systems of the UK and Hungary around half of Europe\\'s croplands have high or very high levels of P supply and somewhat less than one third have low or very low levels of P (Table 2). Our calculations based on the LUCAS data show decreasing areal share of croplands with different P levels in the order of very high, high, medium, very low and low P levels, respectively (Table 2). Most countries in the EU have diverse P levels, with considerable differences among their NUTS regions. More uniform distribution of P levels can be observed in Bulgaria, Czech Republic, Denmark, Finland Ireland and the Netherlands, all having relatively high P levels in all their agricultural land. Plant available P levels in some larger countries like France, Italy and Spain but also in some smaller countries like Austria or Portugal show high inter-regional variability. In contrast to the Benelux countries, Denmark, Germany and Poland some countries like the Baltic States, Bulgaria, Hungary, Portugal and Romania have generally low levels of soil P in most of their NUTS regions. The variability of P levels within the NUTS regions was also analysed by additional descriptive statistics, which show skewed distribution of P levels in soil samples in individual NUTS regions in nearly all cases. While due to volume constraints detailed figures are not presented here, it is worth noting that only regions in the Benelux countries with usually very high or high categories and Bulgaria and Romania with low categories were those where P content of soil samples do not spread over most categories. Estimates for P input need for crop production in the EU Unlike for P supply categories the estimation of input need of different regions of the EU results quite different patterns when based on the recommendation systems from Hungary (Antal et al., 1979) and the UK (DEFRA, 2010). The system of the UK does not recommend additional fertilizer use on croplands with the highest P supply, including regions in Belgium and the Netherlands, Sardinia and two regions in the UK and rather low level inputs are recommended for the rest of the croplands of the EU, except for two Italian, three English and one German regions, where more than 50kgha-1 P input is advised (Fig. 4a). According to the Hungarian system there is a need for fertilizer P input in all regions but two (one in Finland and Sardinia) of the EU (Fig. 4b). The need is higher (100kg P2O5 per ha or more) in regions with high yields. These regions are in France, Northern Italy, the UK, Ireland, Austria and Germany. In some regions of the Netherlands and Belgium, where both yields and soil-P levels are high, the Hungarian system recommends medium-low additional P input. It is only in the case of regions with very low crop yields (<35q/ha) (e.g. in Cyprus, Estonia, Finland, Portugal, Puglia and Sicily in Italy) where the Hungarian system recommends low P inputs of less than 50kgha-1 P2O5 equivalent. Our calculations to sum the total P fertilizer input need on croplands of the EU resulted quite different figures if based on the Hungarian and the UK systems. According to the Hungarian system (Antal et al., 1979) the P2O5 input need of croplands in the European Union (EU27) was 8.2 million tons, while based on the system of the UK (DEFRA, 2010) it was 2.35 million tons. To assess the relationship between the estimates and the fertilizer use statistics we compared those in the two countries where the applied recommendation systems are developed (Table 3). We observed differences between estimated fertilizer consumption calculated on the basis of the LUCAS topsoil data and the fertilizer use according to the national (HCSO, 2013 and DEFRA, 2012b) and international statistics (FAO, 2013), in both countries. The climate zone based integration of the figures from the two systems allowed the preparation of a P fertilizer recommendation overview map of the EU (Fig. 5). Based on the underlying data of this map, the estimated annual P input need of the EU\\'s agriculture is 3.85 million tons, annually. This input might be achieved by the combination of chemical fertilizers and manure. Discussion P manure and fertilizers have been applied in excess in many European countries in the years of 1950-1980 to increase crop yield, resulting in varying accumulation of P within soil systems (Granstedt, 2000; Tunney et al., 2003) explaining high diversity across Europe. A meta-analysis study of P fertilization in 80 years of research in Finland conducted by Valkama et al. (2009) revealed that yield increases due to P fertilization were highly depended upon soil texture and organic matter and decreased in the following order, organic soils>coarse-textured soils>clay soils which was in alignment with studies of Tennberg (1935), Tennberg and Jokihaara (1935), Salonen and Tainio (1957) and Sippola (1980). Moreover, like other soil properties texture and pH show great spatial variability across the EU (Tóth et al., 2013b). As similar P supply to plants requires higher levels of measured extractable P in light soils and pH is mostly linked with the availability of carbonates in soil, which increases the required measured amount of P for adequate plant supply, P adsorption and availability are highly affected by those parameters. Therefore, as to be expected, plant available P levels as determined by the Hungarian fertilizer recommendation systems differ from the extracted P amounts of the LUCAS topsoil samples (Figs. 1b and 3b). As our earlier study (Tóth et al., 2013b) pointed out, three groups of countries can be distinguished in the EU based on the P levels of their cropland soils. These groups are (1) with generally low P levels, (2) with varying P levels and (3) with generally high P levels. Austria, Bulgaria, Cyprus, Estonia, Spain, Greece, Hungary, Italy, Lithuania, Portugal, Romania, Sweden, Slovenia and Slovakia belong to group I. Czech Republic, Germany, Denmark, Finland, France, Ireland, Poland, United Kingdom belong to group 2. Belgium and the Netherlands belong to group 3. According to our current findings, higher P surpluses where detected in north-western regions of the EU compared to other parts of the continent which is in agreement with Csathó and Radimszky (2011). This is likewise in agreement with older studies revealing a substantial accumulation of P in agricultural soils in the Netherlands, France and Germany with surpluses of 25-30kgha-1 (Smil, 2000; Tunney et al., 2003) while in Sweden, Norway and the UK the elemental P surpluses in relation to livestock farms were about 8-20kgha-1. Moreover, P soil surpluses in Central and Eastern European countries are considered lower, even in the pre-1990 accumulative period, compared to the 15 EU countries (Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, Netherlands, Portugal, Spain, Sweden and the United Kingdom) as reported by Ott and Rechberger (2012). Máthé-Gáspár et al. (2012) reported negative P balances for Hungary for the period 1989-2005 and the result of this trend is reflected in our findings, when compared to data presented by Baranyai et al. (1987). The maps displaying soil P supply based on quintiles (Fig. 1b) show that there are zones with high and low soil P levels which can be delineated in the EU and these zones are following climatic patterns. Zones with most intensive P input (Fig. 6) show both the highest P and highest yield levels (Figs. 1 and 2). However, the correlation between yields and P levels in the NUTS2 regions of the EU (Pearson coefficient=0.4; both based on the UK and Hungarian systems) suggest strong, but not exclusive P dependency of yield. This finding might suggest that in most EU regions P application doses are adjusted to targeted yields. However in the extreme cases of very high or very low P levels, this assumption might not hold. With regards to the comparison of P input (Fig. 6) and soil P level (Fig. 1) our results confirm the scientific evidence that high P fertilizer inputs with positive P balance will increase the concentrations in the soil (Cordell et al., 2009), and also provide an insight to the regional distribution of different P supply levels in the regions of the EU in relation to P input. The two recommendation systems used in this study, Antal et al. (1979) for Hungary and DEFRA (2010) for the United Kingdom, differ in their criteria to generate P recommendation. While the DEFRA classifies P level categories according to measured P levels exclusively, the Hungarian system uses additional soil criteria such as soil texture and CaCO3. Underlying soil properties that affect P mobility should be handled with care as agricultural land is not generally uniform and show high spatial variations in soil biogeochemical attributes (Bechmann et al., 2007). Recommendations on fertilizer doses for different regions of Europe differ considerably if assessed by different methods (Fig. 4). The System of the UK suggests fertilization with low P doses for most of the EU (Fig. 4a), while the Hungarian system recommends high doses on areas (Fig. 4b) where high yields are expected (Fig. 2). This difference highlights the complexity of P management decisions in a continental context. The large difference between these two figures shows the constraint of any fertilizer recommendation systems for specific agroecological conditions and highlights the limitations of our current study as well. The comparison of the results obtained from the two methods for the whole EU underlines the methodological differences between them and consequently warns about their applicability over the entire continent. While the Hungarian system seem to overestimate the fertilizer need in the western part of Europe - mainly the areas under Atlantic and sub-Atlantic climate - the UK system underestimates the P input needs in Central and Eastern Europe. One explanation for this difference can be that the UK system is designed for croplands where the nutrient dynamics from soil decomposition processes is not conditioned by long dry (or dry and cold) periods, and natural rate of P release is higher than that of fixation, thus inherent soil P can contribute more to plant requirements. The Hungarian system, on the other hand is developed for dryer and colder conditions, where natural P release is controlled by limited time available for biological activity. In any case, the strongest factor of diverging result is the validity of the systems for different climatic regions. Based on the above assumption a P fertilizer recommendation map (Fig. 5) of the EU was complied, where the UK method is applied for regions where oceanic and sub-oceanic climatic influence prevail and the Hungarian system is applied in climatic zones under continental and Mediterranean influence. Recent studies on continental P supply (Csathó et al., 2011) supports our arguments that estimated P need pattern shown in Fig. 5 is more consistent with the reality than maps produced by either of the two systems for the whole EU separately. However, as the climate borders not always coincide with administrative borders the assessment at bordering regions as well as in transitional climatic zones might not be as accurate as in the regions where the recommendation systems were developed. The adaptation of regionally specific recommendation systems can probably increase the accuracy of similar maps in the future. Based on the regionally stratified combined application of the two systems, the calculated amount of current phosphorus need of the EU (3.85 million tons) is 1.5 million tons higher than the reported 2.36 million tons mineral fertilizer usage. However, as 45% of P input in Eastern Europe and 55% in Western Europe are from manure (Eurostat, 2013a,b,c,d), the overall P balance is positive, with considerable overuse of fertilizer in certain regions of the EU. As seen from Table 3 fertilizer use in the UK as reported by the UK government was 30% lower than what the local advisory system recommends. Based on figures from Tunney et al. (2003) manure application compensates the difference between the required nutrient input and reported mineral fertilizer use. On the other hand, statistics from the FAO (2013) suggest 20% higher P fertilizer usage in the UK over the calculated needed amounts. In the meanwhile the Hungarian P fertilizer input was only about 10% of the optimal calculated by the local system on the basis of soil P levels. While organic P input from livestock of various densities can strengthen or weaken the magnitude of imbalances, the difference in soil P management in the two countries is very evident from the figures obtained from soil P test and fertilizer statistics. Fertilizer use statistics by different sources provide confusing values (Table 3). However, considering the fact that the advisory system of the UK would discourage fertilizer use in some of the most fertile regions of the EU and recommends low input to the rest of the EU as well (Fig. 4a) we might well think that fertilizer statistics do not catch the exact figures of fertilizer consumption in the EU. In any case in some countries like Romania and Bulgaria the actual values certainly fall behind the needs, while in other regions, applications are above the recommended levels. Although high levels of soil P are observed on areas with high input and high yields, like those in north-western Europe, according to the Hungarian system, high fertilizer doses on these areas are still needed to secure the required yield levels. Interestingly, the system of the UK, would not recommend additional fertilizer input on some of these areas of high P levels, e.g. in Belgium and the Netherlands, while in reality, they are constantly further fertilized (FAO, 2013). Organic manure adds considerable amounts of P in regions with high livestock densities. In fact, most countries, where high P levels are measured (Fig. 1) are countries where organic manure provides considerable P inputs (Fig. 7). Inconsistency between recommended and reported fertilizer applications (Table 3) proves differences in the farming practice in different regions of the EU, while also reflect the possible shortcomings of the fertilizer usage reporting systems. The observed differences certainly highlight the possibility to further optimize P management within the EU, as it has been already advised in regional context by a number of authors (Csathó et al., 2011; Hejcmana et al., 2012; Valkama et al., 2009). The need for better statistical data on actual yield levels and P applications is also an essential precondition for optimized P management in the EU. Knowing the different regional distribution of P supply levels in the European Union cannot only be of valuable input in assessing current soil nutrient supply in relation to food security (Lal, 2013) and in assessing the need for fertilizer input to soils in Europe (Schröder et al., 2011) but also be of great value for the study of soil P loss to the environment (Sharpley et al., 2002; Heathwaite et al., 2005). Conclusions To optimize crop production economic benefits have to be considered in relation to environmental criteria. Fertilizer doses have to be based on attainable yield and soil nutrient levels. The recent EU-wide LUCAS topsoil survey provided the opportunity to have reliable comparison of P levels in soils of the EU. Based on measured values from uniform soil tests we provided first time reliable figures of P levels in soils of the EU, also in relation to cropping needs. Based on these figures there are considerable differences within the Union; higher P levels can be observed in regions where higher crop yields can be expected (North-West Europe) due to favourable climatic conditions. On the other hand, higher P levels are measured in the regions where high fertilizer P inputs are reported and where probably the livestock densities are higher too. We made an effort to estimate the P input need of European croplands, and found disparities of calculated input need and reported fertilizer statistics both on local (country) scale and on EU level. However further studies are needed to arrive to exact figures on a continental scale. This might be achieved by the regionalization of the analysis using regional or national fertilizer recommendation systems and with the application of crop-nutrient balance models. Such an analysis is currently hindered by the non-existence or non-accessibility of recommendation system for many regions and the lack of statistics on crop yields within regions. Nevertheless, the first ever uniform topsoil P survey of the EU highlights the contradictions between soil P management of different countries of the Union while also highlights the inconsistencies between reported P fertilizer consumption and advised P doses. Our findings also underline the need to improve statistics on fertilizer use and crop yields in the EU towards finer scale information. We can assume that with the availability of more accurate information on crop yields and fertilizer input - including both mineral and organic P inputs - a coherent framework of soil P management can be worked out for the EU. Our analysis shows a status of a baseline period with data from the years 2009 and 2012, while a repeated LUCAS topsoil survey can be a useful tool to monitor future changes of nutrient levels, including P in soils of the EU.  ', \"Introduction The chirality delivery is a growing topic of interest in light of its importance in biology and advanced materials [1]. To understand the way and mechanism of chirality delivery is important for controllable synthesis of chiral material and understanding the functions of biological system, which is still a challenge to chemists. At present, self-assembly is a distinctive method to construct chiral materials from chiral or achiral constituents [2,3]. Noncovalent interactions such as H-bonding, aromatic stacking, electrostatic, etc. are vital in the self-assembly process and in determining the final geometry of the resulting metallosupramolecular structures [3,4]. As we know, nucleotide is a kind of optically active and fundamental biomolecules and can be chiral ligands for transition metal [5,6]. Although there are a few of nucleotide coordination complexes reported [7,8], the coordination chemistry and supramolecular chemistry of nucleotide-metal systems are still in infant. We have studied the chirality delivery in the nucleotide-Cu(II) complex, [CuNa(GMP)(HGMP)(H2O)7]·6(H2O)·CH3OH (GMP=guanosine 5′-mono phosphate) [7a]. The chirality delivery through non-covalent interactions has been studied based on the viewpoint of supramolecular chemistry [9]. The result indicates the diversity of supramolecular chirality. Herein, an alternative and unusual path of the chirality delivery in GMP-Co(II) complex, different from the path in the GMP-Cu(II) complex, has been investigated based on X-ray single crystal diffraction, liquid- and solid-state circular dichroism (CD) spectroscopy. The multiple and helical H-bonding along 2D layer is a distinctive way of chirality delivery in this work. The space group of the crystal structure of complex 1 is C2, which indicates that the crystal is homochiral. The molecular structure of complex 1 is built up of a Co(II) ion coordinated with one GMP ligand and five coordinated water molecules (Fig. 1, Tables S1 and S2). The chirality of GMP ligand was remained in the molecule of 1 and induced molecular chirality based on the distorted octahedral geometry of the Co(II) center. Different to nucleotide-ratio in complex 1 is 1:1, which makes the large freedom of GMP ligand especially for phosphate group. It leads to the diversity of H-bonding pattern and helical pattern in molecular packing in crystal lattice in the viewpoint of supramolecular chemistry. The molecules are linked by a strong H-bonding, O11-H11A…O5 (1.845Å, 2.657Å, 166.64°) and O9-H9A…O6 (1.945Å, 2.755Å, 168.27°) (Fig. 2(a)) to form a 1D right-handed helical chain along b axis. The molecule is twisted nearly by 180° and the screw pitch is 11.072(4) Å. The twist is mainly origin from the flexibility of sugar moiety and phosphate group and π-π packing between adjacent helical chains, which induces rotation angle to be nearly 180° and is different from the previous report [7a]. Then, these 1D helix chains are assembly by H-bonding at the presence of solvent water molecules (O13, O14) to construct the 3D chiral supramolecular architectures. One kind of the assembly mode is the neighbor helical chains linked by strong and right-handed helical H-bonding along b axis [O8-H80…O14 (1.741Å, 2.550Å, 176.33°), O14-H14B…O4 (1.969Å, 2.782Å, 168.51°), O9-H9B…O4 (2.173Å, 2.173Å, 168.27°)] (Fig. 2(b)). Interestingly, this H-bonding mode is the same with that within the 1D chain, which is twisted nearly by 180° and the screw pitch is 11.072(4)Å. That is to say, the 1D chain consisted of the half of the blue chain and the half of the green chain is the same with homocolor chain, blue or green one. Clearly, the molecules are linked each other by the right-handed helical H-bonding to extend the 2D layer structure. To the best of our knowledge, this is the first report about chirality delivery through 2D continuous helical H-bonding. Further, these 2D sheets are packed based on π-π stacking (3.3304Å) between on the ab plane combining with H-bonding [O14-H14A…O15 (1.890Å, 2.691Å, 164.53°); O14-H14B…O4 (1.969Å, 2.782Å, 168.51°)] (Fig. 2(c)). The 3D topology of complex 1 with each complex as one node and intermolecular interactions as linker is (4,6) network (Fig. 2(d)), which displays the relationship of complex molecules packing in the crystal lattice clearly. Both of the chirality of the ligand and complex 1 have been confirmed by liquid circular dichroism (CD) studies (Fig. 3(a)). The CD spectrum of the GMP ligands in aqueous solution has a strong negative band at 196nm which relates to the conformation of the sugar moiety. Positive bands centered at 219nm, of which the transition is n→π* [10]. Most of the CD active transitions are n→π* in nature [11], although the π→π* transitions dominate UV-vis spectrum (Fig. S1). As observed for purine nucleotides, α-anomers have positive and β-anomers have negative 260nm bands [5]. The absence of this band in the spectrum of GMP might be caused by the balance between α- and β-anomers through mutarotation in water solution [10]. The typical CD spectrum illustrates that the GMP ligands are the d-ribonucleotide which has a positive band near 220nm and was found to be the mirror image of the l-ribo [5]. Compared to the ligand, the positive cotton effect (CE) of complex 1 becomes weaker and narrower, which can be explained by the fact that the coordination of Co(II) to the nucleobase reduces the n→π* transition of the chromophore [12] and the number of G4 is decreased [3]. Weak negative bands centered at 250, 253 and 269nm appear, which are composed of two π→π* transition and one n→π* transition, respectively [5]. This negative CE demonstrates that the chiral of GMP delivers to GMP-Co(II) compounds and GMP mainly keeps the β-anomers in the complex. To deliberate the chirality of the supramolecular architecture, solid state circular dichroism spectra were measured in a KCl matrix for GMP ligand and the single-crystals of complex 1 (Fig. 3(b)). There is a red-shift in the solid state CD spectrum of GMP compared with its liquid CD spectrum. The negative band about the conformation of the sugar moiety has nearly disappeared. In solid-state, mutarotation is more difficult and the negative band at the 260nm nearby can be detected which indicates that the GMP ligand is mainly β-anomers. In the spectrum of complex 1, the weak negative band at 215-225nm indicates that the GMP exists as l-ribo [5], which is induced by the strong π-π and hydrogen bonding interaction. For solid samples, CD spectroscopy is highly sensitive to even a very small distortion from planarity of the aromatic chromophore [12]. The strong negative CE centered at 297nm (θ≈-10mdeg) is due to the excitation coupling of π→π* transitions of the aromatic chromophores, including the intra- and intermolecular coupling of the guanine chromophores [13], which is consistent with the single crystal structure. In this work, the experiment results confirmed that the chirality of GMP can be preserved when coordinated to Co(II) center and the chirality of this nucleotide complex molecule can be delivered to its supramolecule architecture by hydrogen bonding and π-π interaction. Liquid- and solid-state circular dichroism spectroscopy has been carried out and offers an effective ways for chirality delivery. To the best of our knowledge, it offers a new path of chirality delivery from nucleotide-metal complex molecules to its supramolecular architectures, which is significant for understanding the origin of life. In future, preparing suitable nucleotide-metal complexes for single-crystal X-ray studies and the research about the supramolecular chemistry and chirality of them remains an important assignment for us. Acknowledgements We thank the National Science Council of the People's Republic of China for supporting this research (Nos. 20771014 and 21071018) and the specialized research fund for the doctoral program of higher education, State Education Ministry of China (20091101110038). Supplementary material Supplementary materials Supplementary material CCDC 888933 contains the supplementary crystallographic data for 1. The data can be obtained free of charge via http://www.ccdc.cam.ac.uk/conts/retrieving.html, or from the Cambridge Crystallographic Data Centre, 12 Union Road, Cambridge CB2 1EZ, UK; fax: (+44) 1223-336-033; or e-mail: deposit@ccdc.cam.ac.uk. Additional structural figure, scheme, H-bond data and IR spectrum are available as electronic supplementary information in the online version, at http://dx.doi.org/10.1016/j.inoche.2013.04.038.  \", \"Introduction Climate change and ocean acidification are driven by high rates of CO2 emission to the atmosphere. One contribution to the mitigation of CO2 emissions is carbon capture and storage, CCS (Lovell, 2011). This is proposed by the G8 and the International Energy Agency as an essential technology for lowering greenhouse gas emissions. CCS reduces the emission of CO2 at a power station, or other large industrial sources such as oil and gas fields. This captured CO2 is compressed, transported by pipeline, and injected for storage into porous rock formations deep below the land or sea surface. Large-scale sites have been demonstrating engineered CO2 storage technology for nearly two decades. At the Sleipner field, CO2 is produced with natural gas and condensate, separated from the production stream, and compressed. The supercritical CO2 is then injected into the Utsira Formation, offshore Norway. This is the longest running carbon capture and storage experiment in the world, with more than 14 Mt of supercritical CO2 injected during the period 1996-2013. The injection schedule is intended to remain at around 0.9Mt/yr until around 2020, separating and storing CO2 from the West Sleipner gas-condensate field to prevent climate change. This is incentivized by the Norwegian state tax on offshore petroleum industry emissions, commencing in 1991 at NOK 210 and increasing to NOK 410 in 2013, equivalent to about $65 per tonne of CO2. The storage site, a saline formation and sandstone aquifer, is 800m below sea level (mbsl). This is considered to be a shallow depth setting for a storage environment (Chadwick et al., 2008) as the pressure and temperature conditions are likely to be close to the critical point above which CO2 becomes a more buoyant gas phase. The overlying Nordland Group, which extends from the top of the Utsira Formation to the seafloor, is predominantly shale (Fig. 1), and is expected to provide a caprock with a high threshold pressure, sufficient to seal the site and prevent leakage of the buoyant CO2 fluid over several millennia, i.e. the timescale required to offset climate change (Lindeberg and Bergmo, 2003). Saline formations worldwide are considered to be candidates for carbon sequestration because of their suitable depths, large storage volumes, and common occurrence. However, one of the critical uncertainties for saline formation storage is the ability of the caprock (primary seal), and the overlying containment geology (secondary seals), to retain buoyant CO2 without leakage. The fluid flow processes currently occurring in the Sleipner storage site provide a crucial exemplar for saline formation storage projects worldwide. Published interpretations of a series of seismic reflection surveys show that, by 1999, the Sleipner CO2 plume had ascended more than 200m vertically within the Utsira Formation from the injection point (1012mbsl) to the caprock (800mbsl). The plume has encountered and breached eight shale barriers within the storage site: seven thin shales that are approximately 1-2m thick, and an uppermost thick shale that is 6-7m thick and geologically similar to the primary seal (Gregersen and Johannessen, 2001; Zweigel et al., 2004). These shale barriers result in 10-20m thick CO2 layers that are vertically stacked and extend laterally for hundreds of meters. Biennial monitoring of the plume behavior and the resultant six seismic reflection surveys make this experiment ideally suited to numerical modeling studies (Hamborg et al., 2003; Zweigel et al., 2004; Hellevang, 2006). The areal distribution of CO2 stored within the storage site has been precisely mapped from the seismic surveys (Chadwick et al., 2006; Bickle et al., 2007). However, no published dynamic model to date has accurately replicated the layered morphology of the plume or flow behavior, and three significant aspects of the plume are poorly understood, as outlined below. Fundamental aspects of the CO2 plume layering are examined by constructing a mass-balanced flow model to test the physical containment processes through iterative numerical simulation. The simulations are calibrated to published observations. Capillary flow, not Darcy flow, is applied to obtain a good match with observations. It is essential for the calibration that the shale barriers within the Utsira Formation have high vertical permeabilities. This implies that the shales may be fractured. A novel mechanism is proposed to account for fracturing prior to CO2 injection: transient fluid overpressure during rapid deglaciation. Unexplained aspects of the plume Firstly, there is a significant uncertainty concerning mass-balance estimates for the plume. To illustrate this, consider estimates for the uppermost layer alone, circa July 2002, equivalent to 5Mt injected, bearing in mind that the uppermost layer is the best constrained portion of the plume with respect to observational data (Chadwick et al., 2008). The CO2 volume in this layer is estimated to be approximately 110-165 thousand cubic meters (Chadwick et al., 2009). Assuming an upper layer density of 426kg/m3 (after Bickle et al., 2007), this is equivalent to a mass of 0.05-0.07Mt. Although Chadwick and co-workers (2009) favor the upper end of this range, 0.07Mt is considerably less than estimates of 0.11Mt (Bickle et al., 2007) and 0.16Mt (Singh et al., 2010). This wide disparity in outcomes stems partly from (a) uncertainty concerning the plume temperature profile (Fig. 2a), with Singh and co-workers (2010) assuming a much colder plume with an uppermost layer density of 760kg/m3; and (b) poorly constrained assumptions regarding layer thickness (Fig. 2b), which are unresolvable at less than 15m thick given the seismic resolution (Arts et al., 2004). This uncertainty is further compounded by poorly constrained gas saturations for the plume layers. All three aspects are discussed further below. Secondly, despite the layer thickness uncertainty, the plume morphology indicates that column heights for trapped CO2 are unexpectedly low. The only published laboratory measurements for the caprock threshold pressure (Springer and Lindgren, 2006; Harrington et al., 2009), from a cored production well, 15/9A-11, close to the storage site, suggest a range of 1.6-1.9MPa, i.e. the fluid pressure necessary for CO2 to break through a low permeability rock. If this range is applicable to the shale barriers within the Utsira Formation, such a scenario would result in CO2 column heights of hundreds of meters (Springer and Lindgren, 2006). However, estimated column heights for the layered plume consistently fall within the range 7-14m (Chadwick et al., 2005, 2006; Bickle et al., 2007). These thin layers occur beneath all three-barrier types: (a) the 1-2m thin shales within the formation; (b) the uppermost 7m thick shale barrier; and (c) the much thicker Nordland Group shale caprock that forms the primary seal. Thirdly, the plume flow behavior is not indicative of sealing shale barriers punctuated by faults, holes or penetrated by a high permeability chimney or sand injectite (Zweigel et al., 2004), and the means of CO2 ascent is poorly understood. The plume initially ascended 200m vertically through the eight shale barriers in less than three years (Chadwick et al., 2006), resulting in a 'pancake stack' of layers. If the laterally extensive shales had acted as seals, preventing the vertical migration of CO2, the plume would have taken much longer to breakthrough, and its behavior would have been more akin to a 'zig-zag' distribution with lateral offsets, resulting from the CO2 tracking along the base of a barrier until encountering a hole through which to escape up to the next barrier, and then repeating this behavior. Mass balance estimates Addressing the mass balance question first, available areal mapping of the plume layers derived from time-lapse seismic reflection survey images (Bickle et al., 2007) are combined with layer-thickness estimates (Chadwick et al., 2009) to build a three-dimensional representation of the CO2 plume (Fig. 1). The approach assumes a flat gas-water contact for the plume layers, which implies that each layer has backfilled the trap as a gravitationally ponding fluid. The model provides an insight into the volumetric relationship between layer thickness and mass balance (Fig. 2). Recent published estimates of volume and mass balance appear to have been primarily hampered by uncertainty in layer thickness and plume density (Fig. 2). For the uppermost layer, circa 2002, thickness estimates vary from 4 to 7m (Chadwick et al., 2005; Bickle et al., 2007). The CO2 layers circa 2006 have recently been estimated as 10m thick (Lippard et al., 2008; Chadwick et al., 2009). Both these estimates are thinner than the limit of phenomena resolvable by conventional seismic reflection surveys at this depth (Arts et al., 2004; Chadwick et al., 2006). A case has been made for amplitude tuning of the Sleipner seismic to produce greater precision (Chadwick et al., 2006); however, the sensitivity of this seismic interpretation technique is such that the amplitude does not discriminate between layers more than 4m thick and less than 15m thick. Furthermore, the thin shale barriers within the aquifer are unresolvable on the baseline 1994 seismic (Arts et al., 2004) making tuning even more difficult. Published estimates of the CO2 volume in the top layer (110,000m3 from amplitude analysis, and 165,000m3 from structure) differ by 150% for the same seismic survey (Chadwick et al., 2009). Although structure estimates are considered to be more reliable than amplitude (Chadwick et al., 2008), there is no method to discriminate between them. An additional uncertainty arises from saturation profiling, which discerns between pores filled with brine and pores filled with CO2 above a poorly constrained level of saturation (Arts et al., 2004). It follows from the commonly used Gassmann equation (Gassmann, 1951) that: (a) very low saturations of less than a few percent are undetectable; (b) a strong correlation emerges as the gas saturation increases from a few percent to around thirty percent; and (c) for saturations much above thirty percent, it is difficult to distinguish between moderate and high saturations. It follows that the 80% gas saturation that is commonly assumed for the Sleipner plume, while reasonable (Chadwick et al., 2005; Bickle et al., 2007), remains uncertain (Lumley, 2008). If the 80% assumption represents a reasonable upper limit to the mean saturation for the plume, the lower limit could be as low as 40%, halving mass balance estimates premised on the widely assumed high-saturation value. Finally, the liminal pressure and temperature conditions of the plume with respect to the critical point of CO2 (31°C and 7.4MPa) compound the uncertainty. The ambient temperature at the injection depth is reasonably well constrained at 41±1°C (Bickle et al., 2007); however, the temperature at the top of the plume is much more poorly constrained at around 34±3°C (Singh et al., 2010). This results in a potential density range for CO2 in the uppermost layer of 300-700kg/m3; a mid-range scenario is assumed, with a temperature of 35°C at the caprock and a linear geothermal gradient to the injection depth at 41°C for the flow simulations that follow (orange diamonds, Fig. 2a). However, a brief consideration of different thickness and density scenarios (assuming 80% saturation) with respect to the solution space for mass balance (gray area, Fig. 2b) suggests that a cold high-density plume requires an upper layer that is at least 8-9m thick, circa July 2002 and 5Mt injected; while a hot low-density plume exceeds a mass balance with layers that are greater than 13m thick. The uppermost layer thickness necessary for a mass balance, circa July 2002 and 5Mt injected, lies in the range 8-13m. Consequently, these compounded uncertainties in mass balance (layer thickness, gas saturation and gas density) are highly sensitive to small changes in assumptions that are unresolvable by remote geophysical monitoring given the broadly constrained fields of pressure, temperature and saturation. Although an accurate mass balance estimate would be desirable to confirm site integrity with respect to the present-day caprock behavior, this aspect of Sleipner is likely to remain elusive due to the uncertainties inherent in remote geophysical monitoring of gas plumes. Modeling methodology To better understand the mass balance, spatial distribution and flow behavior of the plume, a numerical flow model of the storage site is constructed. This three-dimensional model consists of a geological framework of nine alternating intervals of sandstone and shale that extends from the base of the Utsira Formation below the injection point to the primary seal above the storage site. Published morphology maps for each CO2 layer (Bickle et al., 2007) are used as the topography for the base of each shale barrier, assuming a flat gas-water contact for the layers. This implies that the CO2 is ponding, and the layers are close to equilibrium with respect to any Darcy flow dynamics (Cavanagh, 2013). The flow of CO2 within the storage site is then simulated with an invasion percolation simulator (Permedia, 2012), assuming percolation as a function of CO2 capillary pressure and shale threshold pressure. This approach examines capillary flow in a deep saline formation that is segmented by thin laterally extensive horizontal shale layers. Scenarios are tested for resemblance to the plume morphology and then quantified for CO2 in place to arrive at the best mass balance scenario for the simulated plume with respect to the known injected volume. Conceptual model The conceptual model for the simulation is a plume of vertically stacked buoyant CO2 layers that are trapped in the sandstone intervals as thin but highly saturated layers, breaking through the shale barriers as a result of percolation. Percolation occurs when the threshold pressure of the shale barrier is exceeded by the buoyant gas capillary pressure of the trapped CO2 layer (Boettcher et al., 2002; Carruthers, 2003). The 3D model is calibrated by adjusting the breakthrough threshold pressures of the shales to match published estimates of layer thickness (column heights underlying shale barriers) and the areal distribution for trapped CO2 derived from 4D seismic. The Sleipner plume is likely to ascend as a result of gravity segregation (Singh et al., 2010), given the strong density contrast between the brine and CO2, and the high permeability of the Utsira Formation sandstones. However, the crucial distinction between our model and other approaches is our assumption concerning the dominant physics of the flow process. The buoyant separate phase behavior of CO2 in the formation not only promotes gravity segregation, but also results in strong capillary forces at the interface between the ascending gas (non-wetting fluid) and the brine (wetting fluid) in the pore throats of the geological media, which manifest as threshold pressures for the sandstones and shales. This is quite distinct from other modeling approaches that assume a Darcy flow regime, where the plume behavior is primarily a function of the interplay between viscosity, pressure gradients, and permeability. If capillary forces dominate over viscous forces, the plume will ascend and backfill beneath flow barriers, manifesting a sensitivity to the topography at the base of each shale barrier (Cavanagh, 2013). Given the known correlation in shape between the upper layers and the basal topography of the caprock and underlying thick shale, it is proposed that the flow behavior may be best characterized as a gravity-dominated percolation phenomenon, as explained below. Choice of simulator Previous attempts to model the distribution of CO2 within the Utsira Formation have used fluid flow software based on Darcy flow physics. To obtain multiple layers of CO2, these have either imposed a discrete vertical pathway to by-pass the thin shales within the Formation (Chadwick et al., 2006; Hermanrud et al., 2009, 2010) or assumed a convenient vertical juxtaposition of pre-existing holes (erosional or otherwise) for the eight intra-formational shale barriers. These simulations fail to quantitatively or qualitatively reproduce the multi-layered CO2 plume. To quote Chadwick et al. (2006) 'observed fluxes derived from the seismic data, do not match the flow simulation'. In this approach, a flow simulator based on a different flow physics is used, and a different workflow undertaken to achieve a very good match and calibrated mass balance. Permedia Migration is a multi-phase invasion percolation simulator. The simulator uses capillary threshold pressures and fluid density descriptions for oil, gas and CO2. Assuming a flow domain dominated by capillary and gravity forces, invasion percolation describes the behavior of an immiscible fluid (CO2) that is migrating, or percolating into, a porous medium (sandstones and shales). Commonly known as drainage within petroleum systems modeling, as distinct from imbibition, the buoyant phase displaces the original pore fluid (brine). Invasion percolation is commonly applied to slow moving immiscible fluids in geological flow systems. Below a critical flux velocity threshold, such flow systems tend to self-organize into discrete migration pathways and pooling traps (de Gennes et al., 2004). Percolation theory has been successfully applied to the invasion of non-wetting fluids such as oil and gas into water-wet porous geological media when studying regional hydrocarbon migration and local hydrocarbon field charge processes (Carruthers and Ringrose, 1998; Boettcher et al., 2002; Glass and Yarrington, 2003). The approach is valid at low flux rates, as the viscosity contribution to flow is negligible and the viscous-dominated Darcy flow approximation breaks down. Under capillary-dominated conditions the immiscible fluid is either migrating along a pathway at a low critical saturation or, is backfilling and flooding a trap at a higher saturation not exceeding the maximum gas saturation for the porous medium (Cavanagh and Rostron, 2013). The model presented here differs significantly from earlier conceptual models of the storage site (e.g. Hellevang, 2006; Chadwick et al., 2008; Hermanrud et al., 2009, 2010) where researchers have assumed Darcy flow along a pressure gradient via holes in the shale barriers. This study assumes that the thin shales represent laterally continuous barriers to flow. While a Darcy flow model requires a rising cascade of fluid through the sandstone and around the shales, via holes or breaks in the shale succession, the approach presented here suggests a simpler hypothesis: the vertical stacking of kilometer-wide and meter-thick CO2 layers that are the result of buoyant gas percolating through the vertical heterogeneity of sandstones and shales found in the Utsira Formation, in a similar manner to hydrocarbon migration and trap charge in petroleum systems, as appropriate for two-phase flow with a low capillary number (England et al., 1987). The capillary flow and ponding hypothesis does not require the unlikely geological coincidence of nine vertically aligned holes in the shales through which fluid cascades within the storage site. Sleipner capillary number An invasion percolation simulation assumes a dominance of capillary and gravity forces over viscous forces. The interplay of forces (viscous, capillary and gravity) in reservoir models is commonly defined using scaling-group theory (Ringrose et al., 1993; Li and Lake, 1995), with the commonly accepted limiting condition for invasion percolation being that the capillary force must exceed the viscous force by a ratio of ten thousand-to-one. In other words, the capillary number, Ca, is less than 10-4:(1)Capillary number,Ca=μqγwhere, Ca, the capillary number for a given flow regime; μ, the viscosity of the more viscous fluid in a two-phase system; q, the volumetric flux; γ, the interfacial tension between the invading phase and resident phase. Under these conditions, invasion percolation is a reasonable approximation for the flow physics. For example hydrocarbon migration typically has a capillary number of 10-10-10-14 (England et al., 1987). If the capillary number is less than 10-4, flow modeling lends itself to fast invasion percolation simulations which yield high-resolution numerical solutions (Carruthers, 2003). With respect to the Sleipner CO2 plume, the viscosity of the brine (0.7-0.8mPas) and interfacial tension between the brine and CO2 (25-27mN/m) have been estimated (Singh et al., 2010). Therefore, the limiting migration rate (Ca∼10-4) for the storage site can be calculated as 3-4mm/s. This fits well with observations, as the Sleipner CO2 plume flux is reasonably well constrained both vertically and horizontally: (a) the plume reached the caprock in 1999, approximately 3 years after injection began from the well 210m below - an observed ascent rate of about 70m/yr or 2μm/s; (b) the plume has spread laterally beneath the caprock, and is about 0.5km wide (E-W) and 3km long (N-S) after a decade - an observed lateral flux rate of less than 300m/yr or 10μm/s. It follows that the capillary number for the plume (Ca<10-7) is much lower than the necessary limit for a viscous-dominated Darcy flow simulation. The CO2 ascent and lateral spreading is very likely to be dominated by buoyancy and capillary forces, and therefore highly sensitive to contrasts in formation and shale threshold pressure (Table 1). Given these flow rate indications, it makes sense, a priori, to model the plume distribution with an invasion percolation simulator. The Young-Laplace equation describes the governing physics: a capillary pressure occurs at an interface between two immiscible fluids such as CO2 and brine. This equation, named after the Enlightenment scientists who discovered the principles of capillary action (Young, 1805; Laplace, 1806), is commonly used for multi-phase migration models. The pressure is a result of the tension of the surface that forms at the interface between the two fluids. A popular expression of the Young-Laplace equation (Eq. (2a)) relates the gravitationally stable column height of an oil or gas trap, and related capillary pressure, to the threshold pressure of the rock at which breakthrough occurs (Hobson, 1954). By exchanging the capillary pressure term with the density difference between the two fluids, the column height of the buoyant fluid can be determined (Eqs. (2b) and (2c)):(2a)Threshold pressure, Pth=2γ cos θr(2b)Capillary pressure, Pc=Δρgh(2c)Breakthrough condition, Δρgh=2γ cos θrWhere Pth is the threshold pressure of the rock at which breakthrough occurs; γ, the interfacial tension between the invading phase and resident phase; θ, the wetting angle between the invading phase and the rock (a low angle results in a high threshold pressure as the cosine value approaches unity); r, the representative pore throat radius for the rock. Pc, the capillary pressure of the invading phase at the interface; Δρ, the density difference between the two phases; g, standard gravity; h, the column height of the invading phase. If the capillary pressure does not exceed the threshold pressure, the fluid will pool beneath the shale and back-fill the shale topography until a spill-point is reached. The CO2 will then migrate laterally until trapped again. However, if the capillary pressure at the top of the CO2 pool does exceed the shale threshold pressure, the pool will breach the seal and migrate vertically until the next shale is reached and trapping occurs again, resulting in a vertical stack of pools. Hence, the conceptual model is that this pattern of pooling, breaching and lateral spill will match the observed plume distribution. Plume model The properties of the shale and sandstone intervals are constrained by the geological framework of the Utsira Formation and present-day observations from the Sleipner storage site. The Utsira Formation is internally segmented by laterally extensive horizontal shales, approximately 1-2m thick, acting as barriers (Isaksen and Tonstad, 1989). Approximately 20m beneath the caprock of the Lower Seal, the shallowest shale barrier (7m thick shale) is thought to be lithologically identical to the caprock (Gregersen and Johannessen, 2001). Reviews of the reservoir geology indicate that this 7m thick shale merges laterally with the Lower Seal, which is considered to be an effective primary seal (Arts et al., 2000; Chadwick et al., 2004). As such, above the storage site and inter-layering with it as the 7m thick shale, are the Lower Seal shales of the Nordland Group. This is overlain by the Middle Seal, bounded at the top and base by regional unconformities (Gregersen and Johannessen, 2001; Gregersen and Johannessen, 2007). The upper unconformity provides important evidence of ice sheet erosion and exhumation during the Pleistocene. The shallower Pleistocene shales are referred to as the Upper Seal (Fig. 1). The primary seal and overburden stratigraphy have considerably more complexity and heterogeneity when considered in detail (Nicoll et al., 2012), with numerous shallow sands, chimney structures and sub-glacial relict features. However, a simple first approximation is suitable for this study. Geocellular model design The 3D profile of the plume (Fig. 1) is used to build a three-dimensional model of the storage site. The geometry of each CO2 layer and the associated shale barrier topographies are derived from published images of the CO2 layering (Bickle et al., 2007; Chadwick et al., 2008). The nine layers define the cross-sectional relief of the shale barriers above the CO2. The thin shale barriers that trap the first seven layers are assumed to be 1-2m thick; the thick shale overlying layer 8 is assumed to be 7m thick. The modeled shales are assumed to be petrologically similar to the Lower Seal shale, conforming to petrophysical data from the sampled well, 15/9A-11 (Gregersen and Johannessen, 2001). The model Utsira sandstone conforms to petrophysical data, i.e. clean, well-sorted and poorly cemented (Hellevang, 2006). Numerical simulation The simulation releases CO2 at the known perforation interval of the injection well, close to the base of the model, and calculates the percolation pathway and resultant accumulations for the ascent of a discrete supercritical phase of CO2. The simulation assumes a buoyant gravity-driven flow dominated by capillary forces. The depth range of the storage site is close to the phase boundary conditions for supercritical CO2. This results in significant fluid density changes with ascent (Bickle et al., 2007). These density variations are estimated from a commonly used equation of state for CO2 (Duan and Sun, 2003) assuming hydrostatic pressure and geothermal temperature profiles for the storage site (Table 2). The output of the model is a plume distribution of CO2 with respect to the geological framework (Fig. 3) and a numerical estimate of the associated masses for each CO2 layer and threshold pressures for each shale barrier (Table 2). While the layer distributions are known and stable in the model, the multiple migration pathways and breakthrough points for vertical migration (Fig. 3) only represent equivalent simulation pathways and potential breach locations for a given barrier at its shallowest expression. These are not expected to be better than an accurate approximation of the breach locations, as ascending migration paths in the actual storage site are likely to be sensitive to local heterogeneities in the shale barriers and sandstone formation. However, multiple realisations of the simulation confirm the robust and stable outcome of stacked CO2 layers that insensitive to variations in the precise position of CO2 breakthrough. Results for two simulation scenarios Two scenarios with identical geometry were tested by varying the threshold pressure of the shales within Utsira Formation: the first, a base case, assumed that the threshold pressure of approximately 1.6-1.9MPa measured in core recovered from Lower Seal shales (Springer and Lindgren, 2006) is applicable to all shale barriers within the model. The threshold pressures are assigned to the shales as a global mean value i.e. a single shale barrier has a uniform mean threshold pressure of 1.75MPa (three-sigma standard deviation of ±0.15MPa, equivalent to approximately ±8.5%, assuming a normal distribution). This base case model failed to match the observed CO2 distribution, as the high threshold pressures for the shales resulted in a 'zig-zag' pattern of predominantly laterally spilling migration. The simulation ultimately failed to reach the caprock, as the CO2 backfilled beneath the 7m thick shale without breaching, resulting in a final column height of hundreds of meters and total saturation of the reservoir. This scenario is totally unlike the Sleipner plume seismic observations, and is rejected. The second scenario is unchanged from the base case with the exception of reduced threshold pressures for the shales within the storage site. The threshold pressure was gradually reduced by iterative experimentation until the simulation exhibited thin CO2 layers similar to those observed in the seismic monitoring. The normal distribution range for the lower threshold pressures remained as ±8.5% of the mean assigned to a given shale barrier. This second scenario provides an excellent match with the observed phenomena i.e. a vertical 'pancake stack' of layers approximately 7-11m thick (Figs. 1 and 3). The barrier geometry is prescribed by the observed geometry of the CO2 layers, and so the morphology of the simulated layers is forced, given the derivation of shale layer topography from the 4D seismic. However, the significance of the result lies in the replication of the stacked plume layering, and an estimate of CO2 layer thicknesses that results in a reasonable match to observations and a plausible mass balance (Fig. 3 and Table 2). It is notable that the model breakthrough pressure is surprisingly low in the calibrated match to the observed Sleipner plume. The simulation enables the inference of permeability and threshold pressures for the shales at the kilometer scale of the entire Sleipner storage site which are difficult or impossible to measure directly. The inferred threshold pressures required to achieve a thinly layered plume trapped by intra-formational shales are two orders of magnitude lower than the laboratory values measured on the caprock at well 15/9A-11. This is an important outcome of the model. The mode of CO2 ascent Given the model results, the mode of CO2 ascent within the storage site deserves further scrutiny. The favored scenario of thin, and stacked, CO2 layering with low intra-formational shale threshold pressures raises an obvious concern: if the assumptions hold regarding the similarity of the 7m thick shale to the Lower Seal, and the unbroken lateral continuity of the shale barriers, then the breakthrough condition for CO2 at the caprock may be as low as 50kPa (Table 2). Such an extremely low threshold pressure for shales can only occur in fractured rocks. It is therefore inferred that a network of micro-fractures span the shale barriers within the Utsira Formation, allowing the plume to ascend vertically, leaving a stack of thin CO2 layers. The significance of this finding with respect to the integrity of the caprock and Nordland Group shales is discussed below. Inferred fracture networks for shale barriers The width of the inferred micro-fractures can be characterized to a first approximation by a modified form of the Young-Laplace equation (Pankow and Cherry, 1996). In this modification for a fractured shale, the fracture half-width substitutes for pore throat radius, and the seal is breached when the capillary pressure of the CO2 layer, Pc (Eq. (2b)), exceeds the threshold pressure of a fracture, Pf, equivalent to the product of the interfacial tension, γ, and cosine of the wetting angle, θ, divided by the fracture half-width, f (Eq. (3)). The calibrated invasion percolation simulation (Fig. 3) enables the fracture width to be estimated as a function of the height of column retained beneath the shales. This indicates a fracture width of approximately 2μm (Table 2). Micron-thick fractures commonly occur in shales, and are physically observed in physical shale specimens as pale gossamer-like threads that appear on the faces of rock samples indicating cement-filled fractures, or salts drying out from fluids in open fractures. (3) Fracture threshold pressure, Pf=2γ cos θfwhere, Pf, threshold pressure of the fracture; γ, interfacial tension; θ, wetting angle; f, the half-width of the representative fracture aperture. The micro-fracture explanation for low threshold pressures raises a number of questions: what is the vertical extent of the inferred fractures? And, do these connect to form fracture networks that span the shale barriers? What are the origins of the fractures? And, is the integrity of the caprock compromised? Firstly, it is apparent that the 7m thick shale within the Utsira Formation transmits CO2. It follows that this span is the minimum limit for a connected network of percolating fractures within the storage site. It is unclear if such a percolation bridge might affect the much thicker Lower Seal. No observational evidence to date indicates that CO2 has penetrated the caprock. Given the very thin layers of CO2 (10-15m column heights with correspondingly low capillary pressures), a caprock breach would require a pre-existing fracture network. The evidence from a neighboring shale caprock, albeit older, more compacted and buried to 4km (the seal for the nearby Miller oilfield), shows that CO2 penetration of an unfractured caprock is very slow, taking approximately 70Ma to diffuse 10m into the caprock (Lu et al., 2009). However, the maximum limit of fracture connectivity at the Sleipner caprock, or through the Lower Seal remains unknown. In addition to potential fracture networks, paleo-gas chimneys and glacial tunnel valleys with the Nordland Group shales may provide physical routes for the rapid vertical escape of CO2, though these are not thought to intersect the present CO2 plume footprint or expected future distribution beneath the caprock (Zweigel et al., 2004; Nicoll et al., 2012). Concerning the origin of the micro-fractures, CO2 injection is an unlikely fracture mechanism for the breaching of thick or thin shales. A low capillary pressure of around 50kPa, associated with the buoyancy of a 10-15m CO2 column, may be high enough to allow CO2 to percolate through an existing fracture network for 1-7m thick shale barriers. However, the fracturing of shales as a consequence of such small pressure changes within the Utsira Formation is highly unlikely. By analogy, micro-fractures in overpressured hydrocarbon fields occur when the pore pressures exceed around 80% of the lithostatic pressure, typically equivalent to multi-MPa changes (Aplin et al., 2000). The Sleipner storage site is not significantly stressed or overpressured at the present day (Wei et al., 1990); the pore fluids are effectively at hydrostatic pressure; and so for Sleipner, the fluid pressure increase necessary to fracture an intact seal (about 10MPa) is highly unlikely. It is therefore inferred that the shales have been fractured prior to CO2 injection. Ice sheet unloading as a mechanism for fracturing Previous studies have put forward a number of ideas to explain the rapid vertical migration of CO2 within the Utsira Formation, as neatly summarized by Chadwick et al. (2009). The gamut ranges from small faults and natural holes in the intra-formational shale barriers, to geomechanically induced chimneys, sand injectites and changes in the wetting characteristics of shales exposed to CO2. However, these scenarios are hypothetical and frequently driven by model design, in terms of creating suitable pathways for simulating flow. The lack of sampling and direct observations from within the Utsira Formation has done little to narrow the field of possibilities. The modeling presented in this paper implies fracturing prior to CO2 injection, as outlined above. As a consequence, a plausible pre-injection fracturing mechanism is sought, given the geological history and context of the storage site. The following hypothesis further adds to the various competing ideas concerning the flow properties of the Utsira Formation, and the shale barriers in particular; and in common with those ideas, shares a lack of direct evidence from the storage site itself. However, there are a number of supporting regional indications, presented below, and the simplicity of the model, in itself, is encouraging. The Sleipner storage site location lies within the Pleistocene ice sheet domain of NW Europe (Boulton and Hagdorn, 2006). An erosional unconformity at the Pliocene/Pleistocene boundary between the Middle Seal and Lower Seal of the Nordland Group shales indicates that ice was in contact with the seabed, scouring and eroding the underlying sediments. Additional unconformities within the Middle Seal and Upper Seal indicate several similar events (Nicoll, 2012). The youngest ice sheet (Late Devensian) has left well-preserved sediments to record its position and demise; reconstructions of ice sheet thickness over the British Isles and Southern Norway during this final stage indicate that the ice was around 1000m thick and grounded over the Sleipner area of the Northern North Sea (Boulton and Hagdorn, 2006). Similar grounded ice sheets probably occurred during several glaciations over the last 1Ma from marine isotope stage MIS 12 onwards. Considering the effect of ice on fluid pressure and stress on the underlying shale, the following scenario is hypothesized: The gradual increase of ice sheet thickness during prograde glaciation produces a slow transient additional overburden load. This load is transferred to the sediments below the ice sheet if it is grounded rather than floating. The youngest grounded ice sheet (Late Devensian, MIS 2) can be precisely mapped; the MIS 2 ice sheet formed as a confluence of the British and Fennoscandian ice sheets, flowing to the north-west, and ultimately reaching the continental margin (Bradwell et al., 2008; Davison, 2012). The preservation of relict landform features indicates that, during deglaciation, the ice melted very rapidly, possibly catastrophically, within hundreds of years at 24ka, coincident with a global sea-level change (Bradwell et al., 2008). It is possible that the sea level rise transgressed beneath the grounded ice, causing it to float off the seabed; at which point the overburden pressure is removed instantaneously (Davidson, 2005; Bradwell et al., 2008). The significance of this deglaciation event is the possibility of unusually rapid and transient pore pressure changes within shales that may have exceeded the geomechanical limit, fracturing the shales (Cavanagh et al., 2006). Pressure fluctuations associated with deglaciation are distinct from more commonly encountered subsurface overpressure regimes in that both the hydrostatic and lithostatic baseline pressures are offset to higher values by loading from a grounded ice sheet (Fig. 4). Gradients remain the same but a change in the baseline pressure occurs which is dependent on ice sheet thickness. Hypothetically, the loading of the Nordland Group shales by a 1000m thick ice sheet (gray upper layer, Fig. 4) would cause both hydrostatic and lithostatic values to increase by about 10MPa during a glacial stage lasting several tens of millennia (Fig. 4). This affects a pore pressure increase within the Utsira Formation from 7MPa (700m depth) to a new equilibrium beneath the ice of 16MPa (1700m depth, including ice sheet). The new hydrostatic pressure gradient (gray lines, Fig. 4) starts near the top of the ice sheet; the new lithostatic gradient runs from the base of the ice sheet. The maximum ice load does not need to be located directly above the Utsira Formation as, for a hydraulically connected aquifer, a lateral transmission of pressure will occur for tens of kilometers. During deglaciation the ice melts rapidly, and both the lithostatic and hydrostatic equilibrium lines return geologically instantaneously to their normal positions (black lines, Fig. 4). Since pore fluids in the deep subsurface cannot flow instantaneously to re-equilibrate the rock, this rapid deglaciation leaves the pore fluids isolated at, or above the re-established normal fracture gradient. While the highly permeable Utsira Formation is free to flow and equilibrate rapidly, the much lower permeability shales are not. The shale pore pressure collides with the fracture gradient, at about 80% of lithostatic pressure, creating a zone of hydraulic fracturing potential (red triangle, Fig. 4). The thickness and strength of the overburden for any given stage is highly uncertain, hence the dashed line (fracture gradient, Fig. 4), which is conservatively estimated from present day leak-off test data (Nicoll, 2012). This mechanism suggests that, within this zone of fracturing potential, the shales of the Utsira Formation pervasively micro-fractured perpendicular to geological bedding, enabling rapid porewater escape during deglaciation. This fracturing is now responsible for the rapid migration of CO2 within the storage site. Five independent lines of evidence are considered to support this hypothesis: Firstly, reservoirs in the much deeper Sleipner hydrocarbon fields, both East and West, appear to be overpressured by about 10MPa (Wei et al., 1990; NPD, 2013). Secondly, a phenomenon known as the 'Oligocene-Miocene bump' appears to record a regional occurrence of 5-10MPa overpressure at about 1.3-1.5km depth in the Northern North Sea (Vik et al., 1998). There is no diagenetic reaction or hydrocarbon charge effect to explain such an overpressure. This suggests that both pressure phenomena are relicts from regional deglaciation events. Thirdly, seismic data for the Sleipner storage site locale display a number of discrete anomalies (pock marks, soft kicks, small chimney features) where sediment layering is disrupted, indicative of vertical gas migration in the Nordland Group shales (Zweigel, 2000; Nicoll et al., 2012). It is proposed that these shallow gas indications are relicts of natural gas leakage from the Utsira Formation during depressurization. Fourthly, observations of thirty nine deeper hydrocarbon fields across the North Sea show that thermogenic gas and condensates have leaked through caprocks to form diffuse zones hundreds of meters thick above the hydrocarbon reservoirs (Aplin et al., 2006). These seismic chimneys suggest that the leakage may be a regional manifestation of vertical micro-fractures created by the same transient depressurization phenomenon associated with ice sheet unloading. Finally, tectonophysical modeling of lithospheric flexure in response to the elastic deformation of the crust during glaciation and deglaciation (Grollimund and Zoback, 2000) indicates that the Sleipner region is currently in a state of horizontal extensional stress (5-15MPa at 2-3km depth) as a consequence of ice sheet unloading. Grollimund and Zoback suggest an explanation that is sympathetic to the hypothesis proposed here, i.e. the present-day unstressed hydrostatic pore pressure regime results from fracturing and pore fluid leakage: 'The stress decrease due to deglaciation might have brought horizontal compressional stress down to the existing pore pressure at the time when the stress decrease took place and parts of the fluids leaked away, leading to a pore pressure reduction' (Grollimund and Zoback, 2000). A recent, more detailed, analysis by Grollimund and Zoback (2003) includes the influence of ice sheet loading from 1Ma onwards, and a history of recent ice sheet melting events, with similar results. They determine that the common and widely observed indications of vertical fluid leakage from oil and gas fields within the region via faults or fractures is unlikely during glacial stages, but much more likely during regional deglaciations, with particular emphasis on events at 60ka and 15ka. It seems reasonable that these regional findings for paleo-stress, pore pressure, and fluid flow also apply to the Sleipner storage site. This combination of present-day overpressure observations, local vertical natural gas migration features and regional indications of pore pressure fluctuations associated with ice sheet-related lithospheric stress changes affirm the possibility of our inferred process for fracturing the Utsira Formation shales, both with respect to mechanism and magnitude. Discussion Successive seismic reflection surveys of the Sleipner CO2 storage site have shown by observation that the plume of injected CO2 at the Sleipner storage site has a geometry which reveals partial trapping beneath thin shales within the Utsira Formation. The flow modeling approach and analysis presented here suggest that the shale barriers have uncharacteristically low threshold pressures, enabling the rapid vertical migration of injected CO2 within the storage site. The inferred threshold of around 50kPa is much lower than the observed value of approximately 1.75MPa from core measurements by Springer and Lindgren (2006) and Harrington et al. (2009). However, it should be noted that standard methodologies for core sampling and threshold pressure measurements specifically avoid fractured samples, assuming such fractures to be indicative of drilling damage. As such, the influence of natural fracture networks on bulk shale threshold pressures is likely to be under-represented, if considered at all. The model results indicate that a definitive mass balance for the plume is difficult to ascertain given the inherent uncertainties in the data. However, a mid-range temperature profile and reasonable layer thickness estimate indicates that a mass balance for the plume is certainly possible. The modeled low shale barrier threshold pressures are potentially indicative of fracturing. However, the current pressure regime is extremely unlikely to cause fracturing, and so it is inferred that the shales may have been pervasively fractured in the past. A fracturing process consistent with the geological history of the region is that the shale barriers have undergone a transient pulse of fluid overpressure sufficient to fracture the rock. Such a pressure history could be a consequence of the rapid unloading of hundreds of meters of ice from the overburden during a deglaciation. A transient ice sheet load equivalent to 10MPa of overpressure could have been removed rapidly by melting, or instantly by a sea level rise and the flotation of an ice sheet grounded on the seabed, during a typical ablation timespan of less than a few thousand years. At the shallow burial depth of these shales, 10MPa is sufficient to exceed the tensional rock strength and cause pervasive hydraulic fracturing. This may have happened as a consequence of one or more regional ice sheet excursions into the basin. While the strength and thickness of the overburden at any given ice collapse is unknown, a conservative approximation for the fracture gradient, based on present-day leak-off tests (Nicoll, 2012), suggests that such an event is plausible. Two rival processes are acting on today's CO2 storage performance at this site. The first process, favoring retention, is the spreading and dispersal of the injected CO2. For this process, weak shale barriers within the storage formation are beneficial. These encourage the development of many thin layers of injected CO2, rather than one thick plume. This maximizes the surface area contact of CO2 to porewater within the formation while decreasing the column height acting on the caprock and minimizing the lateral spread of CO2. An increased surface area promotes the dissolution of CO2 in brine (Gilfillan et al., 2009), which results in permanent sequestration, as the slightly increased brine density causes the dissolved CO2 to descend away from the upper barrier surface (Arts et al., 2008). Solubility trapping is desirable as it eliminates the need to physically retain a lower density buoyant fluid; sequestration of CO2 by growth into new minerals is likely to be too slow in nature to effect trapping over the required timescale of hundreds-to-thousands of years (Wilkinson et al., 2011). The second process, opposing retention, is the vertical flow of CO2 which may be mediated by existing micro-fracture networks that span the shale barriers. This may be disadvantageous to CO2 storage for two reasons: firstly, because the CO2 is only partially spread laterally beneath the shale barriers, potentially resulting in a focused vertical flow path; secondly, because CO2 could potentially break through the caprock seal. Evidence from Sleipner indicates that the fracturing affects all the internal shale barriers including the uppermost 7m thick shale. This thick shale is geologically considered to be part of the overlying Lower Seal (Gregersen and Johannessen, 2001; Eidvin et al., 2013). Fracturing of the Lower Seal would be of particular concern, as the caprock acts as the primary seal for storage site. The question arises, has the caprock been fractured by the same deglaciation mechanism? And if so, do the observed chimney-like structures in the secondary seal within the vicinity of the storage site (Nicoll et al., 2012) contain vertical fracture networks that compromise the seal integrity? As yet, a percolation network of connected fractures through the much thicker shale sequences of the overburden has not been proven. Evidence against such a fracture network is the continued lateral migration of the uppermost CO2 plume at about 1m per day to the north (Chadwick et al., 2008), and the lack of seismic velocity changes observed in the caprock above the storage site, and within the Middle Seal, on successive 4D seismic surveys (StatoilHydro, 2009; Chadwick et al., 2009). This is encouraging, as even small amounts of CO2 gas added to the Lower Seal would result in a strong seismic reflection signal, given the strong density contrast between brine and gas for the pressure and temperature conditions of the overburden. These are reasonable grounds to assert that the caprock is a functioning seal that is significantly thicker than the underlying shales. If CO2 were to seep into the Lower Seal, then predictions of observable phenomena to be expected would include: (a) Stabilization or shrinkage of the upper plume layer on a timescale of years rather than decades, due to the seepage of CO2 via fractures rather than dissolution into the underlying formation. (b) Observable brightening and extending of seismic anomalies for the existing bright spots of natural gas pockets above and within the Lower Seal; 'lighting up' of existing seismic chimneys. (c) New 'soft kick' seismic anomalies appearing in the overburden as a consequence of gaseous CO2 ascending through micro-fractures and charging shallow sand bodies within the near future. (d) Ultimately, some form of surface expression at the sea floor, including pockmarking, seawater geochemical changes, bubble trains, and possibly behavioral changes in bottom-dwelling fauna. While none of these are apparent, or inferred at present (Pedersen et al., 2012), future monitoring of the Sleipner storage site could be directed toward testing such possibilities. Note that a significant increase in fluid pressure for aquifers and sand bodies above Utsira, a commonly cited detection criteria, is not expected due to the buoyant capillary-dominated nature of the flow and associated weak pressure differentials. Processes occurring in this storage site are relevant for all areas of the North Sea affected by regional ice sheets. The hydraulic fracturing of thin, shallow shale intervals during deglaciation could be a widespread feature beneath continental ice sheets. Similar processes may be expected to affect all northern hemisphere regions exposed to thick regional ice sheets in the past million years. Consequently, shales shallower than about 1.5km may be less effective primary seals than anticipated, but vertical stacking of multiple thin CO2 layers within storage formations is advantageous, as is the distribution of further CO2 layers beneath secondary seals within the storage complex, increasing the contact area with porewater and enhancing dissolution trapping. Dissolution uses more of the reservoir pore volume efficiently, and potentially increases site capacity relative to regional estimates of storage efficiency. However, a lack of understanding with respect to flow processes in the overburden will likely lead to conservative estimates of the secondary sealing potential and an underestimation of the important contribution of the overlying stratigraphy to the performance of a storage site. At the present time, in our opinion, the evidence from the very extensively analyzed geophysical data for Sleipner is compatible with retention of CO2. Conclusions The Sleipner CO2 storage site is an iconic carbon sequestration project and extraordinary fluid flow experiment in terms of the setting, nature and volume of fluid injected. This paper has documented a numerical modeling approach that results in a plausible CO2 distribution and mass balance estimate. Data acquisition and monitoring at Sleipner has been via remote geophysical sensing, primarily 4D seismic. As a consequence, a number of quite large uncertainties remain with respect to the observed plume distribution. The paper demonstrates the sensitivity of mass balance estimates associated with a poorly constrained temperature profile close to the critical point of CO2, and poorly constrained thickness estimates of the multiple thin layers that characterize the plume. With respect to simulations, it is noted that previous attempts to numerically model the plume have used software governed by Darcy flow physics, and achieved poor results. This paper presents an alternative approach that leads to a successful 3D simulation of the injected plume distribution using a model based on capillary flow and percolation physics. The paper carefully documents the modeling approach and justification for this unusual but effective method. The modeling indicates that the multiple layers of thin CO2 are likely a consequence of unexpectedly low threshold pressures for vertical migration through the shale barriers within the Utsira Formation. The estimated threshold pressures for CO2, at around 50kPa, are approximately 35-fold less than that measured on a sample of the caprock shale from well 15/9A-11, close to the site, at approximately 1.75MPa. This difference is most plausibly explained by microfracturing of the shale barriers, but CO2 injection at Sleipner is not considered to be a likely cause of fracturing. It is postulated that the fracturing occurred long before CO2 injection commenced, as a result of rapid pore pressure fluctuations associated with the collapse of thick ice sheets during multiple episodes of deglaciation in the region over the last million years. A number of independent observations are considered to support this hypothesis. Concerning seal integrity, the caprock and overlying Nordland Group shales of the Lower Seal may have also been fractured during deglaciation. However, it is noted that there is no evidence of CO2 leaking, possibly as a result of a different caprock response to CO2 retention, which is notably thicker than the thin shale barriers within the storage site. It is inferred that similar fracture networks within the primary seal are potentially only proximal to the formation, and may have limited percolation connectivity, preventing vertical migration through the overburden. Acknowledgements AJC was funded by the Scottish Funding Council. RSH was funded by the UK Energy Research Center, NERC, Scottish Power, and EPSRC (EP/F034520/1 and EP/K000446/1). We wish to thank Permedia for a license to use their software. We are also grateful to the Norwegian Petroleum Directorate archives for open-access. Much thanks to Grant Nicoll and Mark Wilkinson, University of Edinburgh, for helpful discussions. We sincerely appreciate the editorial work of Stefan Bachu, and constructive comments from the two anonymous reviewers. Surface datum is mean sea level. All measurements are in SI units unless stated otherwise. Geothermal gradient assumed linear between 41°C at 1012m (Bickle et al., 2007) and 35°C at 800m (Singh et al., 2010). Hydrostatic gradient, Pz=Pa+ρgz, the standard formulation (atmospheric pressure, 101.3kPa; gravitational constant, 9.81kg/m3). Pore water density, 1020kg/m3, assumed constant. CO2 interfacial tension, 27mN/m and CO2-pore water contact angle, 140°, assumed constant given reservoir temperature and pressure gradients, after published data (Shah et al., 2008; Bachu and Bennion, 2009). CO2 density 377-625kg/m3, from crest to injection point, calculated from Permedia PVT solver, based on a published equation of state (Huang et al., 1985). Sandstone porosity, 0.36±0.04 below the caprock. Sandstone porosity, 0.34±0.04at the base of the formation. Sandstone permeability assumed to be 1-5 darcy, after published data (Singh et al., 2010). Sandstone threshold pressure ∼10-20kPa (Sorkhabi and Tsuji, 2005). Sandstone CO2 saturation 0.80 (after a published Scw value of 0.2 (Bickle et al., 2007). Shale porosity, 0.30±0.02 assumed given depth.  \", 'Introduction It has been estimated that approximately 1 to 6 infants per 1000 are born with severe to profound congenital sensori-neural hearing loss (SNHL) (Bachmann and Arvedson, 1998; Cunningham and Cox, 2003; Kemper and Downs, 2000; Northern, 1994). Those children receive little or no benefit from hearing aids and face challenges in developing language abilities due to their inability to detect acoustic-phonetic signals, which are essential for hearing-dependent learning. Cochlear implantation (CI) is a surgical procedure that inserts an electronic device into the cochlea for direct stimulation of the auditory nerve and has been demonstrated to be effective in restoring hearing in patients suffering from SNHL. Statistical data from the National Institute on Deafness and Other Communication Disorders (NIDCD) indicate that approximately 28,400 children in the United States have received a cochlear implant as of December 2010. While many congenitally deaf CI recipients achieve a high degree of accuracy in speech perception and develop near-normal language skills, about 30% of the recipients do not derive any benefit from the CI (Niparko et al., 2010). A deeper understanding of hearing loss and better characterization of the brain regions affected by hearing loss will help reduce the high variance in CI outcomes and result in a more effective treatment of children with hearing loss. In recent years, Magnetic Resonance (MR) images have been used to study neurological disorders and brain development in children, such as reading and attention problems, traumatic brain injury, hearing impairment, perinatal stroke and other conditions (Horowitz-Kraus and Holland, 2012; Leach and Holland, 2010; Smith et al., 2011; Tillema et al., 2008; Tlustos et al., 2011). Brain MRI scans have revealed significant differences between Hearing Impaired (HI) and Normal Hearing (NH) children. Jonas et al. reviewed a total number of 162 patients\\' structural MRI scans, and detected 51 abnormalities in 49 patients. Those abnormalities included white matter changes, structural or anatomical abnormalities, neoplasms, gray matter changes, vasculitis and neuro-metabolic changes (Jonas et al., 2012). Similar studies have showed consistent results (Lapointe et al., 2006; Smith et al., 2011; Trimble et al., 2007). Furthermore, functional MRI studies have demonstrated that the activation pattern of HI is different from that of NH during certain scanning tasks (Bilecen et al., 2000; Patel et al., 2007; Propst et al., 2010; Scheffler et al., 1998; Tschopp et al., 2000). For example, Propst and colleagues studied the activation pattern of HI with narrowband noise and speech-in-noise tasks (Propst et al., 2010). In the narrowband noise task, they found that HI children had weaker activation in the auditory areas when compared to NH children. Meanwhile, NH also activated auditory association areas and attention networks, which were not detected in HI children. In the speech-in-noise task, HI children activated the secondary auditory processing areas only in the left hemisphere, rather than bilaterally as is typical of NH. Recently, we have tried to use the activation in the primary auditory cortex (A1) to predict CI outcomes. A strong correlation (linear regression coefficient, R=0.88) was detected between the improvement in post-CI hearing threshold and the amount of activation in the A1 region before CI (Patel et al., 2007). Despite these recent advances, it remains unclear whether these structural and functional abnormalities are sufficient to distinguish HI from NH individuals. In this study, we set out to investigate whether we can accurately classify HI from NH individuals based on MR images alone by utilizing machine learning techniques. We have trained three classifiers, one based on structural MR (sMRI) images, another based on functional MR (fMRI) images, and a third that integrates sMRI and fMRI images. While traditional methods utilize voxel-based morphometric (VBM) features, in which each single voxel serves as an independent feature, we extracted high-level features to characterize the 3D images. Specifically, we employed the Scale Invariant Feature Transform (SIFT) algorithm to detect and describe local features in sMRI and extracted region-level features to represent the functional contrast maps. Based upon the extracted features, SVM classifiers were trained to separate HI from NH. The SIFT algorithm was first proposed by Lowe for object recognition (Lowe, 1999). Since then, it has been widely used in the computer vision field. Basically, the SIFT algorithm detects blob-like image components and calculates a vector to describe each of these components. Each vector becomes a SIFT feature. The set of SIFT features extracted from an image contains important characteristics of this image and can be used for subsequent analysis, e.g. object recognition, gesture recognition etc. In this study, we employed the SIFT algorithm to extract SIFT features from brain structural MR images, and devised an approach for the automatic classification of NH vs. HI based on the SIFT features. There are three levels of significance for this study. First of all, we convincingly demonstrate that hearing loss can be accurately diagnosed based on MR images alone. Secondly, brain regions identified by the classifiers enable us to better understand hearing loss, and may serve as valuable indicators for the CI outcome and facilitate follow-up treatment post-CI (Jonas et al., 2012). Finally, our algorithm can be easily extended to assist in diagnosing other disorders affecting children\\'s brains, e.g., speech sound disorders of childhood, leading to a path for improving child health. The organization of this article is as follows. In Materials and methods, we describe in sequence the data sources and the preprocessing procedures, the methods of analyzing sMRI and fMRI images, the integrative model that combines these two methods, and the validation of our classifiers. In Results, we compare the classification performance of the sMRI classifier, the fMRI classifier and the combined classifier, and assess the stability of feature selection as well as the discriminatory power of features. Finally, in Discussion, we summarize the present work, highlight the significance of our approach, and discuss the limitations and envisioned future improvements. We also examine the predictive brain regions our classifiers identified and discuss their relevance in the context of hearing loss. Materials and methods Data acquisition and preprocessing Participants Thirty-nine infants and toddlers participated in a clinically indicated MRI brain study under sedation. This study was conducted with approval from the Cincinnati Children\\'s Hospital Medical Center Institutional Review Board (IRB). Eighteen of the participants had SNHL (10 females, average age=14months, range=8-24months). All hearing impaired participants were referred by the Division of Otolaryngology for MRI as part of the cochlear implant staging process and consented to participate in our adjoining fMRI protocol. They had documented bilateral severe to profound hearing loss with average hearing thresholds in the range of 90dB or greater. Nine of these subjects had no measureable hearing response in either ear at the maximum level of our audiometry equipment, at 120dB and can be considered deaf. The remaining 21 participants were normal hearing controls (15 females, average age=12months, range=8-17months). These children received clinical MRI scans with sedation for non-hearing related indications. They were recruited for the control group if they met the inclusion criteria: gestational age of at least 36weeks, normal otoacoustic emissions hearing, and normal neuroanatomy determined by the neuroradiologist. Informed consent of parent or guardian was obtained prior to the study protocol, and the parent agreed to additional hearing tests at a separate visit. The child\\'s reason for referral for brain MRI was not related to hearing. Exclusions included head circumference <5 percentile or >95 percentile, orthodontic or metallic implants that interfere with the MRI, abnormal brain pathology in the central auditory pathways. Examples of indications for scanning in this group were, \"odd body positioning-rule out chiari malformation\", \"recent onset irritable behavior-rule out brain tumor\". All participants were screened for hearing loss using otoacoustic emission (OAE) prior to the MRI scan. Failed OAE at the time of scan was also an exclusion criterion for the normal control group. All of these brain scans of both hearing impaired group and control group were reviewed by a pediatric neuroradiologist and assessed as having no anatomical findings of significance. One of the challenges of research in pediatric neuroimaging is that it is unethical to expose children to more than minimal risk for the purposes of research. This principle is dictated by our conscience as well as by the IRB at most institutions. Consequently, one of the fine points in the design of the present study is that we were required to select our control population among infants who were referred for an MRI scan with sedation because of a clinical indication. With the precautions described above and other procedures we took to insure normal auditory function and brain anatomy, this is perhaps the best control group that could be obtained for this age group in an ethical fashion. However, it is important to note that the controls were not randomly sampled from the general population. MRI/fMRI acquisition Anatomical images for this study were acquired using a 3.0Tesla Siemens Trio MRI scanner in the clinical Department of Radiology. Isotropic images of the brain were acquired using an inversion recovery prepared rapid gradient-echo 3D method (MP-RAGE) covering the entire brain at a spatial resolution of 1×1×1mm in an axial orientation. 3D MP-RAGE acquisition parameters were as follows: TI/TR/TE=1100/1900/4.1ms, FOV=25.6×20.8cm, matrix=256×208, scan time=3min and 50s. These high resolution 3D-T1 weighted images were used for co-registration of fMRI scans which were also acquired during this scheduled MRI. Functional MRI scans were performed using a silent background fMRI acquisition technique that allowed auditory stimuli to be presented during a silent gradient interval of the scan, followed by an acquisition interval that captured the peak BOLD response of relevant brain regions (Schmithorst and Holland, 2004). Using the scanner described above we acquired BOLD fMRI scans in an axial plane (4×4mm resolution), using the manufacturer\\'s standard gradient echo, EPI sequence covering the same FOV as the 3D T1 images (see paragraph above), with the following parameters: TR/TE=2000/23msec, flip angle=90°, matrix=64×64 and 25 axial slices with thickness=5mm. In the present study, all stimulus and control intervals were of equal duration (5s) in a three-phase auditory paradigm consisting of speech, silence, and narrow band noise tones interleaved with acquisition periods of 6s during which 3 image volumes were obtained covering the whole brain. A timing diagram for the fMRI data acquisition and stimulation paradigm is shown in Fig. 1. The speech stimulus consisted of sentences read in a female voice. Altogether 36 sentences were read in 18 segments of 5s duration and comprising 2 sentences each. This condition was followed by a 6s data acquisition and then a 5s interval of silence as a control condition. After another 6s control interval acquisition, a second auditory control condition was played. This condition consisted of Narrow Band Noise (NBN) tones patterned after standard audiology evaluations for detection of hearing thresholds. Five NBN tones of 1s duration with center frequencies of 250, 500, 1000, 2000 and 4000Hz and bandwidth of 50% were played in random order during this control condition, for a total of 5s during a silent interval of the scanner. An additional interval of 1s of silence followed each acquisition to provide an acoustic demarcation prior to the stimulus onset of each stimulus condition. This resulted in the fMRI acquisition time of approximately 11min. See Fig. 1 for a detailed schematic of the task and timing. Auditory stimuli were administered through calibrated MR compatible headphones at a sound level of 10-15dB greater than the individual participant\\'s Pure Tone Average (PTA) hearing threshold. Each hearing impaired participant in the study had a recent audiogram, which was used to determine the sound level for fMRI. Our MR compatible audio system was modified to allow for an output through the headphones measuring up to 130dB. Data analysis - preprocessing fMRI data were initially analyzed on a voxel-by-voxel basis to identify the activated brain regions using a standard pre-processing pipeline implemented in the Cincinnati Children\\'s Hospital Image Processing Software (CCHIPS) (Schmithorst et al., 2010) written in IDL computer language. In this paper, we use voxel for 3-dimensional images and pixel for 2-dimensional images. Since the subjects were sedated, we assumed that the anatomical image was naturally aligned with the functional images for each individual. Therefore, alignments between anatomical images and functional images were not needed in preprocessing. In this case, it does not matter if we apply the normalization transformation before or after contrast determination. To generate both normalized contrast maps used in the current study as well as contrast maps in native space for other uses, we first generated contrast maps in each individual\\'s native space and then normalized the contrast maps to standard space. The raw EPI images were simultaneously corrected for Nyquist ghosting and geometrical distortion (due to B0 field inhomogeneity) (Schmithorst et al., 2001). EPI functional MR time-series images were corrected on a voxel-by-voxel basis for drift using a quadratic baseline correction. Motion artifacts were corrected using a pyramid iterative co-registration algorithm (Thevenaz et al., 1998). During this stage, infant brain images were transformed to the AC-PC plane. Finally, the individual image volumes (1,2,3) in the event-related fMRI acquisition were separated and submitted to a final pre-processing step using the General Linear Model (Worsley et al., 2002) to construct individual Z-maps for each volume and contrast condition (speech vs. silence, speech vs. tones and tones vs. silence). Z-maps showing activation for each condition for each participant were then computed by averaging the Z-maps from the individual volumes for each contrast condition (Patel et al., 2007; Schmithorst and Holland, 2004). These Z-maps, in each individual\\'s native space were used by the radiologists and neurotologists for clinical interpretation of findings. The neuroradiologist reviewed both functional and anatomical MRI scans for each participant and completed a standardized report indicating whether brain abnormalities or brain activities were detected in primary auditory areas, language areas or other brain regions. After that, we performed spatial normalization using SPM8 with a T1 template constructed from a control group of age matched subjects selected specifically for this infant cohort (Altaye et al., 2008). The normalized anatomical images and functional Z-maps were then submitted to the next stages of analysis. Feature extraction and model learning based on structural MR images For sMRI images, we used SIFT features to represent the brain images and developed an algorithm to analyze the SIFT features. We have previously applied this method to Alzheimer\\'s disease, Parkinson\\'s disease and bipolar disease, and it has demonstrated promising classification performance (Chen et al., 2013). Obtaining 2D slices from 3D brain images Due to the high density of SIFT features in the brain images and the pair-wise comparison among SIFT features required in a later step, analyzing the 3D brain image as a whole is computationally infeasible. Thus, the spatially normalized 3D brain (157×189×136) was divided into 560 20×20×20 cubes. Since the dimensions of brain image were not divisible by 20, the cubes at the end of dimensions only contained the remaining volume of the brain image and therefore had a size smaller than 20×20×20. The number 20 was determined based on our experience from the application of this algorithm to several other diseases. The cube size mainly affects the computation speed and accuracy of the likelihood scores as described in the Feature evaluation section below. A larger size leads to a much longer computation time, while a smaller size decreases the accuracy of likelihood scores and subsequently leads to lower classification accuracy. According to our experimental results, the cube size 20×20×20 provides a good balance between speed and accuracy. Every cube was sliced along three different orientations to obtain 3 sets of 20 2D brain images. We analyzed every cube and every set of 2D brain images individually. The analysis results were combined together in the last step. Extracting SIFT features The SIFT algorithm for analyzing 2D images was implemented in several stable software packages (Lowe; Vedaldi and Fulkerson, 2010). In this study, we used the SIFT algorithm provided in a publicly available computer vision software package vlFeat (Vedaldi and Fulkerson, 2010). The SIFT features are described by center locations, scales, orientations and appearance matrices. An example of SIFT features is shown in Fig. 2. The SIFT features are shown as circles in Fig. 2(a). Each circle represents a SIFT feature. The center and radius of the circle represent the center location and the scale of the SIFT feature. The existence of a SIFT feature suggests that there is a blob-like image component at the center location of the SIFT feature and the scale of the feature represents the radius of the blob-like component. The image intensity distribution around the blob-like component is further characterized by an orientation and an appearance matrix. The orientation, as shown by the line starting from the center of the circle, represents the general direction of change in image intensity. The appearance matrix represents the detailed change in image intensity. An example of an appearance matrix is shown in Fig. 2(b). The square centered at the center location of a SIFT feature is divided into 16 subsquares. There are 8 lines starting from the center of each subsquare along 8 different directions. The length of a line represents the number of pixels which have a gradient direction the same as the line, and some of the lines may have a length of zero. For example, many of the pixels in the lower left corner subsquare, as shown in Fig. 2(b), have a gradient direction pointing to the lower side of the image; therefore the length of the line starting from the center of this subsquare and pointing to the lower side is long. The center location, scale, direction and appearance matrix of a SIFT feature can be organized as a vector of 133 numbers: the center location includes 3 numbers representing its coordinates in the 3D volume of the brain image; the scale and orientation is represented as one number respectively; the appearance matrix is represented by 128 numbers, 8 numbers for each of the 16 subsquares. This vector form is used in the computation; while the isomorphic graph representation, as shown in Fig. 2, is used as a user friendly way of representing the SIFT features. Feature evaluation The extracted SIFT features were identified as one of the three feature types, namely patient feature, healthy feature and noise feature. The features were evaluated based on their frequencies of occurrence in patient brains and healthy brains. There were two steps to evaluate the features, and each SIFT feature was evaluated separately. The first step was to find all the other features that were similar to the feature that was being analyzed. The similarity between two features was measured by four criteria: the distance between the center locations Δx(i, j), the scale difference Δσ(i, j), the orientation difference Δo(i, j) and the difference between their appearance matrix Δa(i, j). They were defined as follows: (1)Δxij=xi-xj2σi (2)Δσij=lnσjσi (3)Δoij=minoi-oj,2π-oi-oj (4)Δaij=ai-aj2 where xi was the center location of feature i, σi was the scale of feature i, oi was the orientation angle of feature i and ai was the appearance matrix of feature i. If all the four differences were less than their corresponding threshold, two features were considered to be similar. All the features that were similar to feature i constituted the similar feature set for feature i: (5)Si=fj:Δxij<ϵx∧Δσij<ϵσ∧Δoij<ϵo∧Δaij<ϵa where ϵx, ϵσ, ϵo and ϵa were similarity thresholds for center locations, scales, orientations and appearance matrix, respectively. According to Toews et al. (2010), the thresholds ϵx and ϵσ were set to 0.5 and 2/3 respectively. The thresholds ϵo and ϵa were set to π/2 and 0.45 respectively based on a grid search (Chang and Lin, 2011). Grid search is an efficient way to find the best parameter combinations, when there are multiple parameters in a model and the parameters are continuous variables. First, we discretized the continuous parameters. Parameter ϵo was discretized into three discrete values [π/4, 2π/4, 3π/4], and parameter ϵa was discretized into five discrete values [0.3, 0.35, 0.4, 0.45, 0.5]. Then all the combinations of these discrete values, 15 combinations in total, were tried and the parameter combination with the highest classification accuracy was chosen as the best parameter setting. The second step for feature evaluation was to assign likelihood scores to the SIFT features. The likelihood score was defined as follows: (6)Li=lnSi∩P/NPSi∩C/NC,Si≥NP+NC0 otherwise where Si was the similar feature set for SIFT feature i, P was the patient feature set which included all the SIFT features extracted from all patient brains in the training set, C was the healthy feature set including all the SIFT features from all healthy brains in the training set, NP and NC was the number of patient brains and the number of healthy control brains in the training set, respectively. A SIFT feature was identified as a patient feature if Li was larger than a threshold ϵl; it was a healthy feature if Li was smaller than -ϵl; it was a noise feature otherwise. Formally, the class labels of the features were determined as follows: (7)Ci=1,Li>ϵl0,Li≤ϵl-1,Li<-ϵl where ϵl was the threshold for likelihood scores. We used grid search to determine the best parameter setting. For the threshold, the value from 0.1 to 1.2 with a step size of 0.1 was searched. After the grid search, ϵl was set to be 0.9. According to the above feature evaluation process, we need to find the similar feature set for every feature (Eq. (5)), which requires comparing this feature with all other features. For more than 105 features in 39 brains, it would require 1010 pair-wise distance calculations, which is a very slow process. Upon those observations, we divided the whole brain volume into small cubes. For the evaluation of a feature, we only calculated its distance to the other features in the same cube. In this way, the computation time is significantly reduced, but the classification accuracy may be adversely affected. For example, a feature close to cube boundaries may have some of its similar features (Eq. (5)) in adjacent cubes. Ignoring those similar features in adjacent cubes could lead to an inaccurate likelihood score (Eq. (6)) for this feature. This issue is especially serious when the number of training samples is limited as in our project. On the other hand, a larger cube size would have fewer features close to cube boundaries, and would result in more accurate likelihood scores and hence higher classification accuracy. According to our previous experience from the application of this algorithm to the classification of several other diseases, such as Parkinson\\'s disease, Alzheimer\\'s disease and bipolar disorder, 20×20×20 was considered to be an appropriate cube size. This cube size 20×20×20, determined based on adult-sized brains in our previous studies, was used directly for the infant brains in the present study, since our infant brains were normalized using the infant template and the infant template was enlarged to the size very close to that of adult brains (Altaye et al., 2008). Training SVM classifiers We trained a linear SVM for every set of 2D slices in every cube to classify the set of SIFT features extracted from this set of 2D slices across subjects into 3 categories. For a new SIFT feature from a brain image whose class-label is unknown, the corresponding SVM is expected to be able to predict the class label of this new SIFT feature without finding its similar feature set in the huge amount of SIFT features extracted from the brain images used for training. Predicting new subjects To predict a new subject to be NH or HI, the subject\\'s sMRI scan was first normalized to the standard space using SPM8 with the infant T1 template (Altaye et al., 2008). The normalized brain was divided into cubes and sliced along three orientations as described above. SIFT features were extracted and then classified using the SVM that was trained for the same cube and same slice orientation. After all the SIFT features were classified, we counted the number of features of the three types. The total number of noise features was not used in the final decision process. The new subject was classified according to the following equation: (8)Classlabel=HI,ifCsum>ϵsNH, otherwise where Csum=∑iC^i, C^i is the predicted class label of the i-th SIFT feature as shown in Eq. (7), ϵs is a threshold for the final classification of sMRI and its value is determined based on the method described in section Validation of the classifier. Feature extraction and model learning based on functional MR images For fMRI images, we constructed contrast maps using the General Linear Model (GLM) (Worsley et al., 2002) as described in the Data acquisition and preprocessing section. Contrast values were estimated from the difference in image intensity for each voxel between two conditions. A positive contrast value indicated that brain activation was higher in the first condition when compared to the second condition, while a negative contrast value suggested a lower activation in the first condition. We generated region-level features and proposed a novel approach to vectorize the contrast maps utilizing the \"bag-of-words\" strategy (Sivic and Zisserman, 2009). Feature generation from contrast maps Normalized Z-maps were thresholded to select voxels with extreme contrast values for subsequent analysis. Among the selected voxels, we connected the voxels which were adjacent to each other in a 3D neighborhood, in which each voxel had 26 neighbors if it was not on the border. As a result, the selected voxels were merged into a set of disjoint regions, each of which was defined as a region of interest (ROI) (Dykstra, 1994; Pokrajac et al., 2005). To prevent mixing positive voxels and negative voxels in a single ROI, which could negate the signal, we considered these two categories of voxels separately. Positive voxels were ranked decreasingly whereas negative voxels were ranked increasingly according to their activation magnitudes. Only the top 5% of each category were selected. The cutoff of 5% was chosen because it outperformed other cutoffs, 1% and 10%, with respect to the classification performance. In this way, a number of ROIs were delineated to characterize the pattern of a contrast map. Due to individual differences and random noise, however, the set of ROIs delineated from different subjects varied significantly. To address this problem, we delineated a set of ROIs based on each subject, and applied all ROIs derived from all subjects to each single subject to form a long vector for each subject, with each dimension representing the mean contrast value over all voxels within the corresponding ROI. Finally, we concatenated the vectors from the three contrast maps, and obtained a 1474-dimension vector for each subject. In other words, each significantly activated/deactivated region was treated as a word, and all words occurring across all subjects constituted the dictionary. The frequency of each word was measured by the mean contrast value. An intuitive view of the contrast map vectorization process is shown in Fig. 3. Since we performed ROI detection on each contrast map and then concatenated all the ROIs together, ROIs that were consistent among subjects were detected more than once. To merge those similar ROIs into one single feature, we performed a hierarchical clustering with average linkage (Johnson, 1967). The original space was represented as:(9)S1, 1⋯S1, 1474⋮⋱⋮SN1⋯SN1474where each row represents a training sample and each column represents a ROI, S(i,j) is the mean contrast value of ROI j for subject i, N is the total number of subjects. The distance between two ROIs was calculated as the Euclidean distance:(10)distROIiROIj=∑k=1NSki-Skj2 We cut the hierarchical tree with the inconsistency coefficient of 0.01, and calculated the mean value of the ROIs that were clustered together as the value of the joint feature. The cutoff of 0.01 was easily determined since the cluster results did not change in the cutoff range from 0.01 to 0.7. After hierarchical clustering, the dimensionality was reduced to 969. Sedation method Subjects were sedated with three different sedation methods during the MRI scanning. Different sedation methods were expected to affect the activation pattern differently (DiFrancesco et al., in press). Therefore, we added sedation method as an additional feature, which was represented as a 3D binary vector (11)100010001. As shown in the matrix defined in Eq. (11), each row of the matrix represented one of the three sedation methods. In this way, we represented each subject as a 972-dimension feature vector, including 969 features from the contrast maps after hierarchical clustering and 3 binary features from sedation method. Therefore, our dataset was represented as D defined in Eq. (12): (12)D=x1y1,⋯,xiyi⋯,x39y39|xi∈R972 where x(i) and y(i) was the feature vector and group label (NH or HI) for the i-th subject, respectively. This dataset D was used for subsequent feature selection and model learning. Feature selection and model learning The WEKA software package was utilized to select a subset of features that were highly correlated with class labels and uncorrelated with each other (Hall, 1999). The merit of a subset of features was measured as: (13)MS=krcf¯k+kk-1rff¯ where rcf¯ was the mean correlation between class label and selected features, rff¯ was the mean correlation between two features, k was the number of features in subset S. Greedy hill-climbing augmented with a backtracking facility was applied to search through the space of feature subsets (Dechter and Pearl, 1985). For explanation purposes, we can imagine that there was a rooted tree, which had included all possible feature subsets. In this tree, each node was a feature subset, which was represented as a 972 dimensional binary vector, with 1(0) indicating that the corresponding feature was (not) selected. Each node had 972 successors/children, each of which was generated by flipping one of the 972 dimensions of the current node. Our goal was to step through this tree to find a node with relatively high Ms. In practice, the whole tree would not be constructed because it was unlimited. Only the successors were generated whenever needed. The search started from the root, which was the empty set of features in our project, and repeatedly chose the successor with the highest Ms at each node. The search terminated when 5 consecutive non-improving steps occurred. With the selected subset of features, we trained a linear SVM classifier (Chang and Lin, 2011). Predicting new subjects Given a new subject, we first normalized the contrast maps to the infant template space (Altaye et al., 2008), so that the given contrast maps were registered with the training contrast maps. A 972-D feature vector was then constructed with procedures described above, which was subsequently filtered based on the feature selection results obtained from the training set. Finally, the formatted feature vector was fed to the trained classifier, yielding a decision score (fMRI_score) for the new subject based on the functional MRI data alone. The rule for classification was formulated as: (14)Classlabel=HI,iffMRI_score≥ϵfNH, otherwise. Important features The importance of a feature was measured as follows: (15)If=∑i=1Nσiwif where || was the absolute value function, N was the total number of folds of cross-validation as described in the following part, wif was the SVM weight for feature f during i-th fold of cross-validation, σi=1 if the feature f was selected in the i-th fold of cross-validation. Otherwise, σi=0. For the ROIs that were merged into a joint feature through the hierarchical clustering, the importance of such an ROI was equal to the importance of the feature, to which this ROI belonged. Integrated model To combine the sMRI and fMRI data, we designed a two-layer classification model (Fig. 4). Given a training set, we trained two classifiers, namely sMRI classifier and fMRI classifier. Then we applied these two classifiers to the training set. As a result, we obtained two predicted scores for each training sample. Thus, the original feature space was transformed into a new two-dimensional feature space through these two classifiers. Finally, we trained a linear SVM classifier (with parameter C=1) in the new feature space to combine the two scores together. When predicting new subjects, we first obtained the two predicted scores from the sMRI classifier and fMRI classifier, then fed these two predicted scores into the second layer classifier to yield the final decision score y. The decision rule was defined as follows: (16)y=fCsum,fMRI_score=w1∗Csum+w2∗fMRI_score+bias (17)Classlabel=HI,ify≥ϵiNH, otherwise where w1, w2 and bias were the parameters in the SVM model, which were learnt from the training. Validation of the classifier Leave-one-out cross-validation (LOOCV) was employed to validate the three classifiers as follows. The total number of subjects was denoted as N. We performed N experiments, each of which was called one fold of cross-validation. In the n-th (n=1,…,N) fold of cross-validation, the n-th subject was used for testing; while the others were used for training. Threshold ϵs was determined so that the false positive rate and false negative rate for the training brains were equal, while ϵf and ϵi were set to be 0. These thresholds were applied to the test images to assign them to be either NH or HI. The classification accuracy for all the N subjects was reported as accuracy. Equal error rate (EER) accuracies were also determined based purely on the predicted scores of the testing brain images, e.g. the threshold ϵs/ϵf/ϵi were chosen so that the false positive rate was equal to false negative rate for the testing brains. In addition, area under curve (AUC) was also calculated to evaluate the performance of classifiers. Results Classifier performance Performances of the three classifiers are shown in Table 1, and receiver operating curves (ROCs) are plotted in Fig. 5. While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance. The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively. From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%. However, the ROC for fMRI was in an opposite situation. The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly. As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations. To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in Figs. 6 and S1. Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data. Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in Figs. 6 and S1. However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures. Performances of the three classifiers are shown in Table 1, and receiver operating curves (ROCs) are plotted in Fig. 5. While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance. The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively. From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%. However, the ROC for fMRI was in an opposite situation. The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly. As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations. To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in Fig. 6 and S1. Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data. Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in Fig. 6 and S1. However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures. Feature selection in sMRI analysis In the analysis of sMRI data, image features were selected based on their likelihood scores. The total number of image features in a brain image ranged from 35,000 to 52,000. Most of these image features were noise features. The total number of selected features, i.e., healthy and patient features, ranged from 300 to 1400 for different brains with a likelihood threshold of 0.9. Different choices of likelihood threshold for the sMRI feature selection resulted in different numbers of selected features and therefore different classification accuracies. Table 2 shows the relation between classification accuracy and the likelihood threshold. The classification accuracy did not change for likelihood threshold ranging from 0.7 to 1.1. The AUC changed within a range of 0.09 with a peak where the likelihood threshold equaled 0.9. The EER accuracy varied within a range of 0.08. All three classification performance measures were stable with different likelihood thresholds. Stability of feature selection in fMRI analysis We have analyzed the stability of feature selection in the analysis of fMRI data. There were in total 972 features as the input for feature selection. Only 6.2% of the features (with a total number of 60) were selected at least once. For each fold of cross-validation, there were usually about 20 features selected for the training, generally 30% of which were consistently present in all folds of cross-validation. We calculated a stability index as follows (Kalousis et al., 2007): (18)Simsisj=si∩sjsi∪sj (19)index=2cc-1∑i=1c-1∑j=i+1cSimsisj where c was the total number of rounds of feature selection, si and sj were two sets of features selected during two runs, |si∩sj| was the cardinality of the intersection between si and sj, and |si∪sj| was the cardinality of the union of si and sj. Our feature selection yielded a stability index of 66.2%, which indicated that 66.2% of the selected features, on average, were common between any two runs of feature selection. Since the Euclidean distance was used in the hierarchical clustering, only very similar ROIs were merged. There was still considerable redundancy among features. For example, two ROIs, e.g. one from the contrast speech vs. silence and the other from the contrast tones vs. silence, were significantly correlated with class labels, and meanwhile they were also highly correlated with each other. Due to the large Euclidean distance between them, however, they were not merged during the hierarchical clustering. In feature selection, these two ROIs were treated as different features and selected interchangeably. This caused the calculated stability index to be lower than the actual value. In this regard, 66.2% represented very high stability. Discriminative brain regions For sMRI, we measured the importance of a SIFT feature with its likelihood score. In our project, however, the SIFT features usually had a scale of 10mm or even larger, and correspondingly the side length of the appearance matrices was larger than 40mm. Due to the large size of the SIFT features, it was more difficult and less useful to interpret the medical implications of such large brain regions. With those considerations, we only focused on the highly predictive brain regions identified by the fMRI classifier. Fig. 7 shows the top 10 functional features extracted from fMRI data that differentiate the HI and NH groups. Features are numbered from A to J in order. ROI A1 and A2 were merged during hierarchical clustering into a joint feature A. Similar procedures were performed for features C, E, F, I and J. We can see that ROIs grouped together during hierarchical clustering are always from the same type of contrast maps (Table 3) and encompass adjoining or sometimes overlapping brain regions as designated by Brodmann\\'s Areas in the 4th column of Table 3. Discussion In this work, we have built a robust two-layer classifier that can accurately separate HI from NH infants. We realize that hearing in newborns can be accurately tested using the auditory brainstem response (ABR) evaluations or the otoacoustic emission (OAE) measures, it is thus not our intention to develop a tool for computer-aided diagnosis of hearing loss. Rather we provide a proof of principle that it is possible to accurately determine the functional, developmental status of the central auditory system in congenitally hearing impaired children based on MR images alone by utilizing machine learning techniques. Such success has been previously reported in other progressive diseases, such as Alzheimer\\'s disease (Cuingnet et al., 2011). However, for many progressive diseases, definite diagnosis is often difficult to establish, in which case the LOOCV approach may not be able to estimate the classifier performance accurately. Therefore, our dataset with solid labels corresponding to diagnostic categories of the participants that have NH or HI enables us to make an objective evaluation of our algorithm, and demonstrate conclusively the feasibility of using machine learning in making automated diagnoses or prognoses based on imaging examinations. The approach described here may not be limited to a specific disease; essentially, any disease dataset with sMRI and fMRI brain images can be analyzed with our method provided that sufficient training data is available. A major innovation that makes highly accurate predictions possible in our approach is that we extracted high-level features instead of using each single voxel as a feature as in traditional approaches. The SIFT features from sMRI images and region-level features from fMRI images are much less sensitive to registration errors when compared to voxel-features. In addition, utilization of high-level features can considerably reduce the dimensionality of feature space, which not only makes our classification problem easier to handle, but also helps to reduce the problem of over-fitting. At last, our classification model is more interpretable, because our model involves fewer features consisting of continuous regions instead of scattered voxels. These features can then be related more easily to disease etiology, diagnosis and prognosis. Another innovation of our approach is that we employed a bag-of-words strategy to analyze the functional contrast maps. This technique can characterize the activation pattern for every individual in spite of the great variability in the activation pattern among individuals. Considering the relatively small sample size, we constructed our feature pool with all available samples, including the one used for testing during the cross-validation. We implemented a variant version of our algorithm, in which we extracted ROIs based only on the training samples, and subsequently applied those ROIs to the testing sample directly. As expected, the variant algorithm performed slightly worse (AUC=0.81) than our original algorithm (AUC=0.83). Adding the ROIs from new samples requires us to retrain the classifier every time when new samples are available. As the feature pool becomes larger in the future, the retraining is not necessary. Integration of different types of data, e.g. data from multiple modalities, has been demonstrated to be more powerful for classification (Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012). However, how to implement such integrations in the best way remains to be explored (Orru et al., 2012). Traditionally, features from different types of data are concatenated and a single classifier is trained (Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012). Specifically, the traditional integration method requires the training set to be organized into matrices, with each row representing a training sample and each column representing a feature. One matrix is constructed for one type of data, and subsequently all the matrices are concatenated into one big matrix, which serves as the input for classifier training. In our project, the fMRI data can be easily organized in this way. For sMRI, however, each training sample has a set of SIFT features, which can be treated as a set of words included in an article. Different articles have different sets of words. Thus, it is not easy to organize the sMRI data into a matrix as described above, and the traditional integration method is not applicable. Under such circumstances, we proposed a two-layer model to integrate the sMRI and fMRI data. Since the traditional approach was not applicable in our project, we did not compare their performances in the present paper. Additionally, our two-layer model is also applicable when features from different modalities can be concatenated. In this case, one classifier is trained for one modality, and a second-layer classifier is subsequently used to integrate the multiple classifiers on the first-layer. This approach is able to combine as many types of data as possible, without worrying about the high dimensionality or overfitting. Although computer-aided diagnosis of hearing loss is not needed, our algorithm can potentially advance the study of congenital hearing loss mechanism by identifying discriminative brain regions as disease biomarkers for hearing impairment at various levels in the auditory system. Inspecting the most important features that differentiate children born with hearing impairment from children with normal hearing in this study, we see some features that are in line with hypotheses about under stimulation of auditory function in HI infants; while other observations already begin to add to our knowledge of how congenital deafness affects brain development and function. For example, features B, F, H, and I include known components of the auditory language network which our group and others have previously shown to be engaged by the narrative comprehension task (Karunanayaka et al., 2007; Schmithorst et al., 2006). These features include (B) the planum temporale and primary auditory cortex in the left hemisphere (including Wernicke\\'s area, the classical language recognition module), as well as the angular gyrus and supramarginal gyrus at the temporal parietal junction of the (F) left and (H, I) right hemispheres, known auditory and visual language association regions. Although all participants were bilaterally severely to profoundly hearing impaired, we observe left dominant auditory/language related activity present in components A, B, and F. In addition, components H and I contain right hemisphere auditory/language activity. Functional features such as these are not unexpected in terms of regions of differential cortical activation between HI and NH children listening to natural language as an auditory stimulus and it is reassuring to see these regions highlighted by our algorithm as potential biomarkers corresponding to hearing impairment. Similarly, there is evidence of differential activation in subcortical features corresponding to the auditory brainstem pathways. Features A, D, and J include elements of the reticular auditory pathway of the brainstem which has been identified by electrophysiological studies to have a key role in auditory perception of location of sounds as well as the ability to filter a source of sound in background noise. Roughly these features appear to encompass key elements of the auditory pathway at the level of the pons (D) including the cochlear nucleus, trapezoid body, lateral lemniscus and superior olive on the right, (A) inferior colliculus, medial geniculate on the left and (J) thalamus bilaterally (Kretschmann and Weinrich, 1998). Although the resolution of the fMRI scans (4×4×5mm) is not sufficient to resolve these structures individually, differences in activation in these regions, as indicated by reference to the higher resolution anatomical images, suggest that brain stem auditory nuclei may be involved. One feature that is conspicuously absent from those illustrated in Fig. 7 is the primary auditory cortex (BA41). We expected that this region would be important in differentiating HI from NH participants and hoped that it could potentially become a biomarker for predicting outcome for hearing and language following cochlear implantation in HI infants as suggested by our earlier work (Patel et al., 2007). The sedation used in the present study is a likely confounding to primary auditory function and may be partly responsible for the absence of a functional MRI feature in primary auditory cortex that differentiates the groups (DiFrancesco et al., in press). However, because Fig. 7 highlights differences between the groups that optimally separate them, it is possible that brain regions beyond primary auditory cortex that are responsible for recognizing sounds as speech and for extracting and associating content are more differentially stimulated in a scenario where the hearing impaired brain receives a rare auditory input that is above the threshold it can detect. Vibrations, loud noise and other stimuli may occasionally stimulate the auditory cortex in a deaf infant so that it is capable of processing sound and responds during our experiment in the same manner as the NH children who are receiving sound stimulation at the same relative SPL. However, unless the HI infant is participating in a successful hearing aid trial, it is much less likely that they are routinely subjected to an auditory stream of speech that is consistently above their hearing threshold and hence unintelligible. HI infants in this study were all severe to profoundly hearing-impaired and ultimately received a cochlear implant because they did not derive sufficient benefit from an external hearing aid. Though this explanation is speculative, it could explain why features B, C, E, F, G, H, and I seem to be more important in separating the HI and NH groups of infants based on brain activation during fMRI. On the other hand, our analysis on the fMRI data in this study also identified a number of areas that are not necessarily expected to play a role in differentiating HI from NH children. In particular, several functional features also appear in various portions of the anterior cingulate cortex (ACC, BA 24,32,33): areas associated with attention management, conflict monitoring, and error detection (Weissman et al., 2005). These features may be related to responses in the HI group to the novel auditory stimulus. ACC features are present in all three contrasts (C2, E1, E2, and G), suggesting a difference in response to sound input in the HI group who do not typically receive an auditory input at a level above their auditory threshold. Important features are also present in secondary visual cortex (H) (BA18), associative visual cortex (BA19) and other subcortical regions; differentiating the two groups. These features provide clues about additional neuroimaging biomarkers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant. This type of prognostic information, currently not available, is obviously of great significance. For example, it helps to calibrate the expectations and avoid subsequent disappointment, save money of the family and avoid anesthetic risks when it is clear that a child will derive no benefit from the procedure. In the present study, all infants were sedated for a clinical MRI scan and the fMRI task was appended to the end of the protocol. Further, there were different agents used for the sedation in the population we sampled, including propofol, Nembutal and sevoflurane. These drugs may have a different influence on the BOLD signal we detected. Note that the influence of sedation is to attenuate the auditory and language related brain activity and corresponding BOLD signal relative to what would be detected in awake or even sleeping babies (Difrancesco et al., 2011; DiFrancesco et al., 2013, in press; Wilke et al., 2003). Therefore, the current approach for automatic classification of NH vs. HI would likely be more effective in a scenario where fMRI data could be recorded from the participants without the influence of sedation. Demonstrating that our approach can accurately classify infants by hearing status even under the confounding influence of sedation encourages optimism for other applications where confounding disease-related conditions may modify the BOLD signal, such as cerebrovascular diseases. In the future, we will try image segmentation algorithms to define ROIs instead of thresholding the contrast maps. Other evidence, such as tissue density maps and functional connectivity networks, may be integrated into our model. For example, we can train a classifier based on the tissue density maps and then integrate it into our model with the second-layer classifier. Beyond the MRI data, our model will also permit integration from electrophysiologic imaging modalities such as evoked response potentials (ERP), electroencephalography (EEG), or magnetoencephalography (MEG). These brain scanning techniques directly record brain activities; however they are limited in their spatial resolution by the algorithms that are used to localize sources of brain activity based on recordings at the surface of the skull. Combining MR imaging features with electrophysiologic features recorded directly from brain responses to auditory input could leverage the benefits of each imaging modality to produce much more accurate predictions about patient outcomes. Due to the inherent properties of our two-layer model, integration of other evidences can be easily implemented. With the improved classifier, the method is likely to have applications to many other diseases. Conclusion First, our study demonstrates that HI and NH infants can be differentiated by brain MR images, e.g. different fMRI contrasts in auditory language network and auditory brain stem nuclei. Based upon the discriminative features, a classification model can be built to predict whether an individual has normal hearing or impaired hearing. The discriminative features may also be used as objective biomarkers of hearing loss or used for further disease mechanism studies. Secondly, our two-layer model integrates sMRI and fMRI in an effective way. While our sMRI classifier and fMRI classifier work moderately well individually, the combination of the two classifiers gives birth to a much more powerful classifier, which corroborates the hypothesis that integration of multiple modalities improves classification accuracy. Besides, our integration approach is very flexible, and it can be easily extended to include many diverse types of data. Future work with this machine learning approach to automated image classification may allow us to make predictions about speech and language outcomes in individual children who receive cochlear implants for remediation of congenital hearing impairment. The following are the supplementary data related to this article: Distribution of sMRI-fMRI scores for all 39 folds of cross validation. Each panel is one-fold of cross-validation. Horizontal axis is the output of the sMRI classifier and vertical axis is the output of the fMRI classifier. Blue dots are HI training samples, red dots are NH training samples, the black star is the testing sample. The true label of the testing sample is HI for fold1 to fold18, and NH for fold19 to fold39. Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.nicl.2013.09.008. Acknowledgment LT designed and developed the fMRI classifier. YC designed and developed the sMRI classifier. LT and YC developed the integration of the sMRI and fMRI classifiers. TCM preprocessed the sMRI and fMRI data. MMC reviewed the anatomical and functional MRI images. LJL and SKH conceived the project idea and supervised the project. LT, YC, SKH and LJL are involved in writing and preparing the manuscript. The project is funded by the CCTST Methodology grant as part of an Institutional Clinical and Translational Science Award (NIH/NCRR 8UL1TR000077-04) and NIH R01-DC07186.  ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Titles:\\n\", titles)\n",
    "print(\"Abstracts:\\n\", abstracts)\n",
    "print(\"Texts:\\n\", texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c3f4e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sylow p-groups of polynomial permutations on t...</td>\n",
       "      <td>Abstract We enumerate and describe the Sylow p...</td>\n",
       "      <td>Introduction Fix a prime p and let n∈N. Every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The transterminator ion flow at Venus at solar...</td>\n",
       "      <td>Abstract The transterminator ion flow in the V...</td>\n",
       "      <td>Introduction The nightside ionosphere of Venus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The modelling of the toughening of epoxy polym...</td>\n",
       "      <td>Abstract Silica nanoparticles possessing three...</td>\n",
       "      <td>Introduction Epoxy polymers are widely used in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flow structure and near-field dispersion in ar...</td>\n",
       "      <td>Abstract Dispersion in the near-field region o...</td>\n",
       "      <td>Introduction Understanding dispersion processe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A memory access model for highly-threaded many...</td>\n",
       "      <td>Abstract A number of highly-threaded, many-cor...</td>\n",
       "      <td>Introduction Highly-threaded, many-core device...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Investigating the feasibility of scale up and ...</td>\n",
       "      <td>Abstract The transfer of a laboratory process ...</td>\n",
       "      <td>Introduction Human induced pluripotent stem ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Phosphorus levels in croplands of the European...</td>\n",
       "      <td>Abstract In the frame of the Land Use/Land Cov...</td>\n",
       "      <td>Introduction Soil represents a temporary reser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chirality delivery through multiple and helica...</td>\n",
       "      <td>Abstract The path of the chirality delivery in...</td>\n",
       "      <td>Introduction The chirality delivery is a growi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Sleipner storage site: Capillary flow mode...</td>\n",
       "      <td>Abstract To prevent ocean acidification and mi...</td>\n",
       "      <td>Introduction Climate change and ocean acidific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Combined analysis of sMRI and fMRI imaging dat...</td>\n",
       "      <td>Abstract In this research, we developed a robu...</td>\n",
       "      <td>Introduction It has been estimated that approx...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Sylow p-groups of polynomial permutations on t...   \n",
       "1  The transterminator ion flow at Venus at solar...   \n",
       "2  The modelling of the toughening of epoxy polym...   \n",
       "3  Flow structure and near-field dispersion in ar...   \n",
       "4  A memory access model for highly-threaded many...   \n",
       "5  Investigating the feasibility of scale up and ...   \n",
       "6  Phosphorus levels in croplands of the European...   \n",
       "7  Chirality delivery through multiple and helica...   \n",
       "8  The Sleipner storage site: Capillary flow mode...   \n",
       "9  Combined analysis of sMRI and fMRI imaging dat...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Abstract We enumerate and describe the Sylow p...   \n",
       "1  Abstract The transterminator ion flow in the V...   \n",
       "2  Abstract Silica nanoparticles possessing three...   \n",
       "3  Abstract Dispersion in the near-field region o...   \n",
       "4  Abstract A number of highly-threaded, many-cor...   \n",
       "5  Abstract The transfer of a laboratory process ...   \n",
       "6  Abstract In the frame of the Land Use/Land Cov...   \n",
       "7  Abstract The path of the chirality delivery in...   \n",
       "8  Abstract To prevent ocean acidification and mi...   \n",
       "9  Abstract In this research, we developed a robu...   \n",
       "\n",
       "                                                text  \n",
       "0  Introduction Fix a prime p and let n∈N. Every ...  \n",
       "1  Introduction The nightside ionosphere of Venus...  \n",
       "2  Introduction Epoxy polymers are widely used in...  \n",
       "3  Introduction Understanding dispersion processe...  \n",
       "4  Introduction Highly-threaded, many-core device...  \n",
       "5  Introduction Human induced pluripotent stem ce...  \n",
       "6  Introduction Soil represents a temporary reser...  \n",
       "7  Introduction The chirality delivery is a growi...  \n",
       "8  Introduction Climate change and ocean acidific...  \n",
       "9  Introduction It has been estimated that approx...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put data into pandas DataFrame\n",
    "data = pd.DataFrame({\"title\": titles,\n",
    "                     \"abstract\": abstracts,\n",
    "                     \"text\": texts})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a0f919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer & BART model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4141d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries\n",
    "def generate_summary(row):\n",
    "    input_text = f\"{row['abstract']} {row['text']}\"  # Combine abstract and text for summarization\n",
    "    tokenized_input = tokenizer([input_text], max_length=1024, return_tensors='pt')\n",
    "    summary_ids = model.generate(tokenized_input['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00dce014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row in the DataFrame to generate summaries\n",
    "data['summary'] = data.apply(generate_summary, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "100a9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Sylow p-groups of polynomial permutations on t...   \n",
      "1  The transterminator ion flow at Venus at solar...   \n",
      "2  The modelling of the toughening of epoxy polym...   \n",
      "3  Flow structure and near-field dispersion in ar...   \n",
      "4  A memory access model for highly-threaded many...   \n",
      "5  Investigating the feasibility of scale up and ...   \n",
      "6  Phosphorus levels in croplands of the European...   \n",
      "7  Chirality delivery through multiple and helica...   \n",
      "8  The Sleipner storage site: Capillary flow mode...   \n",
      "9  Combined analysis of sMRI and fMRI imaging dat...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Abstract We enumerate and describe the Sylow p...   \n",
      "1  Abstract The transterminator ion flow in the V...   \n",
      "2  Abstract Silica nanoparticles possessing three...   \n",
      "3  Abstract Dispersion in the near-field region o...   \n",
      "4  Abstract A number of highly-threaded, many-cor...   \n",
      "5  Abstract The transfer of a laboratory process ...   \n",
      "6  Abstract In the frame of the Land Use/Land Cov...   \n",
      "7  Abstract The path of the chirality delivery in...   \n",
      "8  Abstract To prevent ocean acidification and mi...   \n",
      "9  Abstract In this research, we developed a robu...   \n",
      "\n",
      "                                                text  \\\n",
      "0  Introduction Fix a prime p and let n∈N. Every ...   \n",
      "1  Introduction The nightside ionosphere of Venus...   \n",
      "2  Introduction Epoxy polymers are widely used in...   \n",
      "3  Introduction Understanding dispersion processe...   \n",
      "4  Introduction Highly-threaded, many-core device...   \n",
      "5  Introduction Human induced pluripotent stem ce...   \n",
      "6  Introduction Soil represents a temporary reser...   \n",
      "7  Introduction The chirality delivery is a growi...   \n",
      "8  Introduction Climate change and ocean acidific...   \n",
      "9  Introduction It has been estimated that approx...   \n",
      "\n",
      "                                             summary  \n",
      "0  Theorem 5.1: There are (p-1)p-2 Sylow p-groups...  \n",
      "1  The transterminator ion flow in the Venusian i...  \n",
      "2  Silica nanoparticles possessing three differen...  \n",
      "3  Diverging streamlines around buildings enhance...  \n",
      "4   Threaded Many-core Memory (TMM) model is mean...  \n",
      "5  Human induced pluripotent stem cells (hiPSC) a...  \n",
      "6  Soil phosphorus (P) is an essential element fo...  \n",
      "7  Chirality delivery is a growing topic of inter...  \n",
      "8  Sleipner storage site, offshore Norway, is the...  \n",
      "9  1 to 6 infants per 1000 are born with severe t...  \n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a76586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
